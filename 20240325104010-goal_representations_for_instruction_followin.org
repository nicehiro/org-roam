:PROPERTIES:
:ID:       5cd077c3-64bf-40f8-9c4d-fd855b932d6d
:END:
#+title: Goal Representations for Instruction Following


This paper tries to achieve: given an image of current state and human language instruction, let the robot achieve it.

By learning an embedding from the labeled data that aligns language instruction with state-goal, they decouple the language-conditioned policy $\pi(a|s,l)$ to a policy network $\pi(a|s,z)$ and a language conditioned task encoder $f(l)$, where $z = f(l)$ is the representation of the task.

For alignment of instruction $l$ and goal $(s_0,g)$, they use contrastive learning by assuming the embedding $f(l)$ and $h(s_0, g)$ of the same task to be close in the latent space, while they should be far apart for different task.

For $f$ and $h$ used in this paper, they use the fine-tuned CLIP model for both embedding.

For action policy training, they use Behavior Cloning.

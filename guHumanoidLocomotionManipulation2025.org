:PROPERTIES:
:ID:       B6A18EAC-937F-48B0-820D-F4B8A076462B
:ROAM_REFS: @guHumanoidLocomotionManipulation2025
:END:
#+title: Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning
#+filetags: :survey:humanoid:


* What is Humanoid Robot?

From the perspective of application:

#+begin_quote
Humanoid robots are built to (ideally) replicate human motions in performing various human-level skills, such as _locomotion_, _manipulation_, and _cognitive capabilities_.
#+end_quote

From the perspective of composition:

#+begin_quote
A humanoid robot refers to any anthropomorphic robot that resembles the form of a human. Typically, a humanoid robot possesses a torso, two arms, and two legs, though the degree of anthropomorphism may vary.
#+end_quote

* Background

** Bipedal Locomotion

Locomotion refers to the physical methods and mechanisms that enable a humanoid robot to move its body from one place to another. (Low-level motor control)

** Navigation

Navigation involves the robot's ability to determine its position and plan paths to reach desired locations. (High-level planning)

** Whole-body Manipulation

Antropomorphic manipulation has been the inspiration for bimanual manipulation, loco-manipulation, dexterous manipulation. The ultimate form of the antropomorphic manipulation is whole-body manipulation, referring to the ability to manipulation objects using any part of one's body.

*No robot self mobility (locomotion)*

** Loco-manipulation

As suggested by its name, loco-manipulation involves both the movement of the objects throught manipulation and the mobility of the robot self through locomotion.

*No All surface interaction (whole-body)*

** Whole-body loco-manipulation

Humanoid whole-body loco-manipulation needs to schedule contact for all limbs to simultaneously achieve robust movement and safe object interaction.

** Tactile Sensing

Tactile sensing is yet an underexplored modality for advancing humanoid loco-manipulation, providing direct contact information necessary for tasks involving complex interactions with environments and objects.

* Learning Loco-manipulation Skills

** Reinforcement Learning from Scratch

RL promises an effective way to learn motor skills by rewarding desirable behaviors and penalizing undesired behaviors, with minimal or no supervision during training. The end-to-end RL policies translate raw sensory input to actuation and are executable in real time.

Some problems encountered:

- meticulous design of reward functions

  + *reinforcement learning w/ sparse reward*

- sim-to-real gap

  Humanoid robots posses higher DoFs and unstable dynamics, where the center of mass constantly moves out of the support polygon.

  + *domain randomization* by varing the properties of a robot model, such as mass, friction, and actuator dynamics to train a generalized policy.

  + *system identification* by approximating the system's input-output behavior from real-world data.

  + *domain adaptation* by using real-world data directly to fine-tune a simulator-trained policy.

** Learning from Demonstration

Refer to [[id:b290da32-08f7-4ae4-9a75-1295104b0f53][Imitation Learning]] for more details.

Four possible source of demonstrations for humanoid robots:

- policy execution on hardware (robot)

  + collecting data on physical robots (requires a laborious setup of the environment and raises significant safety concerns)

  + collecting data in simulation (sim-to-real gap)

- teleoperation (robot)

  Control flow for learning from teleoperated demonstrations:

  1. human operator (VR, Exoskeleton, Optical Tracking, Motion Capture, Joystick, ALOHA)

  2. motion retargeting

  3. desired robot trajectory

  Some limitations:

  - a majority of teleoperation systems capture only manipulation skills, full-body sensing, including human gaits are missing (requires IMU or exoskeletons which are expensive)

  - the teleoperation data may limited if the robot's kinematics do not enable seamless retargeting

  - time-consuming to scale

- motion capture from human (3D) (human)

  Captures humans interacting with various objects while moving around.

  - require heavily instrumented environments and actors

  - less outdoor activities

- human videos from internet (2D) (human)

  Obtain rich and diverse human motion datafrom the internet.

  - lower quality, containing noise, non-physical artifacts

** Hybrid Models

- A teacher policy trained from simulation using pure RL. Then a student policy clones the behavior of the teacher.

- Using IL first to pre-train an policy from demonstration. Then a RL policy fine-tunes the policy.

** Representaion of Skills

- motion Representation

- goal representation

- state transition representation

** Learning for Humanoid Loco-manipulation

- most in simulation, the physical interactions with external environment or objects are often oversimplified

- careful reward design using RL


* Foundation Models for Humanoid Robots

** Applying LLMs/VLMs to Humanoid Robots

Refer to [[id:A000C46A-F2AC-4B16-A94A-F741BC67576E][Real-World Robot Applications of Foundation Models: A Review]].

** Building Humanoid Foundation Models

Refer to

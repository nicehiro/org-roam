:PROPERTIES:
:id: D34F35B3-BC69-463A-86F5-CE7CCF441065
:END:
#+title: Asynchronous Advantage Actor Critic
#+STARTUP: latexpreview
#+filetags: :rl:

* Related Works
Experience replay buffer and the approximation of the real action(or state)
function share a common idea: the sequence of observed data encountered by an
online RL is no-stationary, and on-line RL updates are strongly correlated.
Aggregating over memory in this way reduces non-stationarity and decorrelates
updates, but at the same time limits the methods to off-policy algorithms.


Experience replay has several drawbacks:
1. it uses more memory and computation per real interaction;
2. it requires off-policy learning algorithms that can update from data generated
   by an old policy.


In this paper, we asynchronously execute multiple agents in parallel instead of
experience replay. This parallelism also decorrelates the agents' data into more
stationary process. So A3C is a on-policy algorithm.
* Methods

[[file:img/a3c/Methods/2021-05-20_17-41-20_screenshot.png]]

1. The policy and the value function are updated after every $t_{\max}$ actions or
   when a terminal state is reached.
2. While the parameters $\theta$ of the policy and $\theta_{v}$ of the value
   function are shown as being separate for generality, we always share some of
   the parameters in practice. We typically use a convolutional neural network
   that has one softmax output for the policy and one linear output for the
   value function, *with all non-output layers shared*.
3. Adding the entropy of the policy $\pi$ to the objective function improved
   exploration by discouraging premature convergence to suboptimal deterministic
   policies.
* Reference
- [[ebib:mnihAsynchronousMethodsDeep2016][Asynchronous Methods for Deep Reinforcement Learning]]

# Local Variables:
# org-gtd-directory: "./img/a3c/"
# End:
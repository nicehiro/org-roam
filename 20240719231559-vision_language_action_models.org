:PROPERTIES:
:ID:       58c10fcd-edbe-4b15-bc42-04a2ae880a4d
:END:
#+title: Vision-Language-Action Models
#+filetags: :VLA:


Vision-Language-Action models use _pre-trained VLMs_ as base model, and fine-tuned for robot control.


* Current VLAs

- [[id:a8a38a72-f501-4ddc-b097-76f2c182e8cc][OpenVLA: An Open-Source Vision-Language-Action Model]]

- [[id:4d47b657-f192-40e5-8894-0070853731da][Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression]]

- [[id:0ceb13c2-ac2c-4e90-bb44-3d5506cb08e9][TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policiy]]

- [[id:205DE6FB-054E-418A-BC32-C05E82AF1CA0][$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control]]

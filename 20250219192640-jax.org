:PROPERTIES:
:ID:       a839a687-0335-4be4-a616-a21fafbb420b
:ROAM_REFS: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html
:END:
#+title: JAX


* Why JAX?

- It can be trained at least 3x faster than in PyTorch with a similar setup.

A technical detail of running operations on DeviceArrays is that when a JAX function is called, _the corresponding operation takes place asynchronously on the accelerator_ when possible. This way, Python will not block the execution of follow-up statements, but instead only _does it whenever we strictly need the value of out_.

- The multi-devices array operation is much more easier.

- Functional programming.

we can not only use JAX to estimate the gradients at a certain input, but actually return the analytical gradient function.

The functions in JAX need to be written with certain constraints:

1. the functions are not allowed to have side-effects, meaning that they are _not allowed to affect any variable outside of their namespaces_;

2. JAX compiles the functions based on _anticipated shapes_ of all arrays in the function, which will problematic if the shapes within the function depends on the value of the tensor.


* Device Arrays

A simple CPU-based array (=jax.device_get(array)=) is nothing else than a Numpy array. JAX will handle any device clash itself when try to perform operations on a Numpy Array and a Device Array by modeling the output as Device Array again. You can use =jax.device_put(cpu_array)= to explicitly push a Numpy array to the accelerator.


* Function Transformations with Jaxpr

JAX is designed to be functional, as in functional programming. The important feature of functional programming to grok when working with JAX is very simple: _don't write code with side-effects_.

To view the jaxpr representation of the JAX function, we can use =jax.make_jaxpr=. Since the tracing depends on the shape of the input, we need to pass an input to the function, like =jax.make_jaxpr(jax_func)(input)=.


** Automatic Differentiation

The intermediate jaxpr representation defines a computation graph, on which we can perform automatic differentiation. In frameworks like PyTorch with a dynamic computation graph, we would compute the gradients based on the loss tensor, e.g. by calling =loss.backward()=.

However, _JAX directly works with functions_. Instead of backprpagating gradients through tensors, JAX takes as input a function, and outputs another function which directly calculates the gradients for it. _Your gradient of parameters is really a function of parameters and data._

We use =jax.grad=, which takes as input the function, and return another function representing the gradient calculation of the input with respect to the output, e.g. =grad_func = jax.grad(jax_func), grad = grad_func(input)=. We can also print the jaxpr presentation of the gradient function by =jax.make_jaxpr(grad_func)(input)=. Hence, we can not only use JAX to estimate the gradients at a certain input, but actually return the analytical gradient function.

We can use =jax.value_and_grad(jax_func)= to obtain the value and gradients of the function.

JAX offers an elegant data structure =pytree=, which is a container-like object, to summarize all parameters.


** Speeding up computation with Just-In-Time Compilation

The jaxpr representation of the gradient function exists unnecessary scalars. JAX can find such cases to improve efficiency and optimize the code to take full advantage of the available accelerator hardware by _compiling functions just-in-time_ with (Accelerated Linear Algebra). XLA fuses operations to reduce execution time of short-lived operations and eliminates intermediate storage buffers where not needed.

To compile a function, JAX provides the =jax.jit(jax_func)= transformation. The jitted function is _much faster_ than the original one.

* Implementing a Neural Network with Flax

A collection of exist neural network libraries based on JAX: Flax, Haiku, Trax, and others. We will use Flax due to its flexibility, intuitive API and larger community.

** nn.Module

Flax uses "Lazy" initialization so that the initialization function =setup()=, e.g. =__init__()= in PyTorch, is called once before the model is called, or attributes are accessed. The =__call__()= method represents the =forward= function in PyTorch.

Since all layers that we define in =setup= are also used in the ==__call__= function, we use more compact network creation with =@nn.compact=. It is recommended to use the setup version if we define more functions on a module besides =__call__= and want to reuse some modules.

Another different with PyTorch is thant we don't need to explicitly input the dimension of =in= for a layer, e.g. =nn.Dense(features)= because JAX is directly work with functions rather than tensors. Actually, there's no need to tell the specific dimension of the =in=!

We use =model.init(init_rng, input)= to initialize the model and =model.apply(params, input)= to get the output.

:PROPERTIES:
:id: 413C6561-F38A-490D-B350-93AF7FA6F224
:END:
#+title: Gaussian discriminant analysis
#+filed: machine-learning
#+OPTIONS: toc:nil
#+filetags: :ml:

* Description
在一个分类问题中，如果输入特征 $x$ 是连续型随机变量，
那么可以考虑使用 GDA(Gaussian Discriminant Analysis) 模型，
它拟合的对象为 $p(x|y)$ ，并假设 $p(x|y)$ 服从[[id:8AC0AC13-4BFB-48F4-8005-2C763BE9416D][多维正态分布]]。

GDA 模型为：

\begin{aligned}
y &\sim \text{Bernoulli}(\phi) \\
x|y=0 &\sim \mathcal{\mu_0,\Sigma} \\
x|y=1 &\sim \mathcal{\mu_1,\Sigma} \\
\end{aligned}

展开为：

\begin{aligned}
p(y) &=\phi^{y}(1-\phi)^{1-y} \\
p(x \mid y=0) &=\frac{1}{(2 \pi)^{d / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x-\mu_{0}\right)^{T} \Sigma^{-1}\left(x-\mu_{0}\right)\right) \\
p(x \mid y=1) &=\frac{1}{(2 \pi)^{d / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x-\mu_{1}\right)^{T} \Sigma^{-1}\left(x-\mu_{1}\right)\right)
\end{aligned}

请注意，此模型中两个随机变量分别有各自的均值 $\mu_0,\mu_1$ ，
但二者的协方差矩阵都是 $\Sigma$ 。

使用[[id:0863DAB5-25FA-42BD-A02F-9EF1FC11DA78][极大似然法]]求解，计算其 log-likelihood

\begin{aligned}
\ell\left(\phi, \mu_{0}, \mu_{1}, \Sigma\right) &=\log \prod_{i=1}^{n} p\left(x^{(i)}, y^{(i)} ; \phi, \mu_{0}, \mu_{1}, \Sigma\right) \\
&=\log \prod_{i=1}^{n} p\left(x^{(i)} \mid y^{(i)} ; \mu_{0}, \mu_{1}, \Sigma\right) p\left(y^{(i)} ; \phi\right)
\end{aligned}

求得其中参数为：

\begin{aligned}
\phi &=\frac{1}{n} \sum_{i=1}^{n} 1\left\{y^{(i)}=1\right\} \\
\mu_{0} &=\frac{\sum_{i=1}^{n} 1\left\{y^{(i)}=0\right\} x^{(i)}}{\sum_{i=1}^{n} 1\left\{y^{(i)}=0\right\}} \\
\mu_{1} &=\frac{\sum_{i=1}^{n} 1\left\{y^{(i)}=1\right\} x^{(i)}}{\sum_{i=1}^{n} 1\left\{y^{(i)}=1\right\}} \\
\Sigma &=\frac{1}{n} \sum_{i=1}^{n}\left(x^{(i)}-\mu_{y^{(i)}}\right)\left(x^{(i)}-\mu_{y^{(i)}}\right)^{T}
\end{aligned}

上诉参数其实都可以从统计的角度给出合理解释，用样本估计分布参数。

其输出类似下图：
#+begin_center
#+caption: GDA Output
#+attr_latex: scale=0.75
#+attr_html: :width 400
#+attr_org: :width 400
[[file:img/cs229/2/gda.png]]
#+end_center
图中，训练集中的两类数据被两个高斯分布拟合，这两个高斯分布形状相同，只有中心位置不同。
** GDA 和 Logistic Regression 的关系
仔细观察上诉的密度函数 $p(y=1|x;\phi,\mu_0,\mu_1,\Sigma)$ ，它可以展开成：

$$
p\left(y=1 \mid x ; \phi, \Sigma, \mu_{0}, \mu_{1}\right)=\frac{1}{1+\exp \left(-\theta^{T} x\right)}
$$

其中 $\theta$ 是关于 $\phi,\Sigma,\mu_0,\mu_1$ 的近似参数。
这其实就是 [[id:82C8F1D3-2526-4684-B635-FAFD10C227E6][Logistic Regression]] $p(y=1|x)$ 的表达式。

前面我们提到，使用 GDA 需要在随机变量 $x$ 服从多维正态分布才行。
而 Logistic Regression 没有这些限制。
因此，在模型输入可以满足 GDA 的假设时，使用 GDA 会有更高的数据利用率，
并且拟合效果很好；而 Logistic Regression 具有鲁棒性，更适合其他的场景。

:PROPERTIES:
:ID:       5add24c2-bc4d-4501-ae36-88167910513b
:END:
#+title: Self-Consistency Improves Chain of Thought Reasoning in Language Models
#+filetags: :prompt:LLM:paper:


#+begin_quote
[[zotero://select/items/1_YFF5L2YG][Wang, Xuezhi, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” arXiv, March 7, 2023. https://doi.org/10.48550/arXiv.2203.11171.]]
#+end_quote


Self-consistency also based on [[id:dc79e111-6fe7-4d1e-8bcd-efe0667af4ea][CoT]], which aims to replace the "naive" greedy decoding result used in [[id:dc79e111-6fe7-4d1e-8bcd-efe0667af4ea][CoT]].

The intuition is really simple: a model can generate several plausible responses to a math question that all arive at the same correct answer; it can also produce an incorrect reasoning path but those solutions are less likely to arrive at the same answer.

Specific instructions:
1. Prompted with a set of manually written chain-of-thought examples
2. Sample a set of candidate outputs from the LLM's decoder
3. Aggregate the answers and choose the answer that is the most consistent

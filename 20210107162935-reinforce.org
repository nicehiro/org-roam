:PROPERTIES:
:id: D56B0801-4B58-42E1-B9B0-4CDE76A5B657
:ROAM_ALIASES: Monte-Carlo-Policy-Gradient
:END:
#+title: REINFORCE
#+filetags: :rl:

* Description
在[[id:A84D656F-B4D8-4BBA-B20E-9416230D05CF][策略梯度]]中，我们提到了策略梯度的目标函数，即：

$$
J(\theta)=\mathbf{E}_{\tau\sim p(\tau)}[R(\tau)]
$$

其梯度为：

$$
\nabla_{\theta}J(\theta)=\mathbf{E}_{\tau\sim\pi(\theta)}[R(\tau)\cdot\nabla_{\theta}\sum_{t=0}^{T}\log\pi_{\theta}(a_t|s_t)]
$$

REINFORCE 算法（又称 Monte-Carlo 梯度估计算法），
是非确定性策略算法，即策略网络输出的是分布，而不是确定的动作。根据分布获得具体的动作。
使用从真实环境中采样的样本去估计 $R(\tau)$ 的期望。

即：

$$
\mathbf{E}_{\tau\sim p(\tau)}[R(\tau)]=\lim_{N\rightarrow\infty}\frac{1}{N}\sum_{i=1}^{N}R(\tau)
$$

同理，其梯度也可以用样本去估计，

$$
\nabla_{\theta}J(\theta)=\lim_{N\rightarrow\infty}\frac{1}{N}\sum_{i=1}^{N}\left[R(\tau)\cdot\nabla_{\theta}\sum_{t=0}^{T}\log\pi_{\theta}(a_t|s_t)\right]
$$

$\pi_{\theta}(a_t|s_t)$ 表示策略在 $s_t$ 状态执行 $a_t$ 的概率。

因此，我们得到了最简单的强化学习策略梯度算法：

1. 初始化策略网络
2. 循环
   - 使用策略 $\pi_{\theta}$ 生成一条轨迹 $\tau$ ，计算其奖励 $R(\tau)$
   - 更新参数 $\theta\leftarrow\theta+\alpha[R(\tau)\cdot\nabla_{\theta}\sum_{t=0}^{T}\log\pi_{\theta}(a_t|s_t)]$
* Summary
*优点* ：
1. 如果当前奖励很大，那么当前轨迹的的概率（或者是当前一连串动作生成的概率）就会变大。
   它能保证轨迹的更新与奖励梯度的方向相同，这就确保了它可以达到局部最优。

*缺点* ：
1. 使用蒙特卡洛采样估计，可能会具有很大的方差，因此学习缓慢。
2. 数据利用率低。作为一种在线（online）学习方法，每一回合的数据只能使用一次。
3. 只能找到局部最优解，很难找到全局最优解。

#+begin_quote
Unfortunately, the variance of the gradient estimator scales unfavorably
with the time horizon, since the effect of an action is confounded with
the effects of past and future actions.

From GAE paper.
#+end_quote
* Reference
- [[https://towardsdatascience.com/policy-gradient-methods-104c783251e0]]

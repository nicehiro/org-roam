:PROPERTIES:
:ID:       244B1ED5-8DB1-4645-A7BC-A8FFC1285A52
:END:
#+title: Language Modeling from Scratch
#+filetags: :LLM:CS336:


* Lec1: Overview and Tokenizer

The most time-consuming process is coping data from GPU memory to Computer. So the trick is: organize computation to maximize utilization of GPUs by minimizing data movement.

Data movement between GPUs is even slower, but same 'minimize data movement' principle holds.
- Use collective operations (e.g., gather, reduce, all-reduce)
- Shard (parameters, activations, gradients, optimizer states) across GPUs
- How to split computation: {data, tensor, pipeline, sequence} parallelism

** Scaling Law

- With more FLOPs, you can train more big model within same tokens (data).
- With more FLOPs, you can train more tokens (data) on same model.
- TL;DR, $Tokens^* = 20 Model Params^*$.

** Tokenization

[[id:2BEF57EC-5133-48D1-B79D-A260925A8B53][Tokenization]]

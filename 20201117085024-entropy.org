#+title: Entropy
#+filed: math
#+OPTIONS: toc:nil
#+roam_alias:
#+roam_tags: entropy
#+startup: latexpreview

* Description
熵（Entropy）是接受的每条信息中包含的信息的平均量。
熵最好理解为不确定性的度量，而不是确定性的度量，因为越随机的信源，熵越大。

*例一* 如果有一枚理想的硬币，其出现正面和反面的机会相等，则抛硬币事件的熵等于其能够达到的最大值。
使用一枚硬币进行 1 次抛投，这个事件的熵是 1 比特（ $1\times0.5+1\times0.5$ ），因为结果不外乎两个——正面或者反面，
可以表示为 0，1 编码，而且两个结果之间相互独立。
若进行 n 次独立实验，则熵为 n。

*例二* 另一个稍微复杂的例子是假设一个随机变量 $X$ ，取三种可能值 $x_1,x_2,x_3$ ，概率分别是 $1/2,1/4,1/4$ ，
那么编码平均比特长度为 $1/2\times1+1/4\times2+1/4\times2=3/2$ ，其熵为 $3/2$ ，
如果 3 个随机变量，第一个随机变量只需要一个比特就可以表示，后两个变量都需要两个比特才能表示。

香农把随机变量 $X$ 的熵值 $\mathcal{H}$ 定义如下，

$$
H(X)=E[I(X)]=E[-\ln(P(X))]
$$

其中， $P$ 为 $X$ 的概率函数， $E$ 为期望， $I(X)$ 是 $X$ 的信息量。

当取自有限样本时，熵的公式可以表示为：

$$
H(X)=\sum_{i}P(x_i)I(x_i)=-\sum_{i}P(x_i)\log_{b}P(x_i)
$$

为什么可以使用 $-\log_{b}P(x)$ 来表示信息量呢？
我有一个直观未证明的解释。如上 *例二* ， $1=-\log_{2}\frac{1}{2}$ ， $2 = -\log_{2}\frac{1}{4}$ 。

:PROPERTIES:
:id: 40367E42-4090-46D6-9DF5-D20F9ADB2755
:END:
#+title: A Synchronous Advantage Actor-Critic
#+filed:
#+OPTIONS: toc:nil
#+filetags: :rl:


After reading the [[id:D34F35B3-BC69-463A-86F5-CE7CCF441065][Asynchronous Advantage Actor Critic]],
AI research wondered whether the asynchrony led to improved performance
(e.g. "perhaps the added noise would provide some regularization or
exploration?"), or if it was just an implementation detail that allowed
for faster training with a CPU-based implementation.

A2C is a *synchronous*, *deterministic* version of A3C (with the first "A"
removed). In A3C each agent talks to the global parameters independently,
so it is possible sometimes the thread-specific agents would be playing with
policies of different versions and therefore the aggregated update would not
be optimal. A2C *waits for* each actor to finish its segment of experience before
performing an update, *averaging* over all of the actors.

The synchronized gradient update keeps the training more cohesive and potentially
to make convergence faster. And it can more effectively use of GPUs, which
perform best with large batch sizes.


#+DOWNLOADED: screenshot @ 2021-05-24 22:06:26
#+attr_html: scale=0.8 :align center
#+attr_latex: :width 600cm
#+attr_org: :width 600px
#+CAPTION: The architecture of A3C versus A2C.(From liliweng's blog)
[[file:img/a2c/2021-05-24_22-06-26_screenshot.png]]

* Reference
- https://openai.com/blog/baselines-acktr-a2c/
- https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f

# Local Variables:
# org-gtd-directory: "./img/a2c/"
# End:
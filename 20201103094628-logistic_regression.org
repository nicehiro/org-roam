:PROPERTIES:
:ID:       82C8F1D3-2526-4684-B635-FAFD10C227E6
:END:
#+title: Logistic Regression
#+filed: Machine-Learning
#+OPTIONS: toc:nil
#+STARTUP: latexpreview
#+filetags: :machine_learning:classfication:Users:wangfangyuan:Documents:roam:org_roam:
#+hugo_section: ml

* Description
逻辑回归虽然叫做是回归，但其实是分类算法。
为了简单，本节以二分类问题为例。

我们可以忽略标签是离散的，然后直接将线性回归算法应用到分类问题上，例如[[id:8550656B-713F-417E-8E19-4D94BB7E7580][Linear Classification]]。

逻辑回归其实也是在线性回归的基础上改进的。

Logistic 分布是连续的概率分布，其分布函数为：
$$
F(x)=P(X\leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma}}
$$
密度函数为：
$$
f(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}
$$
其中 $\mu$ 表示位置参数， $\gamma > 0$ 为形状参数。
#+begin_center
#+caption: CDF of Logistic distribution
#+attr_org: :width 400
[[file:./img/logistic-regression/cdf_logistic.png]]
#+end_center

#+begin_center
#+caption: PDF of Logistic distribution
#+attr_latex: scale=0.75
#+attr_html: :width 400
#+attr_org: :width 400
[[file:./img/logistic-regression/pdf_logistic.png]]
#+end_center

Sigmoid 方程就是 Logistic 函数在 $\mu=0,\gamma=1$ 时的函数。

Sigmoid 函数的导数为：
\begin{aligned}
g^{\prime}(z) &=\frac{d}{d z} \frac{1}{1+e^{-z}} \\
&=\frac{1}{\left(1+e^{-z}\right)^{2}}\left(e^{-z}\right) \\
&=\frac{1}{\left(1+e^{-z}\right)} \cdot\left(1-\frac{1}{\left(1+e^{-z}\right)}\right) \\
&=g(z)(1-g(z))
\end{aligned}

对于二分类问题，我们做一下假设：

$$
P(y=1|x;\theta)=h_{\theta}(x)
$$
$$
P(y=0|x;\theta)=1-h_{\theta}(x)
$$
其中， $h_{\theta}(x)=g(\theta^{T}x)$ 。

即：
$$
P(y|x;\theta)=h_{\theta}(x)^{y}(1-h_{\theta}(x))^{1-y}
$$

我们的目标是要使得真实分类的概率越大越好，运用[[id:0863DAB5-25FA-42BD-A02F-9EF1FC11DA78][Maximum likelihood Estimation]]，得到：

\begin{aligned}
L(\theta) &=p(\vec{y} \mid X ; \theta) \\
&=\prod_{i=1}^{n} p\left(y^{(i)} \mid x^{(i)} ; \theta\right) \\
&=\prod_{i=1}^{n}\left(h_{\theta}\left(x^{(i)}\right)\right)^{y^{(i)}}\left(1-h_{\theta}\left(x^{(i)}\right)\right)^{1-y^{(i)}}
\end{aligned}

化简损失函数：

\begin{aligned}
\ell(\theta) &=\log L(\theta) \\
&=\sum_{i=1}^{n} y^{(i)} \log h\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h\left(x^{(i)}\right)\right)
\end{aligned}

求解梯度：

\begin{aligned}
\frac{\partial}{\partial \theta_{j}} \ell(\theta) &=\left(y \frac{1}{g\left(\theta^{T} x\right)}-(1-y) \frac{1}{1-g\left(\theta^{T} x\right)}\right) \frac{\partial}{\partial \theta_{j}} g\left(\theta^{T} x\right) \\
&=\left(y \frac{1}{g\left(\theta^{T} x\right)}-(1-y) \frac{1}{1-g\left(\theta^{T} x\right)}\right) g\left(\theta^{T} x\right)\left(1-g\left(\theta^{T} x\right)\right) \frac{\partial}{\partial \theta_{j}} \theta^{T} x \\
&=\left(y\left(1-g\left(\theta^{T} x\right)\right)-(1-y) g\left(\theta^{T} x\right)\right) x_{j} \\
&=\left(y-h_{\theta}(x)\right) x_{j}
\end{aligned}

运用[[id:24B04706-9D53-438E-9C99-4A9FB6AD763B][Gradient Descent]]不断更新参数：

$$
\theta_{j}:=\theta_{j}+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_{j}^{(i)}
$$

线性分类器是在线性回归的基础上对最后一层的输出做不同的处理，使用设计的损失函数来进行分类的。
同样的，逻辑回归在线性回归的基础上加了一层逻辑分布，逻辑分布的结果被当作当前输入对应不同输出的概率分布，
然后再比较不同分类输出的概率的大小来确定分类。

即：
$$
h_{\theta}(x)=g(\theta^{T}x)=\frac{1}{1+e^{-\theta^{T}x}}
$$

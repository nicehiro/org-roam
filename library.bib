@article{agibot-worldAgiBotWorldColosseo,
  title = {{{AgiBot World Colosseo}}: {{Large-scale Manipulation Platform}} for {{Scalable}} and {{Intelligent Embodied Systems}}},
  author = {{AgiBot-World}, Team},
  abstract = {We explore how scalable robot data can address real-world challenges for generalized robotic manipulation. Introducing AgiBot World, a large-scale platform comprising over 1 million trajectories across 217 tasks in five deployment scenarios, we achieve an order-of-magnitude increase in data scale compared to existing datasets. Accelerated by a standardized collection pipeline with human-in-the-loop verification, AgiBot World guarantees high-quality and diverse data distribution. It is extensible from grippers to dexterous hands and visuo-tactile sensors for fine-grained skill acquisition. Building on top of data, we introduce Genie Operator-1 (GO-1), a novel generalist policy that leverages latent action representations to maximize data utilization, demonstrating predictable performance scaling with increased data volume. Policies pre-trained on our dataset achieve an average performance improvement of 30\% over those trained on Open X-Embodiment, both in in-domain and out-of-distribution scenarios. GO-1 exhibits exceptional capability in real-world dexterous and long-horizon tasks, achieving over 60\% success rate on complex tasks and outperforming prior RDT approach by 32\%. By open-sourcing the dataset, tools, and models, we aim to democratize access to large-scale, high-quality robot data, advancing the pursuit of scalable and general-purpose intelligence.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/F99UQ97F/AgiBot-World - AgiBot World Colosseo Large-scale Manipulation Platform for Scalable and Intelligent Embodied Syste.pdf}
}

@misc{albabaNILNodataImitation2025,
  title = {{{NIL}}: {{No-data Imitation Learning}} by {{Leveraging Pre-trained Video Diffusion Models}}},
  shorttitle = {{{NIL}}},
  author = {Albaba, Mert and Li, Chenhao and Diomataris, Markos and Taheri, Omid and Krause, Andreas and Black, Michael},
  year = {2025},
  month = mar,
  number = {arXiv:2503.10626},
  eprint = {2503.10626},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.10626},
  urldate = {2025-03-17},
  abstract = {Acquiring physically plausible motor skills across diverse and unconventional morphologies-including humanoid robots, quadrupeds, and animals-is essential for advancing character simulation and robotics. Traditional methods, such as reinforcement learning (RL) are task- and body-specific, require extensive reward function engineering, and do not generalize well. Imitation learning offers an alternative but relies heavily on high-quality expert demonstrations, which are difficult to obtain for non-human morphologies. Video diffusion models, on the other hand, are capable of generating realistic videos of various morphologies, from humans to ants. Leveraging this capability, we propose a data-independent approach for skill acquisition that learns 3D motor skills from 2D-generated videos, with generalization capability to unconventional and non-human forms. Specifically, we guide the imitation learning process by leveraging vision transformers for video-based comparisons by calculating pair-wise distance between video embeddings. Along with video-encoding distance, we also use a computed similarity between segmented video frames as a guidance reward. We validate our method on locomotion tasks involving unique body configurations. In humanoid robot locomotion tasks, we demonstrate that 'No-data Imitation Learning' (NIL) outperforms baselines trained on 3D motion-capture data. Our results highlight the potential of leveraging generative video models for physically plausible skill learning with diverse morphologies, effectively replacing data collection with data generation for imitation learning.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/VAE3A2R6/Albaba et al. - 2025 - NIL No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models.pdf;/Users/fangyuan/Zotero/storage/SCL58HIR/2503.html}
}

@article{andreasModularMultitaskReinforcement,
  title = {Modular {{Multitask Reinforcement Learning}} with {{Policy Sketches}}},
  author = {Andreas, Jacob and Klein, Dan and Levine, Sergey},
  pages = {10},
  abstract = {We describe a framework for multitask deep reinforcement learning guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them---specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g. intermediate rewards, subtask completion signals, or intrinsic motivations). To learn from sketches, we present a model that associates every subtask with a modular subpolicy, and jointly maximizes reward over full task-specific policies by tying parameters across shared subpolicies. Optimization is accomplished via a decoupled actor--critic training objective that facilitates learning common behaviors from multiple dissimilar reward functions. We evaluate the effectiveness of our approach in three environments featuring both discrete and continuous control, and with sparse rewards that can be obtained only after completing a number of high-level subgoals. Experiments show that using our approach to learn policies guided by sketches gives better performance than existing techniques for learning task-specific or shared policies, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/XW3ZB4HA/Andreas et al. - Modular Multitask Reinforcement Learning with Poli.pdf}
}

@article{andrychowiczHindsightExperienceReplay,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Crow, Dwight and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
  pages = {11},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/67LSTLEE/Andrychowicz et al. - Hindsight Experience Replay.pdf}
}

@misc{argenzianoEMPOWEREmbodiedMultirole2024,
  title = {{{EMPOWER}}: {{Embodied Multi-role Open-vocabulary Planning}} with {{Online Grounding}} and {{Execution}}},
  shorttitle = {{{EMPOWER}}},
  author = {Argenziano, Francesco and Brienza, Michele and Suriani, Vincenzo and Nardi, Daniele and Bloisi, Domenico D.},
  year = {2024},
  month = aug,
  number = {arXiv:2408.17379},
  eprint = {2408.17379},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-03},
  abstract = {Task planning for robots in real-life settings presents significant challenges. These challenges stem from three primary issues: the difficulty in identifying grounded sequences of steps to achieve a goal; the lack of a standardized mapping between high-level actions and low-level commands; and the challenge of maintaining low computational overhead given the limited resources of robotic hardware. We introduce EMPOWER, a framework designed for open-vocabulary online grounding and planning for embodied agents aimed at addressing these issues. By leveraging efficient pre-trained foundation models and a multi-role mechanism, EMPOWER demonstrates notable improvements in grounded planning and execution. Quantitative results highlight the effectiveness of our approach, achieving an average success rate of 0.73 across six different real-life scenarios using a TIAGo robot.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/ZEZG8G5S/Argenziano et al. - 2024 - EMPOWER Embodied Multi-role Open-vocabulary Planning with Online Grounding and Execution.pdf}
}

@book{axlerLinearAlgebraDone2015,
  title = {Linear {{Algebra Done Right}}},
  author = {Axler, Sheldon},
  year = {2015},
  series = {Undergraduate {{Texts}} in {{Mathematics}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-11080-6},
  urldate = {2022-08-27},
  isbn = {978-3-319-11079-0 978-3-319-11080-6},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/H9I5R6ZJ/Axler - 2015 - Linear Algebra Done Right.pdf}
}

@misc{bahlHumantoRobotImitationWild2022,
  title = {Human-to-{{Robot Imitation}} in the {{Wild}}},
  author = {Bahl, Shikhar and Gupta, Abhinav and Pathak, Deepak},
  year = {2022},
  month = jul,
  eprint = {2207.09450},
  primaryclass = {cs, eess},
  urldate = {2022-07-21},
  abstract = {We approach the problem of learning by watching humans in the wild. While traditional approaches in Imitation and Reinforcement Learning are promising for learning in the real world, they are either sample inefficient or are constrained to lab settings. Meanwhile, there has been a lot of success in processing passive, unstructured human data. We propose tackling this problem via an efficient one-shot robot learning algorithm, centered around learning from a third-person perspective. We call our method WHIRL: In-the-Wild Human Imitating Robot Learning. WHIRL extracts a prior over the intent of the human demonstrator, using it to initialize our agent's policy. We introduce an efficient real-world policy learning scheme that improves using interactions. Our key contributions are a simple sampling-based policy optimization approach, a novel objective function for aligning human and robot videos as well as an exploration method to boost sample efficiency. We show one-shot generalization and success in real-world settings, including 20 different manipulation tasks in the wild. Videos and talk at https://human2robot.github.io},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/Z4D4LLND/Bahl et al. - 2022 - Human-to-Robot Imitation in the Wild.pdf}
}

@misc{ballEfficientOnlineReinforcement2023,
  title = {Efficient {{Online Reinforcement Learning}} with {{Offline Data}}},
  author = {Ball, Philip J. and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  year = {2023},
  month = feb,
  number = {arXiv:2302.02948},
  eprint = {2302.02948},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.02948},
  urldate = {2023-02-13},
  abstract = {Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a \${\textbackslash}mathbf\{2.5{\textbackslash}times\}\$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/8GB3C8MN/Ball et al. - 2023 - Efficient Online Reinforcement Learning with Offli.pdf;/Users/fangyuan/Zotero/storage/C2CRQNTD/2302.html}
}

@misc{baoHandsOnVLMVisionLanguageModels2024,
  title = {{{HandsOnVLM}}: {{Vision-Language Models}} for {{Hand-Object Interaction Prediction}}},
  shorttitle = {{{HandsOnVLM}}},
  author = {Bao, Chen and Xu, Jiarui and Wang, Xiaolong and Gupta, Abhinav and Bharadhwaj, Homanga},
  year = {2024},
  month = dec,
  number = {arXiv:2412.13187},
  eprint = {2412.13187},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.13187},
  urldate = {2024-12-18},
  abstract = {How can we predict future interaction trajectories of human hands in a scene given high-level colloquial task specifications in the form of natural language? In this paper, we extend the classic hand trajectory prediction task to two tasks involving explicit or implicit language queries. Our proposed tasks require extensive understanding of human daily activities and reasoning abilities about what should be happening next given cues from the current scene. We also develop new benchmarks to evaluate the proposed two tasks, Vanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We enable solving these tasks by integrating high-level world knowledge and reasoning capabilities of Vision-Language Models (VLMs) with the auto-regressive nature of low-level ego-centric hand trajectories. Our model, HandsOnVLM is a novel VLM that can generate textual responses and produce future hand trajectories through natural-language conversations. Our experiments show that HandsOnVLM outperforms existing task-specific methods and other VLM baselines on proposed tasks, and demonstrates its ability to effectively utilize world knowledge for reasoning about low-level human hand trajectories based on the provided context. Our website contains code and detailed video results {\textbackslash}url\{https://www.chenbao.tech/handsonvlm/\}},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/FJL5G9G8/Bao et al. - 2024 - HandsOnVLM Vision-Language Models for Hand-Object Interaction Prediction.pdf;/Users/fangyuan/Zotero/storage/U5U5L9I9/2412.html}
}

@misc{benHOMIEHumanoidLocoManipulation2025,
  title = {{{HOMIE}}: {{Humanoid Loco-Manipulation}} with {{Isomorphic Exoskeleton Cockpit}}},
  shorttitle = {{{HOMIE}}},
  author = {Ben, Qingwei and Jia, Feiyu and Zeng, Jia and Dong, Junting and Lin, Dahua and Pang, Jiangmiao},
  year = {2025},
  month = feb,
  number = {arXiv:2502.13013},
  eprint = {2502.13013},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13013},
  urldate = {2025-02-20},
  abstract = {Current humanoid teleoperation systems either lack reliable low-level control policies, or struggle to acquire accurate whole-body control commands, making it difficult to teleoperate humanoids for loco-manipulation tasks. To solve these issues, we propose HOMIE, a novel humanoid teleoperation cockpit integrates a humanoid loco-manipulation policy and a low-cost exoskeleton-based hardware system. The policy enables humanoid robots to walk and squat to specific heights while accommodating arbitrary upper-body poses. This is achieved through our novel reinforcement learning-based training framework that incorporates upper-body pose curriculum, height-tracking reward, and symmetry utilization, without relying on any motion priors. Complementing the policy, the hardware system integrates isomorphic exoskeleton arms, a pair of motion-sensing gloves, and a pedal, allowing a single operator to achieve full control of the humanoid robot. Our experiments show our cockpit facilitates more stable, rapid, and precise humanoid loco-manipulation teleoperation, accelerating task completion and eliminating retargeting errors compared to inverse kinematics-based methods. We also validate the effectiveness of the data collected by our cockpit for imitation learning. Our project is fully open-sourced, demos and code can be found in https://homietele.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/8RA5W2XC/Ben et al. - 2025 - HOMIE Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit.pdf;/Users/fangyuan/Zotero/storage/SGQ2JLU7/2502.html}
}

@misc{black$p_0$VisionLanguageActionFlow2024,
  title = {\${$\pi\_$}0\$: {{A Vision-Language-Action Flow Model}} for {{General Robot Control}}},
  shorttitle = {\${$\pi\_$}0\$},
  author = {Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and Jones, Tim and Ke, Liyiming and Levine, Sergey and {Li-Bell}, Adrian and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Shi, Lucy Xiaoyang and Tanner, James and Vuong, Quan and Walling, Anna and Wang, Haohuan and Zhilinsky, Ury},
  year = {2024},
  month = oct,
  number = {arXiv:2410.24164},
  eprint = {2410.24164},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.24164},
  urldate = {2024-11-01},
  abstract = {Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/QZMHPNHG/Black et al. - 2024 - $π_0$ A Vision-Language-Action Flow Model for General Robot Control.pdf;/Users/fangyuan/Zotero/storage/RR58E6ZP/2410.html}
}

@inproceedings{blackZeroShotRoboticManipulation2023,
  title = {Zero-{{Shot Robotic Manipulation}} with {{Pre-Trained Image-Editing Diffusion Models}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Black, Kevin and Nakamoto, Mitsuhiko and Atreya, Pranav and Walke, Homer Rich and Finn, Chelsea and Kumar, Aviral and Levine, Sergey},
  year = {2023},
  month = oct,
  urldate = {2024-12-07},
  abstract = {If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot's own training data. We propose SuSIE, a method that leverages an image-editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller can accomplish. Specifically, we finetune InstructPix2Pix on video data, consisting of both human videos and robot rollouts, such that it outputs hypothetical future ``subgoal'' observations given the robot's current observation and a language command. We also use the robot data to train a low-level goal-conditioned policy to act as the aforementioned low-level controller. We find that the high-level subgoal predictions can utilize Internet scale pretraining and visual understanding to guide the low-level goal-conditioned policy, achieving significantly better generalization and precision than conventional language-conditioned policies. We achieve state-of-the-art results on the CALVIN benchmark, and also demonstrate robust generalization on real-world manipulation tasks, beating strong baselines that have access to privileged information or that utilize orders of magnitude more compute and training data. The project website can be found at http://rail-berkeley.github.io/susie.},
  langid = {english},
  keywords = {Diffusion,Manipulation,Subgoal},
  file = {/Users/fangyuan/Zotero/storage/ZUC7LC93/Black et al. - 2023 - Zero-Shot Robotic Manipulation with Pre-Trained Image-Editing Diffusion Models.pdf}
}

@misc{brohanRT1RoboticsTransformer2022,
  title = {{{RT-1}}: {{Robotics Transformer}} for {{Real-World Control}} at {{Scale}}},
  shorttitle = {{{RT-1}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil J. and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  year = {2022},
  month = dec,
  number = {arXiv:2212.06817},
  eprint = {2212.06817},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.06817},
  urldate = {2022-12-15},
  abstract = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer.github.io},
  archiveprefix = {arXiv},
  keywords = {Manipulation,Mobile Manipulation,Robotic Foundation Model},
  file = {/Users/fangyuan/Zotero/storage/L4G8FUL2/Brohan et al. - 2022 - RT-1 Robotics Transformer for Real-World Control .pdf;/Users/fangyuan/Zotero/storage/UIP3792N/2212.html}
}

@article{brohanRT2VisionLanguageActionModels,
  title = {{{RT-2}}: {{Vision-Language-Action Models Transfer Web Knowledge}} to {{Robotic Control}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and Florence, Pete and Fu, Chuyuan and Arenas, Montse Gonzalez and Gopalakrishnan, Keerthana and Han, Kehang and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ichter, Brian and Irpan, Alex and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Lisa and Lee, Tsang-Wei Edward and Levine, Sergey and Lu, Yao and Michalewski, Henryk and Mordatch, Igor and Pertsch, Karl and Rao, Kanishka and Reymann, Krista and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sermanet, Pierre and Singh, Jaspiar and Singh, Anikait and Soricut, Radu and Tran, Huong and Vanhoucke, Vincent and Vuong, Quan and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Wu, Jialin and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  langid = {english},
  keywords = {Manipulation,Mobile Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/YFYA8ZTV/Brohan et al. - RT-2 Vision-Language-Action Models Transfer Web K.pdf}
}

@misc{calvertBehaviorArchitectureFast2024,
  title = {A {{Behavior Architecture}} for {{Fast Humanoid Robot Door Traversals}}},
  author = {Calvert, Duncan and Penco, Luigi and Anderson, Dexton and Bialek, Tomasz and Chatterjee, Arghya and Mishra, Bhavyansh and Clark, Geoffrey and Bertrand, Sylvain and Griffin, Robert},
  year = {2024},
  month = nov,
  number = {arXiv:2411.03532},
  eprint = {2411.03532},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.03532},
  urldate = {2024-12-06},
  abstract = {Towards the role of humanoid robots as squad mates in urban operations and other domains, we identified doors as a major area lacking capability development. In this paper, we focus on the ability of humanoid robots to navigate and deal with doors. Human-sized doors are ubiquitous in many environment domains and the humanoid form factor is uniquely suited to operate and traverse them. We present an architecture which incorporates GPU accelerated perception and a tree based interactive behavior coordination system with a whole body motion and walking controller. Our system is capable of performing door traversals on a variety of door types. It supports rapid authoring of behaviors for unseen door types and techniques to achieve re-usability of those authored behaviors. The behaviors are modelled using trees and feature logical reactivity and action sequences that can be executed with layered concurrency to increase speed. Primitive actions are built on top of our existing whole body controller which supports manipulation while walking. We include a perception system using both neural networks and classical computer vision for door mechanism detection outside of the lab environment. We present operator-robot interdependence analysis charts to explore how human cognition is combined with artificial intelligence to produce complex robot behavior. Finally, we present and discuss real robot performances of fast door traversals on our Nadia humanoid robot. Videos online at https://www.youtube.com/playlist?list=PLXuyT8w3JVgMPaB5nWNRNHtqzRK8i68dy.},
  archiveprefix = {arXiv},
  keywords = {Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/6USCRYR7/Calvert et al. - 2024 - A Behavior Architecture for Fast Humanoid Robot Door Traversals.pdf;/Users/fangyuan/Zotero/storage/IFJ9N6XD/2411.html}
}

@misc{camperoLearningAMIGoAdversarially2021,
  title = {Learning with {{AMIGo}}: {{Adversarially Motivated Intrinsic Goals}}},
  shorttitle = {Learning with {{AMIGo}}},
  author = {Campero, Andres and Raileanu, Roberta and K{\"u}ttler, Heinrich and Tenenbaum, Joshua B. and Rockt{\"a}schel, Tim and Grefenstette, Edward},
  year = {2021},
  month = feb,
  number = {arXiv:2006.12122},
  eprint = {2006.12122},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-11-18},
  abstract = {A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGO, a novel agent incorporating---as form of meta-learning---a goal-generating teacher that proposes Adversarially Motivated Intrinsic GOals to train a goal-conditioned ``student'' policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective ``constructively adversarial'' objective, the teacher learns to propose increasingly challenging---yet achievable---goals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/TI2SG2GS/Campero et al. - 2021 - Learning with AMIGo Adversarially Motivated Intri.pdf}
}

@article{cariusMPCNetFirstPrinciples2020,
  title = {{{MPC-Net}}: {{A First Principles Guided Policy Search}}},
  shorttitle = {{{MPC-Net}}},
  author = {Carius, Jan and Farshidian, Farbod and Hutter, Marco},
  year = {2020},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {2},
  eprint = {1909.05197},
  primaryclass = {cs},
  pages = {2897--2904},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2020.2974653},
  urldate = {2022-09-17},
  abstract = {We present an Imitation Learning approach for the control of dynamical systems with a known model. Our policy search method is guided by solutions from MPC. Typical policy search methods of this kind minimize a distance metric between the guiding demonstrations and the learned policy. Our loss function, however, corresponds to the minimization of the control Hamiltonian, which derives from the principle of optimality. Therefore, our algorithm directly attempts to solve the optimality conditions with a parameterized class of control laws. Additionally, the proposed loss function explicitly encodes the constraints of the optimal control problem and we provide numerical evidence that its minimization achieves improved constraint satisfaction. We train a mixture-of-expert neural network architecture for controlling a quadrupedal robot and show that this policy structure is well suited for such multimodal systems. The learned policy can successfully stabilize different gaits on the real walking robot from less than 10 min of demonstration data.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/MQXYCJ34/Carius et al. - 2020 - MPC-Net A First Principles Guided Policy Search.pdf}
}

@misc{ceolaLHManipDatasetLongHorizon2024,
  title = {{{LHManip}}: {{A Dataset}} for {{Long-Horizon Language-Grounded Manipulation Tasks}} in {{Cluttered Tabletop Environments}}},
  shorttitle = {{{LHManip}}},
  author = {Ceola, Federico and Natale, Lorenzo and S{\"u}nderhauf, Niko and Rana, Krishan},
  year = {2024},
  month = jul,
  number = {arXiv:2312.12036},
  eprint = {2312.12036},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.12036},
  urldate = {2025-04-23},
  abstract = {Instructing a robot to complete an everyday task within our homes has been a long-standing challenge for robotics. While recent progress in language-conditioned imitation learning and offline reinforcement learning has demonstrated impressive performance across a wide range of tasks, they are typically limited to short-horizon tasks -- not reflective of those a home robot would be expected to complete. While existing architectures have the potential to learn these desired behaviours, the lack of the necessary long-horizon, multi-step datasets for real robotic systems poses a significant challenge. To this end, we present the Long-Horizon Manipulation (LHManip) dataset comprising 200 episodes, demonstrating 20 different manipulation tasks via real robot teleoperation. The tasks entail multiple sub-tasks, including grasping, pushing, stacking and throwing objects in highly cluttered environments. Each task is paired with a natural language instruction and multi-camera viewpoints for point-cloud or NeRF reconstruction. In total, the dataset comprises 176,278 observation-action pairs which form part of the Open X-Embodiment dataset. The full LHManip dataset is made publicly available at https://github.com/fedeceola/LHManip.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/5234M7TC/Ceola et al. - 2024 - LHManip A Dataset for Long-Horizon Language-Grounded Manipulation Tasks in Cluttered Tabletop Envir.pdf;/Users/fangyuan/Zotero/storage/72TK4YJH/2312.html}
}

@misc{chane-saneGoalConditionedReinforcementLearning2021,
  title = {Goal-{{Conditioned Reinforcement Learning}} with {{Imagined Subgoals}}},
  author = {{Chane-Sane}, Elliot and Schmid, Cordelia and Laptev, Ivan},
  year = {2021},
  month = jul,
  number = {arXiv:2107.00541},
  eprint = {2107.00541},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.00541},
  urldate = {2022-11-22},
  abstract = {Goal-conditioned reinforcement learning endows an agent with a large variety of skills, but it often struggles to solve tasks that require more temporally extended reasoning. In this work, we propose to incorporate imagined subgoals into policy learning to facilitate learning of complex tasks. Imagined subgoals are predicted by a separate high-level policy, which is trained simultaneously with the policy and its critic. This high-level policy predicts intermediate states halfway to the goal using the value function as a reachability metric. We don't require the policy to reach these subgoals explicitly. Instead, we use them to define a prior policy, and incorporate this prior into a KL-constrained policy iteration scheme to speed up and regularize learning. Imagined subgoals are used during policy learning, but not during test time, where we only apply the learned policy. We evaluate our approach on complex robotic navigation and manipulation tasks and show that it outperforms existing methods by a large margin.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/VFWE25AS/Chane-Sane et al. - 2021 - Goal-Conditioned Reinforcement Learning with Imagi.pdf;/Users/fangyuan/Zotero/storage/UY65GDKG/2107.html}
}

@article{chebotarQTransformerScalableOffline,
  title = {Q-{{Transformer}}: {{Scalable Offline Reinforcement Learning}} via {{Autoregressive Q-Functions}}},
  author = {Chebotar, Yevgen and Vuong, Quan and Irpan, Alex and Hausman, Karol and Xia, Fei and Lu, Yao and Kumar, Aviral and Yu, Tianhe and Herzog, Alexander and Pertsch, Karl and Gopalakrishnan, Keerthana and Ibarz, Julian and Nachum, Ofir and Salazar, Grecia and Tran, Huong T and Peralta, Jodilyn and Tan, Clayton and Manjunath, Deeksha and Singht, Jaspiar and Zitkovich, Brianna and Jackson, Tomas and Rao, Kanishka and Finn, Chelsea and Levine, Sergey},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/R27RKWSJ/Chebotar et al. - Q-Transformer Scalable Ofﬂine Reinforcement Learn.pdf}
}

@article{chenAutomaticDiscoverySubgoals,
  title = {Automatic {{Discovery}} of {{Subgoals}} for {{Sequential Decision Problems Using Potential Fields}}},
  author = {Chen, Huanwen and Yin, Changming and Xie, Lijuan},
  pages = {8},
  abstract = {This paper presents a new method by which a sequential decision agent can automatically discover subgoals online. The agent discovers subgoals using potential field. The method uses a reward function to generate a potential field, and then abstracts some features from the potential field as candidates of subgoals. Based on the candidates, the agent can determine its behaviors online through some heuristics in unknown environment. The best-known and most often-cited problem with the potential field method is local minima. But our method does not have this limitation because the local minima are used to form subgoals. The disadvantage of the local minima in the previous approaches of potential field turns out to be an advantage in our method. We illustrate the method using a simple gridworld task.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/TN6MLI77/Chen et al. - Automatic Discovery of Subgoals for Sequential Dec.pdf}
}

@misc{chenAutoTAMPAutoregressiveTask2024,
  title = {{{AutoTAMP}}: {{Autoregressive Task}} and {{Motion Planning}} with {{LLMs}} as {{Translators}} and {{Checkers}}},
  shorttitle = {{{AutoTAMP}}},
  author = {Chen, Yongchao and Arkin, Jacob and Dawson, Charles and Zhang, Yang and Roy, Nicholas and Fan, Chuchu},
  year = {2024},
  month = mar,
  number = {arXiv:2306.06531},
  eprint = {2306.06531},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-28},
  abstract = {For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to jointly solve the task and motion plan. To improve translation, we automatically detect and correct both syntactic and semantic errors via autoregressive re-prompting, resulting in significant improvements in task completion. We show that our approach outperforms several methods using LLMs as planners in complex task domains. See our project website{\S} for prompts, videos, and code.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/EJWAYEDZ/Chen et al. - 2024 - AutoTAMP Autoregressive Task and Motion Planning with LLMs as Translators and Checkers.pdf}
}

@misc{chenConRFTReinforcedFinetuning2025,
  title = {{{ConRFT}}: {{A Reinforced Fine-tuning Method}} for {{VLA Models}} via {{Consistency Policy}}},
  shorttitle = {{{ConRFT}}},
  author = {Chen, Yuhui and Tian, Shuai and Liu, Shugao and Zhou, Yingting and Li, Haoran and Zhao, Dongbin},
  year = {2025},
  month = feb,
  number = {arXiv:2502.05450},
  eprint = {2502.05450},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.05450},
  urldate = {2025-02-14},
  abstract = {Vision-Language-Action (VLA) models have shown substantial potential in real-world robotic manipulation. However, fine-tuning these models through supervised learning struggles to achieve robust performance due to limited, inconsistent demonstrations, especially in contact-rich environments. In this paper, we propose a reinforced fine-tuning approach for VLA models, named ConRFT, which consists of offline and online fine-tuning with a unified consistency-based training objective, to address these challenges. In the offline stage, our method integrates behavior cloning and Q-learning to effectively extract policy from a small set of demonstrations and stabilize value estimating. In the online stage, the VLA model is further fine-tuned via consistency policy, with human interventions to ensure safe exploration and high sample efficiency. We evaluate our approach on eight diverse real-world manipulation tasks. It achieves an average success rate of 96.3\% within 45-90 minutes of online fine-tuning, outperforming prior supervised methods with a 144\% improvement in success rate and 1.9x shorter episode length. This work highlights the potential of integrating reinforcement learning to enhance the performance of VLA models for real-world robotic applications.},
  archiveprefix = {arXiv},
  keywords = {Manipulation},
  file = {/Users/fangyuan/Zotero/storage/IQ686P9Z/Chen et al. - 2025 - ConRFT A Reinforced Fine-tuning Method for VLA Models via Consistency Policy.pdf;/Users/fangyuan/Zotero/storage/MQ6KISDN/2502.html}
}

@misc{chenDexterousSafeControl2025,
  title = {Dexterous {{Safe Control}} for {{Humanoids}} in {{Cluttered Environments}} via {{Projected Safe Set Algorithm}}},
  author = {Chen, Rui and Sun, Yifan and Liu, Changliu},
  year = {2025},
  month = feb,
  number = {arXiv:2502.02858},
  eprint = {2502.02858},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.02858},
  urldate = {2025-02-07},
  abstract = {It is critical to ensure safety for humanoid robots in real-world applications without compromising performance. In this paper, we consider the problem of dexterous safety, featuring limb-level geometry constraints for avoiding both external and self-collisions in cluttered environments. Compared to safety with simplified bounding geometries in sprase environments, dexterous safety produces numerous constraints which often lead to infeasible constraint sets when solving for safe robot control. To address this issue, we propose Projected Safe Set Algorithm (p-SSA), an extension of classical safe control algorithms to multi-constraint cases. p-SSA relaxes conflicting constraints in a principled manner, minimizing safety violations to guarantee feasible robot control. We verify our approach in simulation and on a real Unitree G1 humanoid robot performing complex collision avoidance tasks. Results show that p-SSA enables the humanoid to operate robustly in challenging situations with minimal safety violations and directly generalizes to various tasks with zero parameter tuning.},
  archiveprefix = {arXiv},
  keywords = {Dexterous Hand,Manipulation,Safety},
  file = {/Users/fangyuan/Zotero/storage/YNT6NJRS/Chen et al. - 2025 - Dexterous Safe Control for Humanoids in Cluttered Environments via Projected Safe Set Algorithm.pdf;/Users/fangyuan/Zotero/storage/489Q22IY/2502.html}
}

@misc{chenDiffusionAutoencodersAre2025,
  title = {Diffusion {{Autoencoders}} Are {{Scalable Image Tokenizers}}},
  author = {Chen, Yinbo and Girdhar, Rohit and Wang, Xiaolong and Rambhatla, Sai Saketh and Misra, Ishan},
  year = {2025},
  month = jan,
  number = {arXiv:2501.18593},
  eprint = {2501.18593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.18593},
  urldate = {2025-02-01},
  abstract = {Tokenizing images into compact visual representations is a key step in learning efficient and high-quality image generative models. We present a simple diffusion tokenizer (DiTo) that learns compact visual representations for image generation models. Our key insight is that a single learning objective, diffusion L2 loss, can be used for training scalable image tokenizers. Since diffusion is already widely used for image generation, our insight greatly simplifies training such tokenizers. In contrast, current state-of-the-art tokenizers rely on an empirically found combination of heuristics and losses, thus requiring a complex training recipe that relies on non-trivially balancing different losses and pretrained supervised models. We show design decisions, along with theoretical grounding, that enable us to scale DiTo for learning competitive image representations. Our results show that DiTo is a simpler, scalable, and self-supervised alternative to the current state-of-the-art image tokenizer which is supervised. DiTo achieves competitive or better quality than state-of-the-art in image reconstruction and downstream image generation tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/ALG3Z2MT/Chen et al. - 2025 - Diffusion Autoencoders are Scalable Image Tokenizers.pdf;/Users/fangyuan/Zotero/storage/YJ9B73Q9/2501.html}
}

@misc{chenDontStartScratch2024,
  title = {Don't {{Start}} from {{Scratch}}: {{Behavioral Refinement}} via {{Interpolant-based Policy Diffusion}}},
  shorttitle = {Don't {{Start}} from {{Scratch}}},
  author = {Chen, Kaiqi and Lim, Eugene and Lin, Kelvin and Chen, Yiyang and Soh, Harold},
  year = {2024},
  month = may,
  number = {arXiv:2402.16075},
  eprint = {2402.16075},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.16075},
  urldate = {2024-05-20},
  abstract = {Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to mitigate the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning. It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available. In experiments on challenging simulation benchmarks and on real robots, BRIDGER outperforms state-of-the-art diffusion policies. We provide further analysis on design considerations when applying BRIDGER.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/FH4G8VDI/Chen et al. - 2024 - Don't Start from Scratch Behavioral Refinement vi.pdf;/Users/fangyuan/Zotero/storage/TEVY3YB8/2402.html}
}

@misc{chengExpressiveWholeBodyControl2024,
  title = {Expressive {{Whole-Body Control}} for {{Humanoid Robots}}},
  author = {Cheng, Xuxin and Ji, Yandong and Chen, Junming and Yang, Ruihan and Yang, Ge and Wang, Xiaolong},
  year = {2024},
  month = mar,
  number = {arXiv:2402.16796},
  eprint = {2402.16796},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.16796},
  urldate = {2024-12-06},
  abstract = {Can we enable humanoid robots to generate rich, diverse, and expressive motions in the real world? We propose to learn a whole-body control policy on a human-sized robot to mimic human motions as realistic as possible. To train such a policy, we leverage the large-scale human motion capture data from the graphics community in a Reinforcement Learning framework. However, directly performing imitation learning with the motion capture dataset would not work on the real humanoid robot, given the large gap in degrees of freedom and physical capabilities. Our method Expressive Whole-Body Control (Exbody) tackles this problem by encouraging the upper humanoid body to imitate a reference motion, while relaxing the imitation constraint on its two legs and only requiring them to follow a given velocity robustly. With training in simulation and Sim2Real transfer, our policy can control a humanoid robot to walk in different styles, shake hands with humans, and even dance with a human in the real world. We conduct extensive studies and comparisons on diverse motions in both simulation and the real world to show the effectiveness of our approach.},
  archiveprefix = {arXiv},
  keywords = {Unitree,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/9VTPQR7S/Cheng et al. - 2024 - Expressive Whole-Body Control for Humanoid Robots.pdf;/Users/fangyuan/Zotero/storage/KTZ3UXVM/2402.html}
}

@misc{chengNaVILALeggedRobot2024,
  title = {{{NaVILA}}: {{Legged Robot Vision-Language-Action Model}} for {{Navigation}}},
  shorttitle = {{{NaVILA}}},
  author = {Cheng, An-Chieh and Ji, Yandong and Yang, Zhaojing and Zou, Xueyan and Kautz, Jan and B{\i}y{\i}k, Erdem and Yin, Hongxu and Liu, Sifei and Wang, Xiaolong},
  year = {2024},
  month = dec,
  number = {arXiv:2412.04453},
  eprint = {2412.04453},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.04453},
  urldate = {2024-12-06},
  abstract = {This paper proposes to solve the problem of Vision-and-Language Navigation with legged robots, which not only provides a flexible way for humans to command but also allows the robot to navigate through more challenging and cluttered scenes. However, it is non-trivial to translate human language instructions all the way to low-level leg joint actions. We propose NaVILA, a 2-level framework that unifies a Vision-Language-Action model (VLA) with locomotion skills. Instead of directly predicting low-level actions from VLA, NaVILA first generates mid-level actions with spatial information in the form of language, (e.g., "moving forward 75cm"), which serves as an input for a visual locomotion RL policy for execution. NaVILA substantially improves previous approaches on existing benchmarks. The same advantages are demonstrated in our newly developed benchmarks with IsaacLab, featuring more realistic scenes, low-level controls, and real-world robot experiments. We show more results at https://navila-bot.github.io/},
  archiveprefix = {arXiv},
  keywords = {Navigation,Unitree,VLA},
  file = {/Users/fangyuan/Zotero/storage/K6DV7W3E/Cheng et al. - 2024 - NaVILA Legged Robot Vision-Language-Action Model for Navigation.pdf;/Users/fangyuan/Zotero/storage/IAWWZQD7/2412.html}
}

@misc{chengOpenTeleVisionTeleoperationImmersive2024,
  title = {Open-{{TeleVision}}: {{Teleoperation}} with {{Immersive Active Visual Feedback}}},
  shorttitle = {Open-{{TeleVision}}},
  author = {Cheng, Xuxin and Li, Jialong and Yang, Shiqi and Yang, Ge and Wang, Xiaolong},
  year = {2024},
  month = jul,
  number = {arXiv:2407.01512},
  eprint = {2407.01512},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.01512},
  urldate = {2024-12-06},
  abstract = {Teleoperation serves as a powerful method for collecting on-robot data essential for robot learning from demonstrations. The intuitiveness and ease of use of the teleoperation system are crucial for ensuring high-quality, diverse, and scalable data. To achieve this, we propose an immersive teleoperation system Open-TeleVision that allows operators to actively perceive the robot's surroundings in a stereoscopic manner. Additionally, the system mirrors the operator's arm and hand movements on the robot, creating an immersive experience as if the operator's mind is transmitted to a robot embodiment. We validate the effectiveness of our system by collecting data and training imitation learning policies on four long-horizon, precise tasks (Can Sorting, Can Insertion, Folding, and Unloading) for 2 different humanoid robots and deploy them in the real world. The system is open-sourced at: https://robot-tv.github.io/},
  archiveprefix = {arXiv},
  keywords = {FFTAI,Manipulation,Teleoperation,Vision Pro},
  file = {/Users/fangyuan/Zotero/storage/DH8WNM5H/Cheng et al. - 2024 - Open-TeleVision Teleoperation with Immersive Active Visual Feedback.pdf;/Users/fangyuan/Zotero/storage/C8BPSGS6/2407.html}
}

@misc{chenLearningGeneralizableRobotic2021,
  title = {Learning {{Generalizable Robotic Reward Functions}} from "{{In-The-Wild}}" {{Human Videos}}},
  author = {Chen, Annie S. and Nair, Suraj and Finn, Chelsea},
  year = {2021},
  month = mar,
  eprint = {2103.16817},
  primaryclass = {cs},
  urldate = {2022-07-19},
  abstract = {We are motivated by the goal of generalist robots that can complete a wide range of tasks across many environments. Critical to this is the robot's ability to acquire some metric of task success or reward, which is necessary for reinforcement learning, planning, or knowing when to ask for help. For a general-purpose robot operating in the real world, this reward function must also be able to generalize broadly across environments, tasks, and objects, while depending only on on-board sensor observations (e.g. RGB images). While deep learning on large and diverse datasets has shown promise as a path towards such generalization in computer vision and natural language, collecting high quality datasets of robotic interaction at scale remains an open challenge. In contrast, "in-the-wild" videos of humans (e.g. YouTube) contain an extensive collection of people doing interesting tasks across a diverse range of settings. In this work, we propose a simple approach, Domain-agnostic Video Discriminator (DVD), that learns multitask reward functions by training a discriminator to classify whether two videos are performing the same task, and can generalize by virtue of learning from a small amount of robot data with a broad dataset of human videos. We find that by leveraging diverse human datasets, this reward function (a) can generalize zero shot to unseen environments, (b) generalize zero shot to unseen tasks, and (c) can be combined with visual model predictive control to solve robotic manipulation tasks on a real WidowX200 robot in an unseen environment from a single human demo.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/WR9ZWAZ8/Chen et al. - 2021 - Learning Generalizable Robotic Reward Functions from In-The-Wild Human Videos.pdf}
}

@misc{chenLearningSmoothHumanoid2024,
  title = {Learning {{Smooth Humanoid Locomotion}} through {{Lipschitz-Constrained Policies}}},
  author = {Chen, Zixuan and He, Xialin and Wang, Yen-Jen and Liao, Qiayuan and Ze, Yanjie and Li, Zhongyu and Sastry, S. Shankar and Wu, Jiajun and Sreenath, Koushil and Gupta, Saurabh and Peng, Xue Bin},
  year = {2024},
  month = oct,
  number = {arXiv:2410.11825},
  eprint = {2410.11825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.11825},
  urldate = {2024-12-06},
  abstract = {Reinforcement learning combined with sim-to-real transfer offers a general framework for developing locomotion controllers for legged robots. To facilitate successful deployment in the real world, smoothing techniques, such as low-pass filters and smoothness rewards, are often employed to develop policies with smooth behaviors. However, because these techniques are non-differentiable and usually require tedious tuning of a large set of hyperparameters, they tend to require extensive manual tuning for each robotic platform. To address this challenge and establish a general technique for enforcing smooth behaviors, we propose a simple and effective method that imposes a Lipschitz constraint on a learned policy, which we refer to as Lipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can be implemented in the form of a gradient penalty, which provides a differentiable objective that can be easily incorporated with automatic differentiation frameworks. We demonstrate that LCP effectively replaces the need for smoothing rewards or low-pass filters and can be easily integrated into training frameworks for many distinct humanoid robots. We extensively evaluate LCP in both simulation and real-world humanoid robots, producing smooth and robust locomotion controllers. All simulation and deployment code, along with complete checkpoints, is available on our project page: https://lipschitz-constrained-policy.github.io.},
  archiveprefix = {arXiv},
  keywords = {FFTAI,Locomotion},
  file = {/Users/fangyuan/Zotero/storage/MVK39NBL/Chen et al. - 2024 - Learning Smooth Humanoid Locomotion through Lipschitz-Constrained Policies.pdf;/Users/fangyuan/Zotero/storage/EEVGACET/2410.html}
}

@misc{chenObjectCentricDexterousManipulation2024,
  title = {Object-{{Centric Dexterous Manipulation}} from {{Human Motion Data}}},
  author = {Chen, Yuanpei and Wang, Chen and Yang, Yaodong and Liu, C. Karen},
  year = {2024},
  month = nov,
  number = {arXiv:2411.04005},
  eprint = {2411.04005},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.04005},
  urldate = {2024-12-06},
  abstract = {Manipulating objects to achieve desired goal states is a basic but important skill for dexterous manipulation. Human hand motions demonstrate proficient manipulation capability, providing valuable data for training robots with multi-finger hands. Despite this potential, substantial challenges arise due to the embodiment gap between human and robot hands. In this work, we introduce a hierarchical policy learning framework that uses human hand motion data for training object-centric dexterous robot manipulation. At the core of our method is a high-level trajectory generative model, learned with a large-scale human hand motion capture dataset, to synthesize human-like wrist motions conditioned on the desired object goal states. Guided by the generated wrist motions, deep reinforcement learning is further used to train a low-level finger controller that is grounded in the robot's embodiment to physically interact with the object to achieve the goal. Through extensive evaluation across 10 household objects, our approach not only demonstrates superior performance but also showcases generalization capability to novel object geometries and goal states. Furthermore, we transfer the learned policies from simulation to a real-world bimanual dexterous robot system, further demonstrating its applicability in real-world scenarios. Project website: https://cypypccpy.github.io/obj-dex.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Dexterous Hand,Manipulation},
  file = {/Users/fangyuan/Zotero/storage/TTCBPQDZ/Chen et al. - 2024 - Object-Centric Dexterous Manipulation from Human Motion Data.pdf;/Users/fangyuan/Zotero/storage/RGHQEUUC/2411.html}
}

@misc{chenRHINOLearningRealTime2025,
  title = {{{RHINO}}: {{Learning Real-Time Humanoid-Human-Object Interaction}} from {{Human Demonstrations}}},
  shorttitle = {{{RHINO}}},
  author = {Chen, Jingxiao and Li, Xinyao and Cao, Jiahang and Zhu, Zhengbang and Dong, Wentao and Liu, Minghuan and Wen, Ying and Yu, Yong and Zhang, Liqing and Zhang, Weinan},
  year = {2025},
  month = feb,
  number = {arXiv:2502.13134},
  eprint = {2502.13134},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13134},
  urldate = {2025-02-20},
  abstract = {Humanoid robots have shown success in locomotion and manipulation. Despite these basic abilities, humanoids are still required to quickly understand human instructions and react based on human interaction signals to become valuable assistants in human daily life. Unfortunately, most existing works only focus on multi-stage interactions, treating each task separately, and neglecting real-time feedback. In this work, we aim to empower humanoid robots with real-time reaction abilities to achieve various tasks, allowing human to interrupt robots at any time, and making robots respond to humans immediately. To support such abilities, we propose a general humanoid-human-object interaction framework, named RHINO, i.e., Real-time Humanoid-human Interaction and Object manipulation. RHINO provides a unified view of reactive motion, instruction-based manipulation, and safety concerns, over multiple human signal modalities, such as languages, images, and motions. RHINO is a hierarchical learning framework, enabling humanoids to learn reaction skills from human-human-object demonstrations and teleoperation data. In particular, it decouples the interaction process into two levels: 1) a high-level planner inferring human intentions from real-time human behaviors; and 2) a low-level controller achieving reactive motion behaviors and object manipulation skills based on the predicted intentions. We evaluate the proposed framework on a real humanoid robot and demonstrate its effectiveness, flexibility, and safety in various scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/IW2HU2NH/Chen et al. - 2025 - RHINO Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations.pdf;/Users/fangyuan/Zotero/storage/XAHC83YU/2502.html}
}

@misc{chernyadevBiGymDemoDrivenMobile2024,
  title = {{{BiGym}}: {{A Demo-Driven Mobile Bi-Manual Manipulation Benchmark}}},
  shorttitle = {{{BiGym}}},
  author = {Chernyadev, Nikita and Backshall, Nicholas and Ma, Xiao and Lu, Yunfan and Seo, Younggyo and James, Stephen},
  year = {2024},
  month = jul,
  number = {arXiv:2407.07788},
  eprint = {2407.07788},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.07788},
  urldate = {2024-12-06},
  abstract = {We introduce BiGym, a new benchmark and learning environment for mobile bi-manual demo-driven robotic manipulation. BiGym features 40 diverse tasks set in home environments, ranging from simple target reaching to complex kitchen cleaning. To capture the real-world performance accurately, we provide human-collected demonstrations for each task, reflecting the diverse modalities found in real-world robot trajectories. BiGym supports a variety of observations, including proprioceptive data and visual inputs such as RGB, and depth from 3 camera views. To validate the usability of BiGym, we thoroughly benchmark the state-of-the-art imitation learning algorithms and demo-driven reinforcement learning algorithms within the environment and discuss the future opportunities.},
  archiveprefix = {arXiv},
  keywords = {Benchmark},
  file = {/Users/fangyuan/Zotero/storage/4A8ABWZK/Chernyadev et al. - 2024 - BiGym A Demo-Driven Mobile Bi-Manual Manipulation Benchmark.pdf;/Users/fangyuan/Zotero/storage/E4I9YYUM/2407.html}
}

@misc{chuSFTMemorizesRL2025,
  title = {{{SFT Memorizes}}, {{RL Generalizes}}: {{A Comparative Study}} of {{Foundation Model Post-training}}},
  shorttitle = {{{SFT Memorizes}}, {{RL Generalizes}}},
  author = {Chu, Tianzhe and Zhai, Yuexiang and Yang, Jihan and Tong, Shengbang and Xie, Saining and Schuurmans, Dale and Le, Quoc V. and Levine, Sergey and Ma, Yi},
  year = {2025},
  month = jan,
  number = {arXiv:2501.17161},
  eprint = {2501.17161},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.17161},
  urldate = {2025-02-01},
  abstract = {Supervised fine-tuning (SFT) and reinforcement learning (RL) are widely used post-training techniques for foundation models. However, their roles in enhancing model generalization capabilities remain unclear. This paper studies the difference between SFT and RL on generalization and memorization, focusing on text-based rule variants and visual variants. We introduce GeneralPoints, an arithmetic reasoning card game, and adopt V-IRL, a real-world navigation environment, to assess how models trained with SFT and RL generalize to unseen variants in both textual and visual domains. We show that RL, especially when trained with an outcome-based reward, generalizes across both rule-based textual and visual variants. SFT, in contrast, tends to memorize training data and struggles to generalize out-of-distribution scenarios. Further analysis reveals that RL improves the model's underlying visual recognition capabilities, contributing to its enhanced generalization in the visual domain. Despite RL's superior generalization, we show that SFT remains essential for effective RL training; SFT stabilizes the model's output format, enabling subsequent RL to achieve its performance gains. These findings demonstrates the capability of RL for acquiring generalizable knowledge in complex, multi-modal tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/WWIXSKJ4/Chu et al. - 2025 - SFT Memorizes, RL Generalizes A Comparative Study of Foundation Model Post-training.pdf;/Users/fangyuan/Zotero/storage/95NEQT2V/2501.html}
}

@misc{cobbePhasicPolicyGradient2020,
  title = {Phasic {{Policy Gradient}}},
  author = {Cobbe, Karl and Hilton, Jacob and Klimov, Oleg and Schulman, John},
  year = {2020},
  month = sep,
  eprint = {2009.04416},
  primaryclass = {cs, stat},
  urldate = {2022-07-19},
  abstract = {We introduce Phasic Policy Gradient (PPG), a reinforcement learning framework which modifies traditional on-policy actor-critic methods by separating policy and value function training into distinct phases. In prior methods, one must choose between using a shared network or separate networks to represent the policy and value function. Using separate networks avoids interference between objectives, while using a shared network allows useful features to be shared. PPG is able to achieve the best of both worlds by splitting optimization into two phases, one that advances training and one that distills features. PPG also enables the value function to be more aggressively optimized with a higher level of sample reuse. Compared to PPO, we find that PPG significantly improves sample efficiency on the challenging Procgen Benchmark.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/NKD43QDM/Cobbe et al. - 2020 - Phasic Policy Gradient.pdf}
}

@misc{cohenDynamicPlanningOpenEnded2022,
  title = {Dynamic {{Planning}} in {{Open-Ended Dialogue}} Using {{Reinforcement Learning}}},
  author = {Cohen, Deborah and Ryu, Moonkyung and Chow, Yinlam and Keller, Orgad and Greenberg, Ido and Hassidim, Avinatan and Fink, Michael and Matias, Yossi and Szpektor, Idan and Boutilier, Craig and Elidan, Gal},
  year = {2022},
  month = jul,
  number = {arXiv:2208.02294},
  eprint = {2208.02294},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.02294},
  urldate = {2023-06-22},
  abstract = {Despite recent advances in natural language understanding and generation, and decades of research on the development of conversational bots, building automated agents that can carry on rich open-ended conversations with humans "in the wild" remains a formidable challenge. In this work we develop a real-time, open-ended dialogue system that uses reinforcement learning (RL) to power a bot's conversational skill at scale. Our work pairs the succinct embedding of the conversation state generated using SOTA (supervised) language models with RL techniques that are particularly suited to a dynamic action space that changes as the conversation progresses. Trained using crowd-sourced data, our novel system is able to substantially exceeds the (strong) baseline supervised model with respect to several metrics of interest in a live experiment with real users of the Google Assistant.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/Q36A3HUN/Cohen et al. - 2022 - Dynamic Planning in Open-Ended Dialogue using Rein.pdf;/Users/fangyuan/Zotero/storage/QCZLHNPW/2208.html}
}

@article{coumansPyBulletQuickstartGuide,
  title = {{{PyBullet Quickstart Guide}}},
  author = {Coumans, Erwin and Bai, Yunfei},
  pages = {89},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/KC7X5MZQ/Coumans and Bai - PyBullet Quickstart Guide.pdf}
}

@inproceedings{cuiAdaptingHumanoidLocomotion2024,
  title = {Adapting {{Humanoid Locomotion}} over {{Challenging Terrain}} via {{Two-Phase Training}}},
  booktitle = {8th {{Annual Conference}} on {{Robot Learning}}},
  author = {Cui, Wenhao and Li, Shengtao and Huang, Huaxing and Qin, Bangyu and Zhang, Tianchu and {hanjinchao} and Zheng, Liang and Tang, Ziyang and Hu, Chenxu and Yan, Ning and Chen, Jiahao and Jiang, Zheyuan},
  year = {2024},
  month = sep,
  urldate = {2024-12-06},
  abstract = {Humanoid robots are a key focus in robotics, with their capacity to navigate tough terrains being essential for many uses. While strides have been made, creating adaptable locomotion for complex environments is still tough. Recent progress in learning-based systems offers hope for robust legged locomotion, but challenges persist, such as tracking accuracy at high speeds and on uneven ground, and joint oscillations in actual robots. This paper proposes a novel training framework to address these challenges by employing a two-phase training paradigm with reinforcement learning. The proposed framework is further enhanced through the integration of command curriculum learning, refining the precision and adaptability of our approach. Additionally, we adapt DreamWaQ to our humanoid locomotion system and improve it to mitigate joint oscillations. Finally, we achieve the sim-to-real transfer of our method. A series of empirical results demonstrate the superior performance of our proposed method compared to state-of-the-art methods.},
  langid = {english},
  keywords = {Locomotion},
  file = {/Users/fangyuan/Zotero/storage/4PMUR7GG/Cui et al. - 2024 - Adapting Humanoid Locomotion over Challenging Terrain via Two-Phase Training.pdf}
}

@misc{daoSimtoRealLearningHumanoid2023,
  title = {Sim-to-{{Real Learning}} for {{Humanoid Box Loco-Manipulation}}},
  author = {Dao, Jeremy and Duan, Helei and Fern, Alan},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03191},
  eprint = {2310.03191},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03191},
  urldate = {2024-12-06},
  abstract = {In this work we propose a learning-based approach to box loco-manipulation for a humanoid robot. This is a particularly challenging problem due to the need for whole-body coordination in order to lift boxes of varying weight, position, and orientation while maintaining balance. To address this challenge, we present a sim-to-real reinforcement learning approach for training general box pickup and carrying skills for the bipedal robot Digit. Our reward functions are designed to produce the desired interactions with the box while also valuing balance and gait quality. We combine the learned skills into a full system for box loco-manipulation to achieve the task of moving boxes from one table to another with a variety of sizes, weights, and initial configurations. In addition to quantitative simulation results, we demonstrate successful sim-to-real transfer on the humanoid r},
  archiveprefix = {arXiv},
  keywords = {Digit,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/3ZXM9XLV/Dao et al. - 2023 - Sim-to-Real Learning for Humanoid Box Loco-Manipulation.pdf;/Users/fangyuan/Zotero/storage/FZ9F65XW/2310.html}
}

@misc{dasariIngredientsRoboticDiffusion2024,
  title = {The {{Ingredients}} for {{Robotic Diffusion Transformers}}},
  author = {Dasari, Sudeep and Mees, Oier and Zhao, Sebastian and Srirama, Mohan Kumar and Levine, Sergey},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10088},
  eprint = {2410.10088},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10088},
  urldate = {2024-10-18},
  abstract = {In recent years roboticists have achieved remarkable progress in solving increasingly general tasks on dexterous robotic hardware by leveraging high capacity Transformer network architectures and generative diffusion models. Unfortunately, combining these two orthogonal improvements has proven surprisingly difficult, since there is no clear and well-understood process for making important design choices. In this paper, we identify, study and improve key architectural design decisions for high-capacity diffusion transformer policies. The resulting models can efficiently solve diverse tasks on multiple robot embodiments, without the excruciating pain of per-setup hyper-parameter tuning. By combining the results of our investigation with our improved model components, we are able to present a novel architecture, named {\textbackslash}method, that significantly outperforms the state of the art in solving long-horizon (\$1500+\$ time-steps) dexterous tasks on a bi-manual ALOHA robot. In addition, we find that our policies show improved scaling performance when trained on 10 hours of highly multi-modal, language annotated ALOHA demonstration data. We hope this work will open the door for future robot learning techniques that leverage the efficiency of generative diffusion modeling with the scalability of large scale transformer architectures. Code, robot dataset, and videos are available at: https://dit-policy.github.io},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/YXEKTFYK/Dasari et al. - 2024 - The Ingredients for Robotic Diffusion Transformers.pdf;/Users/fangyuan/Zotero/storage/RXU66GQ5/2410.html}
}

@misc{dasariTransformersOneShotVisual2020,
  title = {Transformers for {{One-Shot Visual Imitation}}},
  author = {Dasari, Sudeep and Gupta, Abhinav},
  year = {2020},
  month = nov,
  number = {arXiv:2011.05970},
  eprint = {2011.05970},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-26},
  abstract = {Humans are able to seamlessly visually imitate others, by inferring their intentions and using past experience to achieve the same end goal. In other words, we can parse complex semantic knowledge from raw video and efficiently translate that into concrete motor control. Is it possible to give a robot this same capability? Prior research in robot imitation learning has created agents which can acquire diverse skills from expert human operators. However, expanding these techniques to work with a single positive example during test time is still an open challenge. Apart from control, the difficulty stems from mismatches between the demonstrator and robot domains. For example, objects may be placed in different locations (e.g. kitchen layouts are different in every house). Additionally, the demonstration may come from an agent with different morphology and physical appearance (e.g. human), so one-to-one action correspondences are not available. This paper investigates techniques which allow robots to partially bridge these domain gaps, using their past experience. A neural network is trained to mimic ground truth robot actions given context video from another agent, and must generalize to unseen task instances when prompted with new videos during test time. We hypothesize that our policy representations must be both context driven and dynamics aware in order to perform these tasks. These assumptions are baked into the neural network using the Transformers attention mechanism and a self-supervised inverse dynamics loss. Finally, we experimentally determine that our method accomplishes a \${\textbackslash}sim 2\$x improvement in terms of task success rate over prior baselines in a suite of one-shot manipulation tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/7UAU6Y7R/Dasari and Gupta - 2020 - Transformers for One-Shot Visual Imitation.pdf}
}

@misc{dashoraViVaVideoTrainedValue2025,
  title = {{{ViVa}}: {{Video-Trained Value Functions}} for {{Guiding Online RL}} from {{Diverse Data}}},
  shorttitle = {{{ViVa}}},
  author = {Dashora, Nitish and Ghosh, Dibya and Levine, Sergey},
  year = {2025},
  month = mar,
  number = {arXiv:2503.18210},
  eprint = {2503.18210},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.18210},
  urldate = {2025-03-27},
  abstract = {Online reinforcement learning (RL) with sparse rewards poses a challenge partly because of the lack of feedback on states leading to the goal. Furthermore, expert offline data with reward signal is rarely available to provide this feedback and bootstrap online learning. How can we guide online agents to the right solution without this on-task data? Reward shaping offers a solution by providing fine-grained signal to nudge the policy towards the optimal solution. However, reward shaping often requires domain knowledge to hand-engineer heuristics for a specific goal. To enable more general and inexpensive guidance, we propose and analyze a data-driven methodology that automatically guides RL by learning from widely available video data such as Internet recordings, off-task demonstrations, task failures, and undirected environment interaction. By learning a model of optimal goal-conditioned value from diverse passive data, we open the floor to scaling up and using various data sources to model general goal-reaching behaviors relevant to guiding online RL. Specifically, we use intent-conditioned value functions to learn from diverse videos and incorporate these goal-conditioned values into the reward. Our experiments show that video-trained value functions work well with a variety of data sources, exhibit positive transfer from human video pre-training, can generalize to unseen goals, and scale with dataset size.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{dassLearningLookSeeking2024,
  title = {Learning to {{Look}}: {{Seeking Information}} for {{Decision Making}} via {{Policy Factorization}}},
  shorttitle = {Learning to {{Look}}},
  author = {Dass, Shivin and Hu, Jiaheng and Abbatematteo, Ben and Stone, Peter and {Mart{\'i}n-Mart{\'i}n}, Roberto},
  year = {2024},
  month = oct,
  number = {arXiv:2410.18964},
  eprint = {2410.18964},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.18964},
  urldate = {2024-12-06},
  abstract = {Many robot manipulation tasks require active or interactive exploration behavior in order to be performed successfully. Such tasks are ubiquitous in embodied domains, where agents must actively search for the information necessary for each stage of a task, e.g., moving the head of the robot to find information relevant to manipulation, or in multi-robot domains, where one scout robot may search for the information that another robot needs to make informed decisions. We identify these tasks with a new type of problem, factorized Contextual Markov Decision Processes, and propose DISaM, a dual-policy solution composed of an information-seeking policy that explores the environment to find the relevant contextual information and an information-receiving policy that exploits the context to achieve the manipulation goal. This factorization allows us to train both policies separately, using the information-receiving one to provide reward to train the information-seeking policy. At test time, the dual agent balances exploration and exploitation based on the uncertainty the manipulation policy has on what the next best action is. We demonstrate the capabilities of our dual policy solution in five manipulation tasks that require information-seeking behaviors, both in simulation and in the real-world, where DISaM significantly outperforms existing methods. More information at https://robin-lab.cs.utexas.edu/learning2look/.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,Tiago},
  file = {/Users/fangyuan/Zotero/storage/WCBQUVSX/Dass et al. - 2024 - Learning to Look Seeking Information for Decision Making via Policy Factorization.pdf;/Users/fangyuan/Zotero/storage/WCGU5ZFA/2410.html}
}

@misc{deepseek-aiDeepSeekR1IncentivizingReasoning2025,
  title = {{{DeepSeek-R1}}: {{Incentivizing Reasoning Capability}} in {{LLMs}} via {{Reinforcement Learning}}},
  shorttitle = {{{DeepSeek-R1}}},
  author = {{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  year = {2025},
  month = jan,
  number = {arXiv:2501.12948},
  eprint = {2501.12948},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.12948},
  urldate = {2025-01-25},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/VKKNQHI9/DeepSeek-AI et al. - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf;/Users/fangyuan/Zotero/storage/5IHMNE8P/2501.html}
}

@inproceedings{devinLearningModularNeural2017,
  title = {Learning Modular Neural Network Policies for Multi-Task and Multi-Robot Transfer},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Devin, Coline and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = may,
  pages = {2169--2176},
  doi = {10.1109/ICRA.2017.7989250},
  abstract = {Reinforcement learning (RL) can automate a wide variety of robotic skills, but learning each new skill requires considerable real-world data collection and manual representation engineering to design policy classes or features. Using deep reinforcement learning to train general purpose neural network policies alleviates some of the burden of manual representation engineering by using expressive policy classes, but exacerbates the challenge of data collection, since such methods tend to be less efficient than RL with low-dimensional, hand-designed representations. Transfer learning can mitigate this problem by enabling us to transfer information from one skill to another and even from one robot to another. We show that neural network policies can be decomposed into ``task-specific'' and ``robot-specific'' modules, where the task-specific modules are shared across robots, and the robot-specific modules are shared across all tasks on that robot. This allows for sharing task information, such as perception, between robots and sharing robot information, such as dynamics and kinematics, between tasks. We exploit this decomposition to train mix-and-match modules that can solve new robot-task combinations that were not seen during training. Using a novel approach to train modular neural networks, we demonstrate the effectiveness of our transfer method for enabling zero-shot generalization with a variety of robots and tasks in simulation for both visual and non-visual tasks.},
  keywords = {Games,Learning (artificial intelligence),Neural networks,Robot sensing systems,Training,Visualization},
  file = {/Users/fangyuan/Zotero/storage/952D7EST/Devin et al. - 2017 - Learning modular neural network policies for multi.pdf;/Users/fangyuan/Zotero/storage/THJLWCAL/7989250.html}
}

@article{dingGoalconditionedImitationLearning,
  title = {Goal-Conditioned {{Imitation Learning}}},
  author = {Ding, Yiming and Florensa, Carlos and Abbeel, Pieter and Phielipp, Mariano},
  pages = {12},
  abstract = {Designing rewards for Reinforcement Learning (RL) is challenging because it needs to convey the desired task, be efficient to optimize, and be easy to compute. The latter is particularly problematic when applying RL to robotics, where detecting whether the desired configuration is reached might require considerable supervision and instrumentation. Furthermore, we are often interested in being able to reach a wide range of configurations, hence setting up a different reward every time might be unpractical. Methods like Hindsight Experience Replay (HER) have recently shown promise to learn policies able to reach many goals, without the need of a reward. Unfortunately, without tricks like resetting to points along the trajectory, HER might require many samples to discover how to reach certain areas of the state-space. In this work we propose a novel algorithm goalGAIL, which incorporates demonstrations to drastically speed up the convergence to a policy able to reach any goal, surpassing the performance of an agent trained with other Imitation Learning algorithms. Furthermore, we show our method can also be used when the available expert trajectories do not contain the actions or when the expert is suboptimal, which makes it applicable when only kinesthetic, third-person or noisy demonstrations are available. Our code is open-source 2.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/Z32C2LI8/Ding et al. - Goal-conditioned Imitation Learning.pdf}
}

@misc{dingHumanoidVLAUniversalHumanoid2025,
  title = {Humanoid-{{VLA}}: {{Towards Universal Humanoid Control}} with {{Visual Integration}}},
  shorttitle = {Humanoid-{{VLA}}},
  author = {Ding, Pengxiang and Ma, Jianfei and Tong, Xinyang and Zou, Binghong and Luo, Xinxin and Fan, Yiguo and Wang, Ting and Lu, Hongchao and Mo, Panzhong and Liu, Jinxin and Wang, Yuefan and Zhou, Huaicheng and Feng, Wenshuo and Liu, Jiacheng and Huang, Siteng and Wang, Donglin},
  year = {2025},
  month = feb,
  number = {arXiv:2502.14795},
  eprint = {2502.14795},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.14795},
  urldate = {2025-02-25},
  abstract = {This paper addresses the limitations of current humanoid robot control frameworks, which primarily rely on reactive mechanisms and lack autonomous interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control. Humanoid-VLA begins with language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions, allowing the model to learn universal motion patterns and action semantics. We then incorporate egocentric visual context through a parameter efficient video-conditioned fine-tuning, enabling context-aware motion generation. Furthermore, we introduce a self-supervised data augmentation strategy that automatically generates pseudoannotations directly derived from motion data. This process converts raw motion sequences into informative question-answer pairs, facilitating the effective use of large-scale unlabeled video data. Built upon whole-body control architectures, extensive experiments show that Humanoid-VLA achieves object interaction and environment exploration tasks with enhanced contextual awareness, demonstrating a more human-like capacity for adaptive and intelligent engagement.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/E29YSTFZ/Ding et al. - 2025 - Humanoid-VLA Towards Universal Humanoid Control with Visual Integration.pdf;/Users/fangyuan/Zotero/storage/6QT8KVVC/2502.html}
}

@misc{doshiScalingCrossEmbodiedLearning2024,
  title = {Scaling {{Cross-Embodied Learning}}: {{One Policy}} for {{Manipulation}}, {{Navigation}}, {{Locomotion}} and {{Aviation}}},
  shorttitle = {Scaling {{Cross-Embodied Learning}}},
  author = {Doshi, Ria and Walke, Homer and Mees, Oier and Dasari, Sudeep and Levine, Sergey},
  year = {2024},
  month = aug,
  number = {arXiv:2408.11812},
  eprint = {2408.11812},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-24},
  abstract = {Modern machine learning systems rely on large datasets to attain broad generalization, and this often poses a challenge in robot learning, where each robotic platform and task might have only a small dataset. By training a single policy across many different kinds of robots, a robot learning method can leverage much broader and more diverse datasets, which in turn can lead to better generalization and robustness. However, training a single policy on multi-robot data is challenging because robots can have widely varying sensors, actuators, and control frequencies. We propose CrossFormer, a scalable and flexible transformerbased policy that can consume data from any embodiment. We train CrossFormer on the largest and most diverse dataset to date, 900K trajectories across 20 different robot embodiments. We demonstrate that the same network weights can control vastly different robots, including single and dual arm manipulation systems, wheeled robots, quadcopters, and quadrupeds. Unlike prior work, our model does not require manual alignment of the observation or action spaces. Extensive experiments in the real world show that our method matches the performance of specialist policies tailored for each embodiment, while also significantly outperforming the prior state of the art in cross-embodiment learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Manipulation,Navigation,Robotic Foundation Model},
  file = {/Users/fangyuan/Zotero/storage/KBHLBUID/Doshi et al. - 2024 - Scaling Cross-Embodied Learning One Policy for Manipulation, Navigation, Locomotion and Aviation.pdf}
}

@misc{duanE2HTwoStageNonInvasive2024,
  title = {{{E2H}}: {{A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic Whole-Body Control Framework}}},
  shorttitle = {{{E2H}}},
  author = {Duan, Yiqun and Zhang, Qiang and Zhou, Jinzhao and Sun, Jingkai and Jiang, Xiaowei and Cao, Jiahang and Wang, Jiaxu and Yang, Yiqian and Zhao, Wen and Han, Gang and Guo, Yijie and Lin, Chin-Teng},
  year = {2024},
  month = oct,
  number = {arXiv:2410.02141},
  eprint = {2410.02141},
  publisher = {arXiv},
  urldate = {2024-10-18},
  abstract = {Recent advancements in humanoid robotics, including the integration of hierarchical reinforcement learning-based control and the utilization of LLM planning, have significantly enhanced the ability of robots to perform complex tasks. In contrast to the highly developed humanoid robots, the human factors involved remain relatively unexplored. Directly controlling humanoid robots with the brain has already appeared in many science fiction novels, such as Pacific Rim and Gundam. In this work, we present E2H (EEG-to-Humanoid), an innovative framework that pioneers the control of humanoid robots using high-frequency non-invasive neural signals. As the none-invasive signal quality remains low in decoding precise spatial trajectory, we decompose the E2H framework in an innovative two-stage formation: 1) decoding neural signals (EEG) into semantic motion keywords, 2) utilizing LLM facilitated motion generation with a precise motion imitation control policy to realize humanoid robotics control. The method of directly driving robots with brainwave commands offers a novel approach to human-machine collaboration, especially in situations where verbal commands are impractical, such as in cases of speech impairments, space exploration, or underwater exploration, unlocking significant potential. E2H offers an exciting glimpse into the future, holding immense potential for human-computer interaction.},
  archiveprefix = {arXiv},
  keywords = {Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/TD5LBGZS/Duan et al. - 2024 - E2H A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic Whole-Body Control Framework.pdf;/Users/fangyuan/Zotero/storage/TBGGBUMX/2410.html}
}

@misc{duanE2HTwoStageNonInvasive2024a,
  title = {{{E2H}}: {{A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic Whole-Body Control Framework}}},
  shorttitle = {{{E2H}}},
  author = {Duan, Yiqun and Zhou, Jinzhao and Jiang, Xiaowei and Zhang, Qiang and Sun, Jingkai and Cao, Jiahang and Wang, Jiaxu and Yang, Yiqian and Zhao, Wen and Han, Gang and Guo, Yijie and Lin, Chin-Teng},
  year = {2024},
  month = oct,
  journal = {arXiv.org},
  urldate = {2024-10-07},
  abstract = {Recent advancements in humanoid robotics, including the integration of hierarchical reinforcement learning-based control and the utilization of LLM planning, have significantly enhanced the ability of robots to perform complex tasks. In contrast to the highly developed humanoid robots, the human factors involved remain relatively unexplored. Directly controlling humanoid robots with the brain has already appeared in many science fiction novels, such as Pacific Rim and Gundam. In this work, we present E2H (EEG-to-Humanoid), an innovative framework that pioneers the control of humanoid robots using high-frequency non-invasive neural signals. As the none-invasive signal quality remains low in decoding precise spatial trajectory, we decompose the E2H framework in an innovative two-stage formation: 1) decoding neural signals (EEG) into semantic motion keywords, 2) utilizing LLM facilitated motion generation with a precise motion imitation control policy to realize humanoid robotics control. The method of directly driving robots with brainwave commands offers a novel approach to human-machine collaboration, especially in situations where verbal commands are impractical, such as in cases of speech impairments, space exploration, or underwater exploration, unlocking significant potential. E2H offers an exciting glimpse into the future, holding immense potential for human-computer interaction.},
  howpublished = {https://arxiv.org/abs/2410.02141v1},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/W2NY6B5A/Duan et al. - 2024 - E2H A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic Whole-Body Control Framework.pdf}
}

@article{duanOneShotImitationLearning,
  title = {One-{{Shot Imitation Learning}}},
  author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Ho, OpenAI Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  pages = {12},
  abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/RA936M7R/Duan et al. - One-Shot Imitation Learning.pdf}
}

@misc{dugarLearningMultiModalWholeBody2024,
  title = {Learning {{Multi-Modal Whole-Body Control}} for {{Real-World Humanoid Robots}}},
  author = {Dugar, Pranay and Shrestha, Aayam and Yu, Fangzhou and {van Marum}, Bart and Fern, Alan},
  year = {2024},
  month = sep,
  number = {arXiv:2408.07295},
  eprint = {2408.07295},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-18},
  abstract = {The foundational capabilities of humanoid robots should include robustly standing, walking, and mimicry of whole and partial-body motions. This work introduces the Masked Humanoid Controller (MHC), which supports all of these capabilities by tracking target trajectories over selected subsets of humanoid state variables while ensuring balance and robustness against disturbances. The MHC is trained in simulation using a carefully designed curriculum that imitates partially masked motions from a library of behaviors spanning standing, walking, optimized reference trajectories, re-targeted video clips, and human motion capture data. It also allows for combining joystick-based control with partial-body motion mimicry. We showcase simulation experiments validating the MHC's ability to execute a wide variety of behaviors from partially-specified target motions. Moreover, we demonstrate sim-to-real transfer on the real-world Digit V3 humanoid robot. To our knowledge, this is the first instance of a learned controller that can realize whole-body control of a real-world humanoid for such diverse multi-modal targets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Digit,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/MBRYARW8/Dugar et al. - 2024 - Learning Multi-Modal Whole-Body Control for Real-World Humanoid Robots.pdf}
}

@misc{espeholtIMPALAScalableDistributed2018,
  title = {{{IMPALA}}: {{Scalable Distributed Deep-RL}} with {{Importance Weighted Actor-Learner Architectures}}},
  shorttitle = {{{IMPALA}}},
  author = {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  year = {2018},
  month = jun,
  eprint = {1802.01561},
  primaryclass = {cs},
  urldate = {2022-07-19},
  abstract = {In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/AZE28CV7/Espeholt et al. - 2018 - IMPALA Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures.pdf}
}

@article{eysenbachSearchReplayBuffer,
  title = {Search on the {{Replay Buffer}}: {{Bridging Planning}} and {{Reinforcement Learning}}},
  author = {Eysenbach, Ben and Salakhutdinov, Russ R and Levine, Sergey},
  pages = {12},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/VJUYFWR6/Eysenbach et al. - Search on the Replay Buffer Bridging Planning and.pdf}
}

@misc{fanDiffusionTrajectoryguidedPolicy2025,
  title = {Diffusion {{Trajectory-guided Policy}} for {{Long-horizon Robot Manipulation}}},
  author = {Fan, Shichao and Yang, Quantao and Liu, Yajie and Wu, Kun and Che, Zhengping and Liu, Qingjie and Wan, Min},
  year = {2025},
  month = feb,
  number = {arXiv:2502.10040},
  eprint = {2502.10040},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.10040},
  urldate = {2025-02-18},
  abstract = {Recently, Vision-Language-Action models (VLA) have advanced robot imitation learning, but high data collection costs and limited demonstrations hinder generalization and current imitation learning methods struggle in out-of-distribution scenarios, especially for long-horizon tasks. A key challenge is how to mitigate compounding errors in imitation learning, which lead to cascading failures over extended trajectories. To address these challenges, we propose the Diffusion Trajectory-guided Policy (DTP) framework, which generates 2D trajectories through a diffusion model to guide policy learning for long-horizon tasks. By leveraging task-relevant trajectories, DTP provides trajectory-level guidance to reduce error accumulation. Our two-stage approach first trains a generative vision-language model to create diffusion-based trajectories, then refines the imitation policy using them. Experiments on the CALVIN benchmark show that DTP outperforms state-of-the-art baselines by 25\% in success rate, starting from scratch without external pretraining. Moreover, DTP significantly improves real-world robot performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/5ZMK5G2V/Fan et al. - 2025 - Diffusion Trajectory-guided Policy for Long-horizon Robot Manipulation.pdf;/Users/fangyuan/Zotero/storage/QBBVED9C/2502.html}
}

@article{fangDHERHINDSIGHTEXPERIENCE2019,
  title = {{{DHER}}: {{HINDSIGHT EXPERIENCE REPLAY FOR DYNAMIC GOALS}}},
  author = {Fang, Meng and Zhou, Cheng and Shi, Bei and Gong, Boqing and Xu, Jia and Zhang, Tong},
  year = {2019},
  pages = {12},
  abstract = {Dealing with sparse rewards is one of the most important challenges in reinforcement learning (RL), especially when a goal is dynamic (e.g., to grasp a moving object). Hindsight experience replay (HER) has been shown an effective solution to handling sparse rewards with fixed goals. However, it does not account for dynamic goals in its vanilla form and, as a result, even degrades the performance of existing off-policy RL algorithms when the goal is changing over time.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/MRA9KC4L/Fang et al. - 2019 - DHER HINDSIGHT EXPERIENCE REPLAY FOR DYNAMIC GOAL.pdf}
}

@misc{fangPlanningPracticeEfficient2022,
  title = {Planning to {{Practice}}: {{Efficient Online Fine-Tuning}} by {{Composing Goals}} in {{Latent Space}}},
  shorttitle = {Planning to {{Practice}}},
  author = {Fang, Kuan and Yin, Patrick and Nair, Ashvin and Levine, Sergey},
  year = {2022},
  month = may,
  number = {arXiv:2205.08129},
  eprint = {2205.08129},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-10-25},
  abstract = {General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected data through offline reinforcement learning, and then fine-tunes the policy via online exploration. This fine-tuning process is itself facilitated by the planned subgoals, which breaks down the original target task into short-horizon goal-reaching tasks that are significantly easier to learn. We conduct experiments in both the simulation and real world, in which the policy is pre-trained on demonstrations of short primitive behaviors and fine-tuned for temporally extended tasks that are unseen in the offline data. Our experimental results show that PTP can generate feasible sequences of subgoals that enable the policy to efficiently solve the target tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/N8CBFDR6/Fang et al. - 2022 - Planning to Practice Efficient Online Fine-Tuning.pdf}
}

@article{fangSurveyImitationLearning2019,
  title = {Survey of Imitation Learning for Robotic Manipulation},
  author = {Fang, Bin and Jia, Shidong and Guo, Di and Xu, Muhua and Wen, Shuhuan and Sun, Fuchun},
  year = {2019},
  month = dec,
  journal = {International Journal of Intelligent Robotics and Applications},
  volume = {3},
  number = {4},
  pages = {362--369},
  issn = {2366-5971, 2366-598X},
  doi = {10.1007/s41315-019-00103-5},
  urldate = {2022-08-29},
  abstract = {With the development of robotics, the application of robots has gradually evolved from industrial scenes to more intelligent service scenarios. For multitasking operations of robots in complex and uncertain environments, the traditional manual coding method is not only cumbersome but also unable to adapt to sudden changes in the environment. Imitation learning that avoids learning skills from scratch by using the expert demonstration has become the most effective way for robotic manipulation. The paper is intended to provide the survey of imitation learning of robotic manipulation and explore the future research trend. The review of the art of imitation learning for robotic manipulation involves three aspects that are demonstration, representation and learning algorithms. Towards the end of the paper, we highlight areas of future research potential.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/CB3XH35H/Fang et al. - 2019 - Survey of imitation learning for robotic manipulat.pdf}
}

@misc{fengReflectivePlanningVisionLanguage2025,
  title = {Reflective {{Planning}}: {{Vision-Language Models}} for {{Multi-Stage Long-Horizon Robotic Manipulation}}},
  shorttitle = {Reflective {{Planning}}},
  author = {Feng, Yunhai and Han, Jiaming and Yang, Zhuoran and Yue, Xiangyu and Levine, Sergey and Luo, Jianlan},
  year = {2025},
  month = feb,
  number = {arXiv:2502.16707},
  eprint = {2502.16707},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.16707},
  urldate = {2025-02-25},
  abstract = {Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a framework for tackling such problems. However, in their current form, VLMs lack both the nuanced understanding of intricate physics required for robotic manipulation and the ability to reason over long horizons to address error compounding issues. In this paper, we introduce a novel test-time computation framework that enhances VLMs' physical reasoning capabilities for multi-stage manipulation tasks. At its core, our approach iteratively improves a pretrained VLM with a "reflection" mechanism - it uses a generative model to imagine future world states, leverages these predictions to guide action selection, and critically reflects on potential suboptimalities to refine its reasoning. Experimental results demonstrate that our method significantly outperforms several state-of-the-art commercial VLMs as well as other post-training approaches such as Monte Carlo Tree Search (MCTS). Videos are available at https://reflect-vlm.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/88XKZDB5/Feng et al. - 2025 - Reflective Planning Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation.pdf;/Users/fangyuan/Zotero/storage/7YXDPBMB/2502.html}
}

@misc{florenceImplicitBehavioralCloning2021,
  title = {Implicit {{Behavioral Cloning}}},
  author = {Florence, Pete and Lynch, Corey and Zeng, Andy and Ramirez, Oscar and Wahid, Ayzaan and Downs, Laura and Wong, Adrian and Lee, Johnny and Mordatch, Igor and Tompson, Jonathan},
  year = {2021},
  month = aug,
  number = {arXiv:2109.00137},
  eprint = {2109.00137},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-08},
  abstract = {We find that across a wide range of robot policy learning scenarios, treating supervised policy learning with an implicit model generally performs better, on average, than commonly used explicit models. We present extensive experiments on this finding, and we provide both intuitive insight and theoretical arguments distinguishing the properties of implicit models compared to their explicit counterparts, particularly with respect to approximating complex, potentially discontinuous and multi-valued (set-valued) functions. On robotic policy learning tasks we show that implicit behavioral cloning policies with energy-based models (EBM) often outperform common explicit (Mean Square Error, or Mixture Density) behavioral cloning policies, including on tasks with high-dimensional action spaces and visual image inputs. We find these policies provide competitive results or outperform state-of-the-art offline reinforcement learning methods on the challenging human-expert tasks from the D4RL benchmark suite, despite using no reward information. In the real world, robots with implicit policies can learn complex and remarkably subtle behaviors on contact-rich tasks from human demonstrations, including tasks with high combinatorial complexity and tasks requiring 1mm precision.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/JMNFRLD3/Florence et al. - 2021 - Implicit Behavioral Cloning.pdf}
}

@misc{florensaAutomaticGoalGeneration2018,
  title = {Automatic {{Goal Generation}} for {{Reinforcement Learning Agents}}},
  author = {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
  year = {2018},
  month = jul,
  number = {arXiv:1705.06366},
  eprint = {1705.06366},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.06366},
  urldate = {2023-03-10},
  abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/JBKUQNL6/Florensa et al. - 2018 - Automatic Goal Generation for Reinforcement Learni.pdf;/Users/fangyuan/Zotero/storage/9FFKDUTQ/1705.html}
}

@article{foxPDDL21ExtensionPDDL2003,
  title = {{{PDDL2}}.1: {{An Extension}} to {{PDDL}} for {{Expressing Temporal Planning Domains}}},
  shorttitle = {{{PDDL2}}.1},
  author = {Fox, M. and Long, D.},
  year = {2003},
  month = dec,
  journal = {Journal of Artificial Intelligence Research},
  volume = {20},
  pages = {61--124},
  issn = {1076-9757},
  doi = {10.1613/jair.1129},
  urldate = {2024-06-21},
  abstract = {In recent years research in the planning community has moved increasingly towards application of planners to realistic problems involving both time and many types of resources. For example, interest in planning demonstrated by the space research community has inspired work in observation scheduling, planetary rover exploration and spacecraft control domains. Other temporal and resource-intensive domains including logistics planning, plant control and manufacturing have also helped to focus the community on the modelling and reasoning issues that must be confronted to make planning technology meet the challenges of application.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/P6874Y9J/Fox and Long - 2003 - PDDL2.1 An Extension to PDDL for Expressing Temporal Planning Domains.pdf}
}

@misc{fransOneStepDiffusion2024,
  title = {One {{Step Diffusion}} via {{Shortcut Models}}},
  author = {Frans, Kevin and Hafner, Danijar and Levine, Sergey and Abbeel, Pieter},
  year = {2024},
  month = oct,
  number = {arXiv:2410.12557},
  eprint = {2410.12557},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.12557},
  urldate = {2024-10-18},
  abstract = {Diffusion models and flow-matching models have enabled generating diverse and realistic images by learning to transfer noise to data. However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive. Previous approaches for speeding up sampling require complex training regimes, such as multiple training phases, multiple networks, or fragile scheduling. We introduce shortcut models, a family of generative models that use a single network and training phase to produce high-quality samples in a single or multiple sampling steps. Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process. Across a wide range of sampling step budgets, shortcut models consistently produce higher quality samples than previous approaches, such as consistency models and reflow. Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/CN55XBQR/Frans et al. - 2024 - One Step Diffusion via Shortcut Models.pdf;/Users/fangyuan/Zotero/storage/PBPIGQH6/2410.html}
}

@misc{fuD4RLDatasetsDeep2021,
  title = {{{D4RL}}: {{Datasets}} for {{Deep Data-Driven Reinforcement Learning}}},
  shorttitle = {{{D4RL}}},
  author = {Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  year = {2021},
  month = feb,
  number = {arXiv:2004.07219},
  eprint = {2004.07219},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.07219},
  urldate = {2023-02-09},
  abstract = {The offline reinforcement learning (RL) setting (also known as full batch RL), where a policy is learned from a static dataset, is compelling as progress enables RL methods to take advantage of large, previously-collected datasets, much like how the rise of large datasets has fueled results in supervised learning. However, existing online RL benchmarks are not tailored towards the offline setting and existing offline RL benchmarks are restricted to data generated by partially-trained agents, making progress in offline RL difficult to measure. In this work, we introduce benchmarks specifically designed for the offline setting, guided by key properties of datasets relevant to real-world applications of offline RL. With a focus on dataset collection, examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multitask datasets where an agent performs different tasks in the same environment, and datasets collected with mixtures of policies. By moving beyond simple benchmark tasks and data collected by partially-trained RL agents, we reveal important and unappreciated deficiencies of existing algorithms. To facilitate research, we have released our benchmark tasks and datasets with a comprehensive evaluation of existing algorithms, an evaluation protocol, and open-source examples. This serves as a common starting point for the community to identify shortcomings in existing offline RL methods and a collaborative route for progress in this emerging area.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/F5TGKJS4/Fu et al. - 2021 - D4RL Datasets for Deep Data-Driven Reinforcement .pdf;/Users/fangyuan/Zotero/storage/87PWCP4U/2004.html}
}

@inproceedings{fuHumanPlusHumanoidShadowing2024,
  title = {{{HumanPlus}}: {{Humanoid Shadowing}} and {{Imitation}} from {{Humans}}},
  shorttitle = {{{HumanPlus}}},
  booktitle = {8th {{Annual Conference}} on {{Robot Learning}}},
  author = {Fu, Zipeng and Zhao, Qingqing and Wu, Qi and Wetzstein, Gordon and Finn, Chelsea},
  year = {2024},
  month = sep,
  urldate = {2024-12-07},
  abstract = {One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training.Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simulation via reinforcement learning using existing 40-hour human motion datasets. This policy transfers to the real world and allows humanoid robots to follow human body and hand motion in real time using only a RGB camera, i.e. shadowing. Through shadowing, human operators can teleoperate humanoids to collect whole-body data for learning different tasks in the real world. Using the data collected, we then perform supervised behavior cloning to train skill policies using egocentric vision, allowing humanoids to complete different tasks autonomously by imitating human skills. We demonstrate the system on our customized 33-DoF 180cm humanoid, autonomously completing tasks such as wearing a shoe to stand up and walk, folding a sweatshirt, rearranging objects, typing, and greeting another robot with 60-100\% success rates using up to 40 demonstrations.},
  langid = {english},
  keywords = {Imitation Learning,Manipulation,Retargeting,Unitree},
  file = {/Users/fangyuan/Zotero/storage/TL48JM22/Fu et al. - 2024 - HumanPlus Humanoid Shadowing and Imitation from Humans.pdf}
}

@misc{fujimotoAddressingFunctionApproximation2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and {van Hoof}, Herke and Meger, David},
  year = {2018},
  month = oct,
  eprint = {1802.09477},
  primaryclass = {cs, stat},
  urldate = {2022-07-19},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/PJCK2Z38/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-Critic Methods.pdf}
}

@misc{garrettPDDLStreamIntegratingSymbolic2020,
  title = {{{PDDLStream}}: {{Integrating Symbolic Planners}} and {{Blackbox Samplers}} via {{Optimistic Adaptive Planning}}},
  shorttitle = {{{PDDLStream}}},
  author = {Garrett, Caelan Reed and {Lozano-P{\'e}rez}, Tom{\'a}s and Kaelbling, Leslie Pack},
  year = {2020},
  month = mar,
  number = {arXiv:1802.08705},
  eprint = {1802.08705},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.08705},
  urldate = {2024-06-04},
  abstract = {Many planning applications involve complex relationships defined on high-dimensional, continuous variables. For example, robotic manipulation requires planning with kinematic, collision, visibility, and motion constraints involving robot configurations, object poses, and robot trajectories. These constraints typically require specialized procedures to sample satisfying values. We extend PDDL to support a generic, declarative specification for these procedures that treats their implementation as black boxes. We provide domain-independent algorithms that reduce PDDLStream problems to a sequence of finite PDDL problems. We also introduce an algorithm that dynamically balances exploring new candidate plans and exploiting existing ones. This enables the algorithm to greedily search the space of parameter bindings to more quickly solve tightly-constrained problems as well as locally optimize to produce low-cost solutions. We evaluate our algorithms on three simulated robotic planning domains as well as several real-world robotic tasks.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/K9MBIKLS/Garrett et al. - 2020 - PDDLStream Integrating Symbolic Planners and Blackbox Samplers via Optimistic Adaptive Planning.pdf;/Users/fangyuan/Zotero/storage/79V29GLV/1802.html}
}

@inproceedings{gengGAPartNetCrossCategoryDomainGeneralizable2023,
  title = {{{GAPartNet}}: {{Cross-Category Domain-Generalizable Object Perception}} and {{Manipulation}} via {{Generalizable}} and {{Actionable Parts}}},
  shorttitle = {{{GAPartNet}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He},
  year = {2023},
  month = jun,
  pages = {7081--7091},
  publisher = {IEEE},
  address = {Vancouver, BC, Canada},
  doi = {10.1109/CVPR52729.2023.00684},
  urldate = {2024-12-07},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-0129-8},
  langid = {english},
  keywords = {Dataset,Manipulation,PointCloud},
  file = {/Users/fangyuan/Zotero/storage/4AKH7WGC/Geng et al. - 2023 - GAPartNet Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable.pdf}
}

@article{ghoshOctoOpenSourceGeneralist,
  title = {Octo: {{An Open-Source Generalist Robot Policy}}},
  author = {Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
  langid = {english},
  keywords = {Manipulation,Robotic Foundation Model},
  file = {/Users/fangyuan/Zotero/storage/Q7FQHGRZ/Ghosh et al. - Octo An Open-Source Generalist Robot Policy.pdf}
}

@misc{guAdvancingHumanoidLocomotion2024,
  title = {Advancing {{Humanoid Locomotion}}: {{Mastering Challenging Terrains}} with {{Denoising World Model Learning}}},
  shorttitle = {Advancing {{Humanoid Locomotion}}},
  author = {Gu, Xinyang and Wang, Yen-Jen and Zhu, Xiang and Shi, Chengming and Guo, Yanjiang and Liu, Yichen and Chen, Jianyu},
  year = {2024},
  month = aug,
  number = {arXiv:2408.14472},
  eprint = {2408.14472},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.14472},
  urldate = {2024-12-06},
  abstract = {Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world's first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains. All scenarios run the same learned neural network with zero-shot sim-to-real transfer, indicating the superior robustness and generalization capability of the proposed method.},
  archiveprefix = {arXiv},
  keywords = {Locomotion},
  file = {/Users/fangyuan/Zotero/storage/WHZGK75H/Gu et al. - 2024 - Advancing Humanoid Locomotion Mastering Challenging Terrains with Denoising World Model Learning.pdf;/Users/fangyuan/Zotero/storage/79V3XCDQ/2408.html}
}

@misc{guanLOCZSONLanguagedrivenObjectCentric2024,
  title = {{{LOC-ZSON}}: {{Language-driven Object-Centric Zero-Shot Object Retrieval}} and {{Navigation}}},
  shorttitle = {{{LOC-ZSON}}},
  author = {Guan, Tianrui and Yang, Yurou and Cheng, Harry and Lin, Muyuan and Kim, Richard and Madhivanan, Rajasimman and Sen, Arnie and Manocha, Dinesh},
  year = {2024},
  month = may,
  number = {arXiv:2405.05363},
  eprint = {2405.05363},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.05363},
  urldate = {2024-05-20},
  abstract = {In this paper, we present LOC-ZSON, a novel Language-driven Object-Centric image representation for object navigation task within complex scenes. We propose an object-centric image representation and corresponding losses for visual-language model (VLM) fine-tuning, which can handle complex object-level queries. In addition, we design a novel LLM-based augmentation and prompt templates for stability during training and zero-shot inference. We implement our method on Astro robot and deploy it in both simulated and real-world environments for zero-shot object navigation. We show that our proposed method can achieve an improvement of 1.38 - 13.38\% in terms of text-to-image recall on different benchmark settings for the retrieval task. For object navigation, we show the benefit of our approach in simulation and real world, showing 5\% and 16.67\% improvement in terms of navigation success rate, respectively.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/MIE2B25Z/Guan et al. - 2024 - LOC-ZSON Language-driven Object-Centric Zero-Shot.pdf;/Users/fangyuan/Zotero/storage/UM3JMVJD/2405.html}
}

@misc{guHumanoidGymReinforcementLearning2024,
  title = {Humanoid-{{Gym}}: {{Reinforcement Learning}} for {{Humanoid Robot}} with {{Zero-Shot Sim2Real Transfer}}},
  shorttitle = {Humanoid-{{Gym}}},
  author = {Gu, Xinyang and Wang, Yen-Jen and Chen, Jianyu},
  year = {2024},
  month = may,
  number = {arXiv:2404.05695},
  eprint = {2404.05695},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.05695},
  urldate = {2024-12-06},
  abstract = {Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots, emphasizing zero-shot transfer from simulation to the real-world environment. Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco that allows users to verify the trained policies in different physical simulations to ensure the robustness and generalization of the policies. This framework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) and XBot-L (1.65-meter tall humanoid robot) in a real-world environment with zero-shot sim-to-real transfer. The project website and source code can be found at: https://sites.google.com/view/humanoid-gym/.},
  archiveprefix = {arXiv},
  keywords = {Benchmark},
  file = {/Users/fangyuan/Zotero/storage/NJRVL53D/Gu et al. - 2024 - Humanoid-Gym Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer.pdf;/Users/fangyuan/Zotero/storage/RDKMJW55/2404.html}
}

@misc{guHumanoidLocomotionManipulation2025,
  title = {Humanoid {{Locomotion}} and {{Manipulation}}: {{Current Progress}} and {{Challenges}} in {{Control}}, {{Planning}}, and {{Learning}}},
  shorttitle = {Humanoid {{Locomotion}} and {{Manipulation}}},
  author = {Gu, Zhaoyuan and Li, Junheng and Shen, Wenlan and Yu, Wenhao and Xie, Zhaoming and McCrory, Stephen and Cheng, Xianyi and Shamsah, Abdulaziz and Griffin, Robert and Liu, C. Karen and Kheddar, Abderrahmane and Peng, Xue Bin and Zhu, Yuke and Shi, Guanya and Nguyen, Quan and Cheng, Gordon and Gao, Huijun and Zhao, Ye},
  year = {2025},
  month = jan,
  number = {arXiv:2501.02116},
  eprint = {2501.02116},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.02116},
  urldate = {2025-01-08},
  abstract = {Humanoid robots have great potential to perform various human-level skills. These skills involve locomotion, manipulation, and cognitive capabilities. Driven by advances in machine learning and the strength of existing model-based approaches, these capabilities have progressed rapidly, but often separately. Therefore, a timely overview of current progress and future trends in this fast-evolving field is essential. This survey first summarizes the model-based planning and control that have been the backbone of humanoid robotics for the past three decades. We then explore emerging learning-based methods, with a focus on reinforcement learning and imitation learning that enhance the versatility of loco-manipulation skills. We examine the potential of integrating foundation models with humanoid embodiments, assessing the prospects for developing generalist humanoid agents. In addition, this survey covers emerging research for whole-body tactile sensing that unlocks new humanoid skills that involve physical interactions. The survey concludes with a discussion of the challenges and future trends.},
  archiveprefix = {arXiv},
  keywords = {Diffusion,Locomotion,Manipulation,Robotic Foundation Model,Survey,VLA},
  file = {/Users/fangyuan/Zotero/storage/4INGJFGP/Gu et al. - 2025 - Humanoid Locomotion and Manipulation Current Progress and Challenges in Control, Planning, and Lear.pdf;/Users/fangyuan/Zotero/storage/SIYVI4H7/2501.html}
}

@misc{guoImprovingVisionLanguageActionModel2025,
  title = {Improving {{Vision-Language-Action Model}} with {{Online Reinforcement Learning}}},
  author = {Guo, Yanjiang and Zhang, Jianke and Chen, Xiaoyu and Ji, Xiang and Wang, Yen-Jen and Hu, Yucheng and Chen, Jianyu},
  year = {2025},
  month = jan,
  number = {arXiv:2501.16664},
  eprint = {2501.16664},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.16664},
  urldate = {2025-02-01},
  abstract = {Recent studies have successfully integrated large vision-language models (VLMs) into low-level robotic control by supervised fine-tuning (SFT) with expert robotic datasets, resulting in what we term vision-language-action (VLA) models. Although the VLA models are powerful, how to improve these large models during interaction with environments remains an open question. In this paper, we explore how to further improve these VLA models via Reinforcement Learning (RL), a commonly used fine-tuning technique for large models. However, we find that directly applying online RL to large VLA models presents significant challenges, including training instability that severely impacts the performance of large models, and computing burdens that exceed the capabilities of most local machines. To address these challenges, we propose iRe-VLA framework, which iterates between Reinforcement Learning and Supervised Learning to effectively improve VLA models, leveraging the exploratory benefits of RL while maintaining the stability of supervised learning. Experiments in two simulated benchmarks and a real-world manipulation suite validate the effectiveness of our method.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,RL,VLA},
  file = {/Users/fangyuan/Zotero/storage/8XS2WHG3/Guo et al. - 2025 - Improving Vision-Language-Action Model with Online Reinforcement Learning.pdf;/Users/fangyuan/Zotero/storage/4H5SIRQ8/2501.html}
}

@misc{guptaActionContextualizationAdaptive2024,
  title = {Action {{Contextualization}}: {{Adaptive Task Planning}} and {{Action Tuning}} Using {{Large Language Models}}},
  shorttitle = {Action {{Contextualization}}},
  author = {Gupta, Sthithpragya and Yao, Kunpeng and Niederhauser, Lo{\"i}c and Billard, Aude},
  year = {2024},
  month = apr,
  number = {arXiv:2404.13191},
  eprint = {2404.13191},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-24},
  abstract = {Large Language Models (LLMs) present a promising frontier in robotic task planning by leveraging extensive human knowledge. Nevertheless, the current literature often overlooks the critical aspects of adaptability and error correction within robotic systems. This work aims to overcome this limitation by enabling robots to modify their motion strategies and select the most suitable task plans based on the context. We introduce a novel framework termed action contextualization, aimed at tailoring robot actions to the precise requirements of specific tasks, thereby enhancing adaptability through applying LLM-derived contextual insights. Our proposed motion metrics guarantee the feasibility and efficiency of adjusted motions, which evaluate robot performance and eliminate planning redundancies. Moreover, our framework supports online feedback between the robot and the LLM, enabling immediate modifications to the task plans and corrections of errors. Our framework has achieved an overall success rate of 81.25\% through extensive validation. Finally, integrated with dynamic system (DS)-based robot controllers, the robotic arm-hand system demonstrates its proficiency in autonomously executing LLM-generated motion plans for sequential table-clearing tasks, rectifying errors without human intervention, and completing tasks, showcasing robustness against external disturbances. Our proposed framework features the potential to be integrated with modular control approaches, significantly enhancing robots' adaptability and autonomy in sequential task execution.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/UXKQM5D4/Gupta et al. - 2024 - Action Contextualization Adaptive Task Planning a.pdf;/Users/fangyuan/Zotero/storage/UXRBDANB/2404.html}
}

@misc{guptaEssentialRoleCausality2024,
  title = {The {{Essential Role}} of {{Causality}} in {{Foundation World Models}} for {{Embodied AI}}},
  author = {Gupta, Tarun and Gong, Wenbo and Ma, Chao and Pawlowski, Nick and Hilmkil, Agrin and Scetbon, Meyer and Rigter, Marc and Famoti, Ade and Llorens, Ashley Juan and Gao, Jianfeng and Bauer, Stefan and Kragic, Danica and Sch{\"o}lkopf, Bernhard and Zhang, Cheng},
  year = {2024},
  month = apr,
  number = {arXiv:2402.06665},
  eprint = {2402.06665},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.06665},
  urldate = {2024-05-20},
  abstract = {Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents will require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions and are therefore insufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitating meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook for future research.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/8Q92TDI7/Gupta et al. - 2024 - The Essential Role of Causality in Foundation Worl.pdf;/Users/fangyuan/Zotero/storage/TT2A339U/2402.html}
}

@misc{guptaPretrainedTexttoImageDiffusion2024,
  title = {Pre-Trained {{Text-to-Image Diffusion Models Are Versatile Representation Learners}} for {{Control}}},
  author = {Gupta, Gunshi and Yadav, Karmesh and Gal, Yarin and Batra, Dhruv and Kira, Zsolt and Lu, Cong and Rudner, Tim G. J.},
  year = {2024},
  month = may,
  number = {arXiv:2405.05852},
  eprint = {2405.05852},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.05852},
  urldate = {2024-05-20},
  abstract = {Embodied AI agents require a fine-grained understanding of the physical world mediated through visual and language inputs. Such capabilities are difficult to learn solely from task-specific data. This has led to the emergence of pre-trained vision-language models as a tool for transferring representations learned from internet-scale data to downstream tasks and new domains. However, commonly used contrastively trained representations such as in CLIP have been shown to fail at enabling embodied agents to gain a sufficiently fine-grained scene understanding -- a capability vital for control. To address this shortcoming, we consider representations from pre-trained text-to-image diffusion models, which are explicitly optimized to generate images from text prompts and as such, contain text-conditioned representations that reflect highly fine-grained visuo-spatial information. Using pre-trained text-to-image diffusion models, we construct Stable Control Representations which allow learning downstream control policies that generalize to complex, open-ended environments. We show that policies learned using Stable Control Representations are competitive with state-of-the-art representation learning approaches across a broad range of simulated control settings, encompassing challenging manipulation and navigation tasks. Most notably, we show that Stable Control Representations enable learning policies that exhibit state-of-the-art performance on OVMM, a difficult open-vocabulary navigation benchmark.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/Z9YHPCYB/2405.html}
}

@misc{haarnojaSoftActorCriticAlgorithms2019,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  year = {2019},
  month = jan,
  eprint = {1812.05905},
  primaryclass = {cs, stat},
  urldate = {2022-07-19},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/3VE2WT6Y/Haarnoja et al. - 2019 - Soft Actor-Critic Algorithms and Applications.pdf}
}

@misc{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = aug,
  eprint = {1801.01290},
  primaryclass = {cs, stat},
  urldate = {2022-07-19},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/TBJH7AUG/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf}
}

@misc{haoEmbodiedChainAction2025,
  title = {Embodied {{Chain}} of {{Action Reasoning}} with {{Multi-Modal Foundation Model}} for {{Humanoid Loco-manipulation}}},
  author = {Hao, Yu and Bethala, Geeta Chandra Raju and Pudasaini, Niraj and Huang, Hao and Yuan, Shuaihang and Wen, Congcong and Huang, Baoru and Nguyen, Anh and Fang, Yi},
  year = {2025},
  month = apr,
  number = {arXiv:2504.09532},
  eprint = {2504.09532},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.09532},
  urldate = {2025-04-18},
  abstract = {Enabling humanoid robots to autonomously perform loco-manipulation tasks in complex, unstructured environments poses significant challenges. This entails equipping robots with the capability to plan actions over extended horizons while leveraging multi-modality to bridge gaps between high-level planning and actual task execution. Recent advancements in multi-modal foundation models have showcased substantial potential in enhancing planning and reasoning abilities, particularly in the comprehension and processing of semantic information for robotic control tasks. In this paper, we introduce a novel framework based on foundation models that applies the embodied chain of action reasoning methodology to autonomously plan actions from textual instructions for humanoid loco-manipulation. Our method integrates humanoid-specific chain of thought methodology, including detailed affordance and body movement analysis, which provides a breakdown of the task into a sequence of locomotion and manipulation actions. Moreover, we incorporate spatial reasoning based on the observation and target object properties to effectively navigate where target position may be unseen or occluded. Through rigorous experimental setups on object rearrangement, manipulations and loco-manipulation tasks on a real-world environment, we evaluate our method's efficacy on the decoupled upper and lower body control and demonstrate the effectiveness of the chain of robotic action reasoning strategies in comprehending human instructions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/CBET3WJQ/Hao et al. - 2025 - Embodied Chain of Action Reasoning with Multi-Modal Foundation Model for Humanoid Loco-manipulation.pdf;/Users/fangyuan/Zotero/storage/Z3F7G9L4/2504.html}
}

@misc{hartikainenDynamicalDistanceLearning2020,
  title = {Dynamical {{Distance Learning}} for {{Semi-Supervised}} and {{Unsupervised Skill Discovery}}},
  author = {Hartikainen, Kristian and Geng, Xinyang and Haarnoja, Tuomas and Levine, Sergey},
  year = {2020},
  month = feb,
  number = {arXiv:1907.08225},
  eprint = {1907.08225},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-11-18},
  abstract = {Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. We show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. We evaluate our method both on a real-world robot and in simulation. We show that our method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project website: https://sites.google.com/view/dynamical-distance-learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/J6NWAPDU/Hartikainen et al. - 2020 - Dynamical Distance Learning for Semi-Supervised an.pdf}
}

@misc{hatchGHILGlueHierarchicalControl2024,
  title = {{{GHIL-Glue}}: {{Hierarchical Control}} with {{Filtered Subgoal Images}}},
  shorttitle = {{{GHIL-Glue}}},
  author = {Hatch, Kyle B. and Balakrishna, Ashwin and Mees, Oier and Nair, Suraj and Park, Seohong and Wulfe, Blake and Itkina, Masha and Eysenbach, Benjamin and Levine, Sergey and Kollar, Thomas and Burchfiel, Benjamin},
  year = {2024},
  month = oct,
  number = {arXiv:2410.20018},
  eprint = {2410.20018},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.20018},
  urldate = {2024-11-01},
  abstract = {Image and video generative models that are pre-trained on Internet-scale data can greatly increase the generalization capacity of robot learning systems. These models can function as high-level planners, generating intermediate subgoals for low-level goal-conditioned policies to reach. However, the performance of these systems can be greatly bottlenecked by the interface between generative models and low-level controllers. For example, generative models may predict photorealistic yet physically infeasible frames that confuse low-level policies. Low-level policies may also be sensitive to subtle visual artifacts in generated goal images. This paper addresses these two facets of generalization, providing an interface to effectively "glue together" language-conditioned image or video prediction models with low-level goal-conditioned policies. Our method, Generative Hierarchical Imitation Learning-Glue (GHIL-Glue), filters out subgoals that do not lead to task progress and improves the robustness of goal-conditioned policies to generated subgoals with harmful visual artifacts. We find in extensive experiments in both simulated and real environments that GHIL-Glue achieves a 25\% improvement across several hierarchical models that leverage generative subgoals, achieving a new state-of-the-art on the CALVIN simulation benchmark for policies using observations from a single RGB camera. GHIL-Glue also outperforms other generalist robot policies across 3/4 language-conditioned manipulation tasks testing zero-shot generalization in physical experiments.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/MD3GBBUV/Hatch et al. - 2024 - GHIL-Glue Hierarchical Control with Filtered Subgoal Images.pdf;/Users/fangyuan/Zotero/storage/TGKV4VCC/2410.html}
}

@misc{heASAPAligningSimulation2025,
  title = {{{ASAP}}: {{Aligning Simulation}} and {{Real-World Physics}} for {{Learning Agile Humanoid Whole-Body Skills}}},
  shorttitle = {{{ASAP}}},
  author = {He, Tairan and Gao, Jiawei and Xiao, Wenli and Zhang, Yuanhang and Wang, Zi and Wang, Jiashun and Luo, Zhengyi and He, Guanqi and Sobanbab, Nikhil and Pan, Chaoyi and Yi, Zeji and Qu, Guannan and Kitani, Kris and Hodgins, Jessica and Fan, Linxi "Jim" and Zhu, Yuke and Liu, Changliu and Shi, Guanya},
  year = {2025},
  month = feb,
  number = {arXiv:2502.01143},
  eprint = {2502.01143},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.01143},
  urldate = {2025-02-14},
  abstract = {Humanoid robots hold the potential for unparalleled versatility in performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and the real world. Existing approaches, such as system identification (SysID) and domain randomization (DR) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real-World Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile humanoid whole-body skills. In the first stage, we pre-train motion tracking policies in simulation using retargeted human motion data. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Then, ASAP fine-tunes pre-trained policies with the delta action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios: IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach significantly improves agility and whole-body coordination across various dynamic motions, reducing tracking error compared to SysID, DR, and delta dynamics learning baselines. ASAP enables highly agile motions that were previously difficult to achieve, demonstrating the potential of delta action learning in bridging simulation and real-world dynamics. These results suggest a promising sim-to-real direction for developing more expressive and agile humanoids.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/fangyuan/Zotero/storage/U3ECRNYA/He et al. - 2025 - ASAP Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills.pdf;/Users/fangyuan/Zotero/storage/7VR3TL7P/2502.html}
}

@misc{heHOVERVersatileNeural2024,
  title = {{{HOVER}}: {{Versatile Neural Whole-Body Controller}} for {{Humanoid Robots}}},
  shorttitle = {{{HOVER}}},
  author = {He, Tairan and Xiao, Wenli and Lin, Toru and Luo, Zhengyi and Xu, Zhenjia and Jiang, Zhenyu and Kautz, Jan and Liu, Changliu and Shi, Guanya and Wang, Xiaolong and Fan, Linxi and Zhu, Yuke},
  year = {2024},
  month = oct,
  number = {arXiv:2410.21229},
  eprint = {2410.21229},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.21229},
  urldate = {2024-11-01},
  abstract = {Humanoid whole-body control requires adapting to diverse tasks such as navigation, loco-manipulation, and tabletop manipulation, each demanding a different mode of control. For example, navigation relies on root velocity tracking, while tabletop manipulation prioritizes upper-body joint angle tracking. Existing approaches typically train individual policies tailored to a specific command space, limiting their transferability across modes. We present the key insight that full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control. Building on this, we propose HOVER (Humanoid Versatile Controller), a multi-mode policy distillation framework that consolidates diverse control modes into a unified policy. HOVER enables seamless transitions between control modes while preserving the distinct advantages of each, offering a robust and scalable solution for humanoid control across a wide range of modes. By eliminating the need for policy retraining for each control mode, our approach improves efficiency and flexibility for future humanoid applications.},
  archiveprefix = {arXiv},
  keywords = {Motion Generation,Unitree,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/F7Q8L4AS/He et al. - 2024 - HOVER Versatile Neural Whole-Body Controller for Humanoid Robots.pdf;/Users/fangyuan/Zotero/storage/BHLW4N7Q/2410.html}
}

@misc{heHOVERVersatileNeural2025,
  title = {{{HOVER}}: {{Versatile Neural Whole-Body Controller}} for {{Humanoid Robots}}},
  shorttitle = {{{HOVER}}},
  author = {He, Tairan and Xiao, Wenli and Lin, Toru and Luo, Zhengyi and Xu, Zhenjia and Jiang, Zhenyu and Kautz, Jan and Liu, Changliu and Shi, Guanya and Wang, Xiaolong and Fan, Linxi and Zhu, Yuke},
  year = {2025},
  month = mar,
  number = {arXiv:2410.21229},
  eprint = {2410.21229},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.21229},
  urldate = {2025-03-10},
  abstract = {Humanoid whole-body control requires adapting to diverse tasks such as navigation, loco-manipulation, and tabletop manipulation, each demanding a different mode of control. For example, navigation relies on root velocity tracking, while tabletop manipulation prioritizes upper-body joint angle tracking. Existing approaches typically train individual policies tailored to a specific command space, limiting their transferability across modes. We present the key insight that full-body kinematic motion imitation can serve as a common abstraction for all these tasks and provide general-purpose motor skills for learning multiple modes of whole-body control. Building on this, we propose HOVER (Humanoid Versatile Controller), a multi-mode policy distillation framework that consolidates diverse control modes into a unified policy. HOVER enables seamless transitions between control modes while preserving the distinct advantages of each, offering a robust and scalable solution for humanoid control across a wide range of modes. By eliminating the need for policy retraining for each control mode, our approach improves efficiency and flexibility for future humanoid applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/PKIZPUNJ/He et al. - 2025 - HOVER Versatile Neural Whole-Body Controller for Humanoid Robots.pdf;/Users/fangyuan/Zotero/storage/DW3Q2MV3/2410.html}
}

@misc{heLearningHumantoHumanoidRealTime2024,
  title = {Learning {{Human-to-Humanoid Real-Time Whole-Body Teleoperation}}},
  author = {He, Tairan and Luo, Zhengyi and Xiao, Wenli and Zhang, Chong and Kitani, Kris and Liu, Changliu and Shi, Guanya},
  year = {2024},
  month = mar,
  number = {arXiv:2403.04436},
  eprint = {2403.04436},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.04436},
  urldate = {2024-12-06},
  abstract = {We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable "sim-to-data" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.},
  archiveprefix = {arXiv},
  keywords = {Teleoperation,Unitree,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/27S7Y65H/He et al. - 2024 - Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation.pdf;/Users/fangyuan/Zotero/storage/7LMBXDE5/2403.html}
}

@misc{HelixVisionLanguageActionModel2025,
  title = {Helix: {{A Vision-Language-Action Model}} for {{Generalist Humanoid Control}}},
  shorttitle = {Helix},
  year = {2025},
  month = feb,
  journal = {FigureAI},
  urldate = {2025-03-17},
  abstract = {Figure was founded with the ambition to change the world.},
  howpublished = {https://www.figure.ai/news/helix},
  langid = {english}
}

@misc{heOmniH2OUniversalDexterous2024,
  title = {{{OmniH2O}}: {{Universal}} and {{Dexterous Human-to-Humanoid Whole-Body Teleoperation}} and {{Learning}}},
  shorttitle = {{{OmniH2O}}},
  author = {He, Tairan and Luo, Zhengyi and He, Xialin and Xiao, Wenli and Zhang, Chong and Zhang, Weinan and Kitani, Kris and Liu, Changliu and Shi, Guanya},
  year = {2024},
  month = jun,
  number = {arXiv:2406.08858},
  eprint = {2406.08858},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-06-16},
  abstract = {We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for whole-body humanoid teleoperation and autonomy. Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including using realtime teleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4o. OmniH2O demonstrates versatility and dexterity in various real-world whole-body tasks through teleoperation or autonomy, such as playing multiple sports, moving and manipulating objects, and interacting with humans, as shown in Figure 1. We develop an RL-based sim-to-real pipeline, which involves large-scale retargeting and augmentation of human motion datasets, learning a real-world deployable policy with sparse sensor input by imitating a privileged teacher policy, and reward designs to enhance robustness and stability. We release the first humanoid whole-body control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate humanoid wholebody skill learning from teleoperated datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Diffusion,Retargeting,Unitree,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/G2LHKIJ4/2406.pdf}
}

@article{herzogDeepRLScale,
  title = {Deep {{RL}} at {{Scale}}: {{Sorting Waste}} in {{Office Buildings}} with a {{Fleet}} of {{Mobile Manipulators}}},
  author = {Herzog, Alexander and Rao, Kanishka and Hausman, Karol and Lu, Yao and Wohlhart, Paul and Yan, Mengyuan and Lin, Jessica and Arenas, Montserrat Gonzalez and Xiao, Ted and Kappler, Daniel and Ho, Daniel and Rettinghouse, Jarek and Chebotar, Yevgen and Lee, Kuang-Huei and Gopalakrishnan, Keerthana and Julian, Ryan and Li, Adrian and Fu, Chuyuan Kelly and Wei, Bob and Ramesh, Sangeetha and Holden, Khem and Kleiven, Kim and Rendleman, David and Kirmani, Sean and Bingham, Jeff and Weisz, Jon and Xu, Ying and Lu, Wenlong and Bennice, Matthew and Fong, Cody and Do, David and Lam, Jessica and Bai, Yunfei and Holson, Benjie and Quinlan, Michael and Brown, Noah and Kalakrishnan, Mrinal and Ibarz, Julian and Pastor, Peter and Levine, Sergey},
  abstract = {We describe a system for deep reinforcement learning of robotic manipulation skills applied to a large-scale realworld task: sorting recyclables and trash in office buildings. Real-world deployment of deep RL policies requires not only effective training algorithms, but the ability to bootstrap realworld training and enable broad generalization. To this end, our system combines scalable deep RL from real-world data with bootstrapping from training in simulation, and incorporates auxiliary inputs from existing computer vision systems as a way to boost generalization to novel objects, while retaining the benefits of end-to-end training. We analyze the tradeoffs of different design decisions in our system, and present a large-scale empirical validation that includes training on real-world data gathered over the course of 24 months of experimentation, across a fleet of 23 robots in three office buildings, with a total training set of 9527 hours of robotic experience. Our final validation also consists of 4800 evaluation trials across 240 waste station configurations, in order to evaluate in detail the impact of the design decisions in our system, the scaling effects of including more real-world data, and the performance of the method on novel objects. The projects website and videos can be found at rl-at-scale.github.io.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/SALKNW6G/Herzog et al. - Deep RL at Scale Sorting Waste in Ofﬁce Buildings.pdf}
}

@misc{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.11239},
  urldate = {2025-01-19},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/RHTC65N2/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/Users/fangyuan/Zotero/storage/NNJYIFGJ/2006.html}
}

@inproceedings{hong3DLLMInjecting3D2023,
  title = {{{3D-LLM}}: {{Injecting}} the {{3D World}} into {{Large Language Models}}},
  shorttitle = {{{3D-LLM}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
  year = {2023},
  month = nov,
  urldate = {2024-12-07},
  abstract = {Large language models (LLMs) and Vision-Language Models (VLMs) have been proved to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs could better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin ({\textbackslash}textit\{e.g.\}, the BLEU-1 score surpasses state-of-the-art score by 9{\textbackslash}\%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Our model and data will be publicly available.},
  langid = {english},
  keywords = {Robotic Foundation Model,Vision+Language+3D2Language},
  file = {/Users/fangyuan/Zotero/storage/TNE3IL2M/Hong et al. - 2023 - 3D-LLM Injecting the 3D World into Large Language Models.pdf}
}

@article{hongMultiPLYMultisensoryObjectCentric,
  title = {{{MultiPLY}}: {{A Multisensory Object-Centric Embodied Large Language Model}} in {{3D World}}},
  author = {Hong, Yining and Zheng, Zishuo and Chen, Peihao and Wang, Yian and Li, Junyan and Gan, Chuang},
  langid = {english},
  keywords = {Robotic Foundation Model,Vision+Language+3D+Audio+Thermal+Tactil2Language},
  file = {/Users/fangyuan/Zotero/storage/UP9V4J4A/Hong et al. - MultiPLY A Multisensory Object-Centric Embodied Large Language Model in 3D World.pdf}
}

@misc{hongQSFTQLearningLanguage2024,
  title = {Q-{{SFT}}: {{Q-Learning}} for {{Language Models}} via {{Supervised Fine-Tuning}}},
  shorttitle = {Q-{{SFT}}},
  author = {Hong, Joey and Dragan, Anca and Levine, Sergey},
  year = {2024},
  month = nov,
  number = {arXiv:2411.05193},
  eprint = {2411.05193},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.05193},
  urldate = {2024-11-28},
  abstract = {Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite the widespread use of policy gradient methods to train large language models for single turn tasks (e.g., question answering), value-based methods for multi-turn RL in an off-policy or offline setting have proven particularly challenging to scale to the setting of large language models. This setting requires effectively leveraging pretraining, scaling to large architectures with billions of parameters, and training on large datasets, all of which represent major challenges for current value-based RL methods. In this work, we propose a novel offline RL algorithm that addresses these drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT) problem where the probabilities of tokens directly translate to Q-values. In this way we obtain an algorithm that smoothly transitions from maximizing the likelihood of the data during pretraining to learning a near-optimal Q-function during finetuning. Our algorithm has strong theoretical foundations, enjoying performance bounds similar to state-of-the-art Q-learning methods, while in practice utilizing an objective that closely resembles SFT. Because of this, our approach can enjoy the full benefits of the pretraining of language models, without the need to reinitialize any weights before RL finetuning, and without the need to initialize new heads for predicting values or advantages. Empirically, we evaluate our method on both pretrained LLMs and VLMs, on a variety of tasks including both natural language dialogue and robotic manipulation and navigation from images.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/W8S78847/Hong et al. - 2024 - Q-SFT Q-Learning for Language Models via Supervised Fine-Tuning.pdf;/Users/fangyuan/Zotero/storage/L8PHKV5G/2411.html}
}

@misc{hoqueVisuoSpatialForesightMultiStep2021,
  title = {{{VisuoSpatial Foresight}} for {{Multi-Step}}, {{Multi-Task Fabric Manipulation}}},
  author = {Hoque, Ryan and Seita, Daniel and Balakrishna, Ashwin and Ganapathi, Aditya and Tanwani, Ajay Kumar and Jamali, Nawid and Yamane, Katsu and Iba, Soshi and Goldberg, Ken},
  year = {2021},
  month = feb,
  number = {arXiv:2003.09044},
  eprint = {2003.09044},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-19},
  abstract = {Robotic fabric manipulation has applications in home robotics, textiles, senior care and surgery. Existing fabric manipulation techniques, however, are designed for specific tasks, making it difficult to generalize across different but related tasks. We extend the Visual Foresight framework to learn fabric dynamics that can be efficiently reused to accomplish different fabric manipulation tasks with a single goal-conditioned policy. We introduce VisuoSpatial Foresight (VSF), which builds on prior work by learning visual dynamics on domain randomized RGB images and depth maps simultaneously and completely in simulation. We experimentally evaluate VSF on multi-step fabric smoothing and folding tasks against 5 baseline methods in simulation and on the da Vinci Research Kit (dVRK) surgical robot without any demonstrations at train or test time. Furthermore, we find that leveraging depth significantly improves performance. RGBD data yields an 80\% improvement in fabric folding success rate over pure RGB data. Code, data, videos, and supplementary material are available at https://sites.google.com/view/fabric-vsf/.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/4HWKKZHL/Hoque et al. - 2021 - VisuoSpatial Foresight for Multi-Step, Multi-Task .pdf}
}

@article{houCredibilityAssessmentBased2022,
  title = {Credibility {{Assessment Based Byzantine-Resilient Decentralized Learning}}},
  author = {Hou, Jian and Wang, Fangyuan and Wei, Chunling and Huang, Hongyun and Hu, Yong and Gui, Ning},
  year = {2022},
  journal = {IEEE Transactions on Dependable and Secure Computing},
  pages = {1--12},
  issn = {1545-5971, 1941-0018, 2160-9209},
  doi = {10.1109/TDSC.2022.3183337},
  urldate = {2023-12-17},
  abstract = {Decentralized deep learning has made significant success since it avoids the single point of failure in centralized solutions. However, the system might deviate from the correct model due to Byzantine attacks. Existing Byzantine-resilient defense models are mainly of a one-step evaluation fashion, making them vulnerable to rigorous topology and sophisticated cyber-attacks due to lack of historical evaluations.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/9NSEJE7M/Hou et al. - 2022 - Credibility Assessment Based Byzantine-Resilient D.pdf}
}

@article{houReinforcementLearningBased2021,
  title = {Reinforcement {{Learning Based Multi-Agent Resilient Control}}: {{From Deep Neural Networks}} to an {{Adaptive Law}}},
  shorttitle = {Reinforcement {{Learning Based Multi-Agent Resilient Control}}},
  author = {Hou, Jian and Wang, Fangyuan and Wang, Lili and Chen, Zhiyong},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {9},
  pages = {7737--7745},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v35i9.16945},
  urldate = {2023-12-17},
  abstract = {Recent advances in Multi-agent Reinforcement Learning (MARL) have made it possible to implement various tasks in cooperative as well as competitive scenarios through trial and error, and deep neural networks. These successes motivate us to bring the mechanism of MARL into the Multi-agent Resilient Consensus (MARC) problem that studies the consensus problem in a network of agents with faulty ones. Relying on the natural characteristics of the system goal, the key component in MARL, reward function, can thus be directly constructed via the relative distance among agents. Firstly, we apply Deep Deterministic Policy Gradient (DDPG) on each single agent to train and learn adjacent weights of neighboring agents in a distributed manner, that we call DistributedDDPG (D-DDPG), so as to minimize the weights from suspicious agents and eliminate the corresponding influences. Secondly, to get rid of neural networks and their time-consuming training process, a Q-learning based algorithm, called Qconsensus, is further presented by building a proper reward function and a credibility function for each pair of neighboring agents so that the adjacent weights can update in an adaptive way. The experimental results indicate that both algorithms perform well with appearance of constant and/or random faulty agents, yet the Q-consensus algorithm outperforms the faulty ones running D-DDPG. Compared to the traditional resilient consensus strategies, e.g., Weighted-MeanSubsequence-Reduced (W-MSR) or trustworthiness analysis, the proposed Q-consensus algorithm has greatly relaxed the topology requirements, as well as reduced the storage and computation loads. Finally, a smart-car hardware platform consisting of six vehicles is used to verify the effectiveness of the Q-consensus algorithm by achieving resilient velocity synchronization.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/4GZDGMGA/Hou et al. - 2021 - Reinforcement Learning Based Multi-Agent Resilient.pdf}
}

@misc{huangEMOTIONExpressiveMotion2024,
  title = {{{EMOTION}}: {{Expressive Motion Sequence Generation}} for {{Humanoid Robots}} with {{In-Context Learning}}},
  shorttitle = {{{EMOTION}}},
  author = {Huang, Peide and Hu, Yuhan and Nechyporenko, Nataliya and Kim, Daehwa and Talbott, Walter and Zhang, Jian},
  year = {2024},
  month = oct,
  number = {arXiv:2410.23234},
  eprint = {2410.23234},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.23234},
  urldate = {2024-12-06},
  abstract = {This paper introduces a framework, called EMOTION, for generating expressive motion sequences in humanoid robots, enhancing their ability to engage in humanlike non-verbal communication. Non-verbal cues such as facial expressions, gestures, and body movements play a crucial role in effective interpersonal interactions. Despite the advancements in robotic behaviors, existing methods often fall short in mimicking the diversity and subtlety of human non-verbal communication. To address this gap, our approach leverages the in-context learning capability of large language models (LLMs) to dynamically generate socially appropriate gesture motion sequences for human-robot interaction. We use this framework to generate 10 different expressive gestures and conduct online user studies comparing the naturalness and understandability of the motions generated by EMOTION and its human-feedback version, EMOTION++, against those by human operators. The results demonstrate that our approach either matches or surpasses human performance in generating understandable and natural robot motions under certain scenarios. We also provide design implications for future research to consider a set of variables when generating expressive robotic gestures.},
  archiveprefix = {arXiv},
  keywords = {FFTAI,Motion Generation,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/FXHDZP8X/Huang et al. - 2024 - EMOTION Expressive Motion Sequence Generation for Humanoid Robots with In-Context Learning.pdf;/Users/fangyuan/Zotero/storage/SBVY7PCA/2410.html}
}

@misc{huangGoalReachingPolicyLearning2024,
  title = {Goal-{{Reaching Policy Learning}} from {{Non-Expert Observations}} via {{Effective Subgoal Guidance}}},
  author = {Huang, RenMing and Liu, Shaochong and Pei, Yunqiang and Wang, Peng and Wang, Guoqing and Yang, Yang and Shen, Hengtao},
  year = {2024},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-09-10},
  abstract = {In this work, we address the challenging problem of long-horizon goal-reaching policy learning from non-expert, action-free observation data. Unlike fully labeled expert data, our data is more accessible and avoids the costly process of action labeling. Additionally, compared to online learning, which often involves aimless exploration, our data provides useful guidance for more efficient exploration. To achieve our goal, we propose a novel subgoal guidance learning strategy. The motivation behind this strategy is that long-horizon goals offer limited guidance for efficient exploration and accurate state transition. We develop a diffusion strategy-based high-level policy to generate reasonable subgoals as waypoints, preferring states that more easily lead to the final goal. Additionally, we learn state-goal value functions to encourage efficient subgoal reaching. These two components naturally integrate into the off-policy actor-critic framework, enabling efficient goal attainment through informative exploration. We evaluate our method on complex robotic navigation and manipulation tasks, demonstrating a significant performance advantage over existing methods. Our ablation study further shows that our method is robust to observation data with various corruptions.},
  howpublished = {https://arxiv.org/abs/2409.03996v1},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/CM6WGI4C/Huang et al. - 2024 - Goal-Reaching Policy Learning from Non-Expert Observations via Effective Subgoal Guidance.pdf}
}

@misc{huangOTTERVisionLanguageActionModel2025,
  title = {{{OTTER}}: {{A Vision-Language-Action Model}} with {{Text-Aware Visual Feature Extraction}}},
  shorttitle = {{{OTTER}}},
  author = {Huang, Huang and Liu, Fangchen and Fu, Letian and Wu, Tingfan and Mukadam, Mustafa and Malik, Jitendra and Goldberg, Ken and Abbeel, Pieter},
  year = {2025},
  month = mar,
  number = {arXiv:2503.03734},
  eprint = {2503.03734},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.03734},
  urldate = {2025-03-06},
  abstract = {Vision-Language-Action (VLA) models aim to predict robotic actions based on visual observations and language instructions. Existing approaches require fine-tuning pre-trained visionlanguage models (VLMs) as visual and language features are independently fed into downstream policies, degrading the pre-trained semantic alignments. We propose OTTER, a novel VLA architecture that leverages these existing alignments through explicit, text-aware visual feature extraction. Instead of processing all visual features, OTTER selectively extracts and passes only task-relevant visual features that are semantically aligned with the language instruction to the policy transformer. This allows OTTER to keep the pre-trained vision-language encoders frozen. Thereby, OTTER preserves and utilizes the rich semantic understanding learned from large-scale pre-training, enabling strong zero-shot generalization capabilities. In simulation and real-world experiments, OTTER significantly outperforms existing VLA models, demonstrating strong zeroshot generalization to novel objects and environments. Video, code, checkpoints, and dataset: https://ottervla.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,Robotic Foundation Model},
  file = {/Users/fangyuan/Zotero/storage/ZAGKNQAM/Huang et al. - 2025 - OTTER A Vision-Language-Action Model with Text-Aware Visual Feature Extraction.pdf;/Users/fangyuan/Zotero/storage/XADXNQHQ/2503.html}
}

@misc{huangSkillTransformerMonolithic2023,
  title = {Skill {{Transformer}}: {{A Monolithic Policy}} for {{Mobile Manipulation}}},
  shorttitle = {Skill {{Transformer}}},
  author = {Huang, Xiaoyu and Batra, Dhruv and Rai, Akshara and Szot, Andrew},
  year = {2023},
  month = aug,
  number = {arXiv:2308.09873},
  eprint = {2308.09873},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.09873},
  urldate = {2024-01-29},
  abstract = {We present Skill Transformer, an approach for solving long-horizon robotic tasks by combining conditional sequence modeling and skill modularity. Conditioned on egocentric and proprioceptive observations of a robot, Skill Transformer is trained end-to-end to predict both a high-level skill (e.g., navigation, picking, placing), and a whole-body low-level action (e.g., base and arm motion), using a transformer architecture and demonstration trajectories that solve the full task. It retains the composability and modularity of the overall task through a skill predictor module while reasoning about low-level actions and avoiding hand-off errors, common in modular approaches. We test Skill Transformer on an embodied rearrangement benchmark and find it performs robust task planning and low-level control in new scenarios, achieving a 2.5x higher success rate than baselines in hard rearrangement problems.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,Mobile Manipulation,Robotic Foundation Model},
  file = {/Users/fangyuan/Zotero/storage/X8E8BD5F/Huang et al. - 2023 - Skill Transformer A Monolithic Policy for Mobile Manipulation.pdf;/Users/fangyuan/Zotero/storage/H3GQ83IK/2308.html}
}

@misc{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2025-01-17},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/DH8VPZAP/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/Users/fangyuan/Zotero/storage/A9Z6EF4V/2106.html}
}

@article{husseinImitationLearningSurvey2018,
  title = {Imitation {{Learning}}: {{A Survey}} of {{Learning Methods}}},
  shorttitle = {Imitation {{Learning}}},
  author = {Hussein, Ahmed and Gaber, Mohamed Medhat and Elyan, Eyad and Jayne, Chrisina},
  year = {2018},
  month = mar,
  journal = {ACM Computing Surveys},
  volume = {50},
  number = {2},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3054912},
  urldate = {2022-08-29},
  abstract = {Imitation learning techniques aim to mimic human behavior in a given task. An agent (a learning machine) is trained to perform a task from demonstrations by learning a mapping between observations and actions. The idea of teaching by imitation has been around for many years; however, the field is gaining attention recently due to advances in computing and sensing as well as rising demand for intelligent applications. The paradigm of learning by imitation is gaining popularity because it facilitates teaching complex tasks with minimal expert knowledge of the tasks. Generic imitation learning methods could potentially reduce the problem of teaching a task to that of providing demonstrations, without the need for explicit programming or designing reward functions specific to the task. Modern sensors are able to collect and transmit high volumes of data rapidly, and processors with high computational power allow fast processing that maps the sensory data to actions in a timely manner. This opens the door for many potential AI applications that require real-time perception and reaction such as humanoid robots, self-driving vehicles, human computer interaction, and computer games, to name a few. However, specialized algorithms are needed to effectively and robustly learn models as learning by imitation poses its own set of challenges. In this article, we survey imitation learning methods and present design options in different steps of the learning process. We introduce a background and motivation for the field as well as highlight challenges specific to the imitation problem. Methods for designing and evaluating imitation learning tasks are categorized and reviewed. Special attention is given to learning methods in robotics and games as these domains are the most popular in the literature and provide a wide array of problems and methodologies. We extensively discuss combining imitation learning approaches using different sources and methods, as well as incorporating other motion learning methods to enhance imitation. We also discuss the potential impact on industry, present major applications, and highlight current and future research directions.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/SLZWNJ4Y/Hussein et al. - 2018 - Imitation Learning A Survey of Learning Methods.pdf}
}

@misc{huTreePlannerEfficientCloseloop2024,
  title = {Tree-{{Planner}}: {{Efficient Close-loop Task Planning}} with {{Large Language Models}}},
  shorttitle = {Tree-{{Planner}}},
  author = {Hu, Mengkang and Mu, Yao and Yu, Xinmiao and Ding, Mingyu and Wu, Shiguang and Shao, Wenqi and Chen, Qiguang and Wang, Bin and Qiao, Yu and Luo, Ping},
  year = {2024},
  month = jul,
  number = {arXiv:2310.08582},
  eprint = {2310.08582},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-28},
  abstract = {This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose TREE-PLANNER, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. TREE-PLANNER starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that TREE-PLANNER achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2\% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5\% decrease in error corrections.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Planning},
  file = {/Users/fangyuan/Zotero/storage/L6B4PWX6/Hu et al. - 2024 - Tree-Planner Efficient Close-loop Task Planning with Large Language Models.pdf}
}

@misc{intelligence$p_05$VisionLanguageActionModel2025,
  title = {\${$\pi\_$}\{0.5\}\$: A {{Vision-Language-Action Model}} with {{Open-World Generalization}}},
  shorttitle = {\${$\pi\_$}\{0.5\}\$},
  author = {Intelligence, Physical and Black, Kevin and Brown, Noah and Darpinian, James and Dhabalia, Karan and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Galliker, Manuel Y. and Ghosh, Dibya and Groom, Lachy and Hausman, Karol and Ichter, Brian and Jakubczak, Szymon and Jones, Tim and Ke, Liyiming and LeBlanc, Devin and Levine, Sergey and {Li-Bell}, Adrian and Mothukuri, Mohith and Nair, Suraj and Pertsch, Karl and Ren, Allen Z. and Shi, Lucy Xiaoyang and Smith, Laura and Springenberg, Jost Tobias and Stachowicz, Kyle and Tanner, James and Vuong, Quan and Walke, Homer and Walling, Anna and Wang, Haohuan and Yu, Lili and Zhilinsky, Ury},
  year = {2025},
  month = apr,
  number = {arXiv:2504.16054},
  eprint = {2504.16054},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16054},
  urldate = {2025-04-23},
  abstract = {In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open question how far such models can generalize in the wild. We describe \${\textbackslash}pi\_\{0.5\}\$, a new model based on \${\textbackslash}pi\_\{0\}\$ that uses co-training on heterogeneous tasks to enable broad generalization. \${\textbackslash}pi\_\{0.5\}\${\textbackslash} uses data from multiple robots, high-level semantic prediction, web data, and other sources to enable broadly generalizable real-world robotic manipulation. Our system uses a combination of co-training and hybrid multi-modal examples that combine image observations, language commands, object detections, semantic subtask prediction, and low-level actions. Our experiments show that this kind of knowledge transfer is essential for effective generalization, and we demonstrate for the first time that an end-to-end learning-enabled robotic system can perform long-horizon and dexterous manipulation skills, such as cleaning a kitchen or bedroom, in entirely new homes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/XSQJR6ES/Intelligence et al. - 2025 - $π_ 0.5 $ a Vision-Language-Action Model with Open-World Generalization.pdf;/Users/fangyuan/Zotero/storage/LHCIWYCY/2504.html}
}

@article{jangBCZZeroShotTask,
  title = {{{BC-Z}}: {{Zero-Shot Task Generalization}} with {{Robotic Imitation Learning}}},
  author = {Jang, Eric and Irpan, Alex and Khansari, Mohi and Kappler, Daniel and Ebert, Frederik and Lynch, Corey and Levine, Sergey and Finn, Chelsea},
  pages = {12},
  abstract = {In this paper, we study the problem of enabling a vision-based robotic manipulation system to generalize to novel tasks, a long-standing challenge in robot learning. We approach the challenge from an imitation learning perspective, aiming to study how scaling and broadening the data collected can facilitate such generalization. To that end, we develop an interactive and flexible imitation learning system that can learn from both demonstrations and interventions and can be conditioned on different forms of information that convey the task, including pretrained embeddings of natural language or videos of humans performing the task. When scaling data collection on a real robot to more than 100 distinct tasks, we find that this system can perform 24 unseen manipulation tasks with an average success rate of 44\%, without any robot demonstrations for those tasks.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/BUJ823R3/Jang et al. - BC-Z Zero-Shot Task Generalization with Robotic I.pdf}
}

@misc{jannerPlanningDiffusionFlexible2022,
  title = {Planning with {{Diffusion}} for {{Flexible Behavior Synthesis}}},
  author = {Janner, Michael and Du, Yilun and Tenenbaum, Joshua B. and Levine, Sergey},
  year = {2022},
  month = may,
  eprint = {2205.09991},
  primaryclass = {cs},
  urldate = {2022-07-19},
  abstract = {Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/3MEQ5ZH5/Janner et al. - 2022 - Planning with Diffusion for Flexible Behavior Synthesis.pdf}
}

@misc{jiangDexMimicGenAutomatedData2024,
  title = {{{DexMimicGen}}: {{Automated Data Generation}} for {{Bimanual Dexterous Manipulation}} via {{Imitation Learning}}},
  shorttitle = {{{DexMimicGen}}},
  author = {Jiang, Zhenyu and Xie, Yuqi and Lin, Kevin and Xu, Zhenjia and Wan, Weikang and Mandlekar, Ajay and Fan, Linxi and Zhu, Yuke},
  year = {2024},
  month = oct,
  number = {arXiv:2410.24185},
  eprint = {2410.24185},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.24185},
  urldate = {2024-12-06},
  abstract = {Imitation learning from human demonstrations is an effective means to teach robots manipulation skills. But data acquisition is a major bottleneck in applying this paradigm more broadly, due to the amount of cost and human effort involved. There has been significant interest in imitation learning for bimanual dexterous robots, like humanoids. Unfortunately, data collection is even more challenging here due to the challenges of simultaneously controlling multiple arms and multi-fingered hands. Automated data generation in simulation is a compelling, scalable alternative to fuel this need for data. To this end, we introduce DexMimicGen, a large-scale automated data generation system that synthesizes trajectories from a handful of human demonstrations for humanoid robots with dexterous hands. We present a collection of simulation environments in the setting of bimanual dexterous manipulation, spanning a range of manipulation behaviors and different requirements for coordination among the two arms. We generate 21K demos across these tasks from just 60 source human demos and study the effect of several data generation and policy learning decisions on agent performance. Finally, we present a real-to-sim-to-real pipeline and deploy it on a real-world humanoid can sorting task. Videos and more are at https://dexmimicgen.github.io/},
  archiveprefix = {arXiv},
  keywords = {FFTAI,Manipulation,Real2Sim,Sim2Real,Teleoperation},
  file = {/Users/fangyuan/Zotero/storage/BHNMZKXC/Jiang et al. - 2024 - DexMimicGen Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning.pdf;/Users/fangyuan/Zotero/storage/QGRCXDN7/2410.html}
}

@misc{jiangDexMimicGenAutomatedData2025,
  title = {{{DexMimicGen}}: {{Automated Data Generation}} for {{Bimanual Dexterous Manipulation}} via {{Imitation Learning}}},
  shorttitle = {{{DexMimicGen}}},
  author = {Jiang, Zhenyu and Xie, Yuqi and Lin, Kevin and Xu, Zhenjia and Wan, Weikang and Mandlekar, Ajay and Fan, Linxi and Zhu, Yuke},
  year = {2025},
  month = mar,
  number = {arXiv:2410.24185},
  eprint = {2410.24185},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.24185},
  urldate = {2025-03-10},
  abstract = {Imitation learning from human demonstrations is an effective means to teach robots manipulation skills. But data acquisition is a major bottleneck in applying this paradigm more broadly, due to the amount of cost and human effort involved. There has been significant interest in imitation learning for bimanual dexterous robots, like humanoids. Unfortunately, data collection is even more challenging here due to the challenges of simultaneously controlling multiple arms and multi-fingered hands. Automated data generation in simulation is a compelling, scalable alternative to fuel this need for data. To this end, we introduce DexMimicGen, a large-scale automated data generation system that synthesizes trajectories from a handful of human demonstrations for humanoid robots with dexterous hands. We present a collection of simulation environments in the setting of bimanual dexterous manipulation, spanning a range of manipulation behaviors and different requirements for coordination among the two arms. We generate 21K demos across these tasks from just 60 source human demos and study the effect of several data generation and policy learning decisions on agent performance. Finally, we present a real-to-sim-to-real pipeline and deploy it on a real-world humanoid can sorting task. Generated datasets, simulation environments and additional results are at https://dexmimicgen.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/38DTNYR3/Jiang et al. - 2025 - DexMimicGen Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning.pdf;/Users/fangyuan/Zotero/storage/SH6RJXV6/2410.html}
}

@misc{jiangHarmonWholeBodyMotion2024,
  title = {Harmon: {{Whole-Body Motion Generation}} of {{Humanoid Robots}} from {{Language Descriptions}}},
  shorttitle = {Harmon},
  author = {Jiang, Zhenyu and Xie, Yuqi and Li, Jinhan and Yuan, Ye and Zhu, Yifeng and Zhu, Yuke},
  year = {2024},
  month = oct,
  number = {arXiv:2410.12773},
  eprint = {2410.12773},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.12773},
  urldate = {2024-12-06},
  abstract = {Humanoid robots, with their human-like embodiment, have the potential to integrate seamlessly into human environments. Critical to their coexistence and cooperation with humans is the ability to understand natural language communications and exhibit human-like behaviors. This work focuses on generating diverse whole-body motions for humanoid robots from language descriptions. We leverage human motion priors from extensive human motion datasets to initialize humanoid motions and employ the commonsense reasoning capabilities of Vision Language Models (VLMs) to edit and refine these motions. Our approach demonstrates the capability to produce natural, expressive, and text-aligned humanoid motions, validated through both simulated and real-world experiments. More videos can be found at https://ut-austin-rpl.github.io/Harmon/.},
  archiveprefix = {arXiv},
  keywords = {FFTAI,Motion Generation,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/BRJ52664/Jiang et al. - 2024 - Harmon Whole-Body Motion Generation of Humanoid Robots from Language Descriptions.pdf;/Users/fangyuan/Zotero/storage/BWI8288C/2410.html}
}

@article{jiangSimulationBasedReinforcementLearning,
  title = {Simulation-{{Based Reinforcement Learning}} for {{Automatic Trajectory Planning}} in {{Conformal Needle-Based Ultrasound Hyperthermia}}},
  author = {Jiang, Yiwei and Zhao, Zhanyue and Wang, Yang and Yang, Kehan and Gandomi, Katie and Nycz, Christopher J and Fischer, Gregory Scott},
  abstract = {The pursuit of effective cancer treatment strategies focuses on eradicating malignant cells with the least damage to surrounding healthy tissues. A minimally invasive option of notable interest is needle-based ultrasound hyperthermia. This technique involves an applicator with an ultrasound transducer to generate heat as acoustic pressure waves propagate through tissue. By segmenting the transducer for directed heating, ablation patterns can be shaped for irregularly shaped tumors using rotational control. However, the complexity of modeling the interactions between ultrasound waves and biological tissues poses significant challenges to both pre-operative planning and real-time control of the process. In this paper, we present a novel application of Reinforcement Learning (RL) within a custom-built simulation environment designed to replicate the dynamics of ultrasound thermal ablation. We also developed a specialized state definition and reward function to effectively train RL agents in learning optimal ablation strategies. The trained RL agent achieved a 14.3\% improvement in radial deviation between the target treatment contour and the ablation contour, reducing the average radial difference to 1.8 mm. Furthermore, the agent generated ablation trajectories in an average of 5.0 seconds (25 ms per step), demonstrating its potential as a valuable tool for assisting physicians and enabling closed-loop conformal ablation.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/ZCITK4CN/Jiang et al. - Simulation-Based Reinforcement Learning for Automatic Trajectory Planning in Conformal Needle-Based.pdf}
}

@misc{jiExBody2AdvancedExpressive2024,
  title = {{{ExBody2}}: {{Advanced Expressive Humanoid Whole-Body Control}}},
  shorttitle = {{{ExBody2}}},
  author = {Ji, Mazeyu and Peng, Xuanbin and Liu, Fangchen and Li, Jialong and Yang, Ge and Cheng, Xuxin and Wang, Xiaolong},
  year = {2024},
  month = dec,
  number = {arXiv:2412.13196},
  eprint = {2412.13196},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.13196},
  urldate = {2024-12-18},
  abstract = {This paper enables real-world humanoid robots to maintain stability while performing expressive motions like humans do. We propose ExBody2, a generalized whole-body tracking framework that can take any reference motion inputs and control the humanoid to mimic the motion. The model is trained in simulation with Reinforcement Learning and then transferred to the real world. It decouples keypoint tracking with velocity control, and effectively leverages a privileged teacher policy to distill precise mimic skills into the target student policy, which enables high-fidelity replication of dynamic movements such as running, crouching, dancing, and other challenging motions. We present a comprehensive qualitative and quantitative analysis of crucial design factors in the paper. We conduct our experiments on two humanoid platforms and demonstrate the superiority of our approach against state-of-the-arts, providing practical guidelines to pursue the extreme of whole-body control for humanoid robots.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/J9KCVB5N/Ji et al. - 2024 - ExBody2 Advanced Expressive Humanoid Whole-Body Control.pdf;/Users/fangyuan/Zotero/storage/BQW6Q4PV/2412.html}
}

@misc{jonesSightFinetuningGeneralist2025,
  title = {Beyond {{Sight}}: {{Finetuning Generalist Robot Policies}} with {{Heterogeneous Sensors}} via {{Language Grounding}}},
  shorttitle = {Beyond {{Sight}}},
  author = {Jones, Joshua and Mees, Oier and Sferrazza, Carmelo and Stachowicz, Kyle and Abbeel, Pieter and Levine, Sergey},
  year = {2025},
  month = jan,
  number = {arXiv:2501.04693},
  eprint = {2501.04693},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.04693},
  urldate = {2025-01-13},
  abstract = {Interacting with the world is a multi-sensory experience: achieving effective general-purpose interaction requires making use of all available modalities -- including vision, touch, and audio -- to fill in gaps from partial observation. For example, when vision is occluded reaching into a bag, a robot should rely on its senses of touch and sound. However, state-of-the-art generalist robot policies are typically trained on large datasets to predict robot actions solely from visual and proprioceptive observations. In this work, we propose FuSe, a novel approach that enables finetuning visuomotor generalist policies on heterogeneous sensor modalities for which large datasets are not readily available by leveraging natural language as a common cross-modal grounding. We combine a multimodal contrastive loss with a sensory-grounded language generation loss to encode high-level semantics. In the context of robot manipulation, we show that FuSe enables performing challenging tasks that require reasoning jointly over modalities such as vision, touch, and sound in a zero-shot setting, such as multimodal prompting, compositional cross-modal prompting, and descriptions of objects it interacts with. We show that the same recipe is applicable to widely different generalist policies, including both diffusion-based generalist policies and large vision-language-action (VLA) models. Extensive experiments in the real world show that FuSeis able to increase success rates by over 20\% compared to all considered baselines.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/FMVISYNY/Jones et al. - 2025 - Beyond Sight Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding.pdf;/Users/fangyuan/Zotero/storage/UG5H2VVW/2501.html}
}

@article{julgRefinedPolicyDistillation,
  title = {Refined {{Policy Distillation}}: {{From VLA Generalists}} to {{RL Experts}}},
  author = {J{\"u}lg, Tobias Thomas and Burgard, Wolfram and Walter, Florian},
  abstract = {Recent generalist Vision-Language-Action Models (VLAs) can perform a variety of tasks on real robots with remarkable generalization capabilities. However, reported success rates are often not on par with those of expert policies. Moreover, VLAs usually do not work out of the box and often must be fine-tuned as they are sensitive to setup changes. In this work, we present Refined Policy Distillation (RPD), an RLbased policy refinement method that enables the distillation of large generalist models into small, high-performing expert policies. The student policy is guided during the RL exploration by actions of a teacher VLA for increased sample efficiency and faster convergence. Different from previous work that focuses on applying VLAs to real-world experiments, we create finetuned versions of Octo and OpenVLA for ManiSkill2 to evaluate RPD in simulation. As our results for different manipulation tasks demonstrate, RPD enables the RL agent to learn expert policies that surpass the teacher's performance in both dense and sparse reward settings. Our approach is even robust to changes in the camera perspective and can generalize to task variations that the underlying VLA cannot solve.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/XKQ8I3AF/Jülg et al. - Refined Policy Distillation From VLA Generalists to RL Experts.pdf}
}

@article{kalashnikovScalableDeepReinforcement,
  title = {Scalable {{Deep Reinforcement Learning}} for {{Vision-Based Robotic Manipulation}}},
  author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
  pages = {23},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/UGH2PN5M/Kalashnikov et al. - Scalable Deep Reinforcement Learning for Vision-Ba.pdf}
}

@misc{kangIncorporatingTaskProgress2024,
  title = {Incorporating {{Task Progress Knowledge}} for {{Subgoal Generation}} in {{Robotic Manipulation}} through {{Image Edits}}},
  author = {Kang, Xuhui and Kuo, Yen-Ling},
  year = {2024},
  month = dec,
  number = {arXiv:2410.11013},
  eprint = {2410.11013},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.11013},
  urldate = {2024-12-19},
  abstract = {Understanding the progress of a task allows humans to not only track what has been done but also to better plan for future goals. We demonstrate TaKSIE, a novel framework that incorporates task progress knowledge into visual subgoal generation for robotic manipulation tasks. We jointly train a recurrent network with a latent diffusion model to generate the next visual subgoal based on the robot's current observation and the input language command. At execution time, the robot leverages a visual progress representation to monitor the task progress and adaptively samples the next visual subgoal from the model to guide the manipulation policy. We train and validate our model in simulated and real-world robotic tasks, achieving state-of-the-art performance on the CALVIN manipulation benchmark. We find that the inclusion of task progress knowledge can improve the robustness of trained policy for different initial robot poses or various movement speeds during demonstrations. The project website can be found at https://live-robotics-uva.github.io/TaKSIE/ .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/WQM6SDHA/Kang and Kuo - 2024 - Incorporating Task Progress Knowledge for Subgoal Generation in Robotic Manipulation through Image E.pdf;/Users/fangyuan/Zotero/storage/2KP39PZD/2410.html}
}

@inproceedings{karatzoglouLearningRankRecommender2013,
  title = {Learning to Rank for Recommender Systems},
  booktitle = {Proceedings of the 7th {{ACM}} Conference on {{Recommender}} Systems},
  author = {Karatzoglou, Alexandros and Baltrunas, Linas and Shi, Yue},
  year = {2013},
  month = oct,
  series = {{{RecSys}} '13},
  pages = {493--494},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2507157.2508063},
  urldate = {2022-12-19},
  abstract = {Recommender system aim at providing a personalized list of items ranked according to the preferences of the user, as such ranking methods are at the core of many recommendation algorithms. The topic of this tutorial focuses on the cutting-edge algorithmic development in the area of recommender systems. This tutorial will provide an in depth picture of the progress of ranking models in the field, summarizing the strengths and weaknesses of existing methods, and discussing open issues that could be promising for future research in the community. A qualitative and quantitative comparison between different models will be provided while we will also highlight recent developments in the areas of Reinforcement Learning.},
  isbn = {978-1-4503-2409-0},
  keywords = {collaborative filtering,learning to rank,ranking,recommender systems},
  file = {/Users/fangyuan/Zotero/storage/3NICLD42/Karatzoglou et al. - 2013 - Learning to rank for recommender systems.pdf}
}

@misc{kareerEgoMimicScalingImitation2024,
  title = {{{EgoMimic}}: {{Scaling Imitation Learning}} via {{Egocentric Video}}},
  shorttitle = {{{EgoMimic}}},
  author = {Kareer, Simar and Patel, Dhruv and Punamiya, Ryan and Mathur, Pranay and Cheng, Shuo and Wang, Chen and Hoffman, Judy and Xu, Danfei},
  year = {2024},
  month = oct,
  number = {arXiv:2410.24221},
  eprint = {2410.24221},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.24221},
  urldate = {2024-12-06},
  abstract = {The scale and diversity of demonstration data required for imitation learning is a significant challenge. We present EgoMimic, a full-stack framework which scales manipulation via human embodiment data, specifically egocentric human videos paired with 3D hand tracking. EgoMimic achieves this through: (1) a system to capture human embodiment data using the ergonomic Project Aria glasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap to human data, (3) cross-domain data alignment techniques, and (4) an imitation learning architecture that co-trains on human and robot data. Compared to prior works that only extract high-level intent from human videos, our approach treats human and robot data equally as embodied demonstration data and learns a unified policy from both data sources. EgoMimic achieves significant improvement on a diverse set of long-horizon, single-arm and bimanual manipulation tasks over state-of-the-art imitation learning methods and enables generalization to entirely new scenes. Finally, we show a favorable scaling trend for EgoMimic, where adding 1 hour of additional hand data is significantly more valuable than 1 hour of additional robot data. Videos and additional information can be found at https://egomimic.github.io/},
  archiveprefix = {arXiv},
  keywords = {Imitation Learning,Manipulation},
  file = {/Users/fangyuan/Zotero/storage/VAKYME7Y/Kareer et al. - 2024 - EgoMimic Scaling Imitation Learning via Egocentric Video.pdf;/Users/fangyuan/Zotero/storage/IYHW8ULG/2410.html}
}

@misc{kawaharazukaRealWorldRobotApplications2024,
  title = {Real-{{World Robot Applications}} of {{Foundation Models}}: {{A Review}}},
  shorttitle = {Real-{{World Robot Applications}} of {{Foundation Models}}},
  author = {Kawaharazuka, Kento and Matsushima, Tatsuya and Gambardella, Andrew and Guo, Jiaxian and Paxton, Chris and Zeng, Andy},
  year = {2024},
  month = oct,
  number = {arXiv:2402.05741},
  eprint = {2402.05741},
  publisher = {arXiv},
  urldate = {2024-10-27},
  abstract = {Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.},
  archiveprefix = {arXiv},
  keywords = {Survey},
  file = {/Users/fangyuan/Zotero/storage/6PLKSSL9/Kawaharazuka et al. - 2024 - Real-World Robot Applications of Foundation Models A Review.pdf;/Users/fangyuan/Zotero/storage/SHAVJKXU/2402.html}
}

@misc{khazatskyDROIDLargeScaleInTheWild2024,
  title = {{{DROID}}: {{A Large-Scale In-The-Wild Robot Manipulation Dataset}}},
  shorttitle = {{{DROID}}},
  author = {Khazatsky, Alexander and Pertsch, Karl and Nair, Suraj and Balakrishna, Ashwin and Dasari, Sudeep and Karamcheti, Siddharth and Nasiriany, Soroush and Srirama, Mohan Kumar and Chen, Lawrence Yunliang and Ellis, Kirsty and Fagan, Peter David and Hejna, Joey and Itkina, Masha and Lepert, Marion and Ma, Yecheng Jason and Miller, Patrick Tree and Wu, Jimmy and Belkhale, Suneel and Dass, Shivin and Ha, Huy and Jain, Arhan and Lee, Abraham and Lee, Youngwoon and Memmel, Marius and Park, Sungjae and Radosavovic, Ilija and Wang, Kaiyuan and Zhan, Albert and Black, Kevin and Chi, Cheng and Hatch, Kyle Beltran and Lin, Shan and Lu, Jingpei and Mercat, Jean and Rehman, Abdul and Sanketi, Pannag R. and Sharma, Archit and Simpson, Cody and Vuong, Quan and Walke, Homer Rich and Wulfe, Blake and Xiao, Ted and Yang, Jonathan Heewon and Yavary, Arefeh and Zhao, Tony Z. and Agia, Christopher and Baijal, Rohan and Castro, Mateo Guaman and Chen, Daphne and Chen, Qiuyu and Chung, Trinity and Drake, Jaimyn and Foster, Ethan Paul and Gao, Jensen and Herrera, David Antonio and Heo, Minho and Hsu, Kyle and Hu, Jiaheng and Jackson, Donovon and Le, Charlotte and Li, Yunshuang and Lin, Kevin and Lin, Roy and Ma, Zehan and Maddukuri, Abhiram and Mirchandani, Suvir and Morton, Daniel and Nguyen, Tony and O'Neill, Abigail and Scalise, Rosario and Seale, Derick and Son, Victor and Tian, Stephen and Tran, Emi and Wang, Andrew E. and Wu, Yilin and Xie, Annie and Yang, Jingyun and Yin, Patrick and Zhang, Yunchu and Bastani, Osbert and Berseth, Glen and Bohg, Jeannette and Goldberg, Ken and Gupta, Abhinav and Gupta, Abhishek and Jayaraman, Dinesh and Lim, Joseph J. and Malik, Jitendra and {Mart{\'i}n-Mart{\'i}n}, Roberto and Ramamoorthy, Subramanian and Sadigh, Dorsa and Song, Shuran and Wu, Jiajun and Yip, Michael C. and Zhu, Yuke and Kollar, Thomas and Levine, Sergey and Finn, Chelsea},
  year = {2024},
  month = mar,
  number = {arXiv:2403.12945},
  eprint = {2403.12945},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.12945},
  urldate = {2025-02-18},
  abstract = {The creation of large, diverse, high-quality robot manipulation datasets is an important stepping stone on the path toward more capable and robust robotic manipulation policies. However, creating such datasets is challenging: collecting robot manipulation data in diverse environments poses logistical and safety challenges and requires substantial investments in hardware and human labour. As a result, even the most general robot manipulation policies today are mostly trained on data collected in a small number of environments with limited scene and task diversity. In this work, we introduce DROID (Distributed Robot Interaction Dataset), a diverse robot manipulation dataset with 76k demonstration trajectories or 350 hours of interaction data, collected across 564 scenes and 84 tasks by 50 data collectors in North America, Asia, and Europe over the course of 12 months. We demonstrate that training with DROID leads to policies with higher performance and improved generalization ability. We open source the full dataset, policy learning code, and a detailed guide for reproducing our robot hardware setup.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/8HXW53WT/Khazatsky et al. - 2024 - DROID A Large-Scale In-The-Wild Robot Manipulation Dataset.pdf;/Users/fangyuan/Zotero/storage/34ESCKC3/2403.html}
}

@misc{khazatskyWhatCanHere2021,
  title = {What {{Can I Do Here}}? {{Learning New Skills}} by {{Imagining Visual Affordances}}},
  shorttitle = {What {{Can I Do Here}}?},
  author = {Khazatsky, Alexander and Nair, Ashvin and Jing, Daniel and Levine, Sergey},
  year = {2021},
  month = jun,
  number = {arXiv:2106.00671},
  eprint = {2106.00671},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-09-20},
  abstract = {A generalist robot equipped with learned skills must be able to perform many tasks in many different environments. However, zero-shot generalization to new settings is not always possible. When the robot encounters a new environment or object, it may need to finetune some of its previously learned skills to accommodate this change. But crucially, previously learned behaviors and models should still be suitable to accelerate this relearning. In this paper, we aim to study how generative models of possible outcomes can allow a robot to learn visual representations of affordances, so that the robot can sample potentially possible outcomes in new situations, and then further train its policy to achieve those outcomes. In effect, prior data is used to learn what kinds of outcomes may be possible, such that when the robot encounters an unfamiliar setting, it can sample potential outcomes from its model, attempt to reach them, and thereby update both its skills and its outcome model. This approach, visuomotor affordance learning (VAL), can be used to train goal-conditioned policies that operate on raw image inputs, and can rapidly learn to manipulate new objects via our proposed affordance-directed exploration scheme. We show that VAL can utilize prior data to solve real-world tasks such drawer opening, grasping, and placing objects in new scenes with only five minutes of online experience in the new scene.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/BPNF9EPR/Khazatsky et al. - 2021 - What Can I Do Here Learning New Skills by Imagini.pdf;/Users/fangyuan/Zotero/storage/449QD6E8/2106.html}
}

@misc{kimOpenVLAOpenSourceVisionLanguageAction2024,
  title = {{{OpenVLA}}: {{An Open-Source Vision-Language-Action Model}}},
  shorttitle = {{{OpenVLA}}},
  author = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
  year = {2024},
  month = jun,
  number = {arXiv:2406.09246},
  eprint = {2406.09246},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-16},
  abstract = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5\% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4\%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Action Tokenizer,Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/DQ7DBSLF/2406.pdf}
}

@misc{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2022},
  month = dec,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-09-26},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/4VXB9VSY/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf}
}

@misc{kostrikovOfflineReinforcementLearning2021,
  title = {Offline {{Reinforcement Learning}} with {{Implicit Q-Learning}}},
  author = {Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  year = {2021},
  month = oct,
  number = {arXiv:2110.06169},
  eprint = {2110.06169},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.06169},
  urldate = {2023-02-14},
  abstract = {Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function. Then, we extract the policy via advantage-weighted behavioral cloning. We dub our method implicit Q-learning (IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/Y3XVHCZS/Kostrikov et al. - 2021 - Offline Reinforcement Learning with Implicit Q-Lea.pdf;/Users/fangyuan/Zotero/storage/ZMTWEZQW/2110.html}
}

@misc{kubaCliqueformerModelBasedOptimization2024,
  title = {Cliqueformer: {{Model-Based Optimization}} with {{Structured Transformers}}},
  shorttitle = {Cliqueformer},
  author = {Kuba, Jakub Grudzien and Abbeel, Pieter and Levine, Sergey},
  year = {2024},
  month = oct,
  number = {arXiv:2410.13106},
  eprint = {2410.13106},
  publisher = {arXiv},
  urldate = {2024-10-18},
  abstract = {Expressive large-scale neural networks enable training powerful models for prediction tasks. However, in many engineering and science domains, such models are intended to be used not just for prediction, but for design -- e.g., creating new proteins that serve as effective therapeutics, or creating new materials or chemicals that maximize a downstream performance measure. Thus, researchers have recently grown an interest in building deep learning methods that solve offline {\textbackslash}emph\{model-based optimization\} (MBO) problems, in which design candidates are optimized with respect to surrogate models learned from offline data. However, straightforward application of predictive models that are effective at predicting in-distribution properties of a design are not necessarily the best suited for use in creating new designs. Thus, the most successful algorithms that tackle MBO draw inspiration from reinforcement learning and generative modeling to meet the in-distribution constraints. Meanwhile, recent theoretical works have observed that exploiting the structure of the target black-box function is an effective strategy for solving MBO from offline data. Unfortunately, discovering such structure remains an open problem. In this paper, following first principles, we develop a model that learns the structure of an MBO task and empirically leads to improved designs. To this end, we introduce {\textbackslash}emph\{Cliqueformer\} -- a scalable transformer-based architecture that learns the black-box function's structure in the form of its {\textbackslash}emph\{functional graphical model\} (FGM), thus bypassing the problem of distribution shift, previously tackled by conservative approaches. We evaluate Cliqueformer on various tasks, ranging from high-dimensional black-box functions from MBO literature to real-world tasks of chemical and genetic design, consistently demonstrating its state-of-the-art performance.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/GYY5XIDY/Kuba et al. - 2024 - Cliqueformer Model-Based Optimization with Structured Transformers.pdf;/Users/fangyuan/Zotero/storage/D8WPKLNR/2410.html}
}

@misc{kubaCliqueformerModelBasedOptimization2025,
  title = {Cliqueformer: {{Model-Based Optimization}} with {{Structured Transformers}}},
  shorttitle = {Cliqueformer},
  author = {Kuba, Jakub Grudzien and Abbeel, Pieter and Levine, Sergey},
  year = {2025},
  month = jan,
  number = {arXiv:2410.13106},
  eprint = {2410.13106},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13106},
  urldate = {2025-02-07},
  abstract = {Large neural networks excel at prediction tasks, but their application to design problems, such as protein engineering or materials discovery, requires solving offline model-based optimization (MBO) problems. While predictive models may not directly translate to effective design, recent MBO algorithms incorporate reinforcement learning and generative modeling approaches. Meanwhile, theoretical work suggests that exploiting the target function's structure can enhance MBO performance. We present Cliqueformer, a transformer-based architecture that learns the black-box function's structure through functional graphical models (FGM), addressing distribution shift without relying on explicit conservative approaches. Across various domains, including chemical and genetic design tasks, Cliqueformer demonstrates superior performance compared to existing methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/T2TQHIWR/Kuba et al. - 2025 - Cliqueformer Model-Based Optimization with Structured Transformers.pdf;/Users/fangyuan/Zotero/storage/9KD9YNAF/2410.html}
}

@misc{kubaFunctionalGraphicalModels2024,
  title = {Functional {{Graphical Models}}: {{Structure Enables Offline Data-Driven Optimization}}},
  shorttitle = {Functional {{Graphical Models}}},
  author = {Kuba, Jakub Grudzien and Uehara, Masatoshi and Abbeel, Pieter and Levine, Sergey},
  year = {2024},
  month = oct,
  number = {arXiv:2401.05442},
  eprint = {2401.05442},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.05442},
  urldate = {2024-10-18},
  abstract = {While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomposing the original high-dimensional optimization problem into smaller sub-problems. This allows us to derive much more practical regret bounds for DDO, and the result implies that DDO with FGMs can achieve nearly optimal designs in situations where naive approaches fail due to insufficient coverage of the offline data. We further present a data-driven optimization algorithm that inferes the FGM structure itself, either over the original input variables or a latent variable representation of the inputs.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/IKSFZHF8/Kuba et al. - 2024 - Functional Graphical Models Structure Enables Offline Data-Driven Optimization.pdf;/Users/fangyuan/Zotero/storage/NVZYZJWP/2401.html}
}

@article{kumarOFFLINEQLEARNINGDIVERSE2023,
  title = {{{OFFLINE Q-LEARNING ON DIVERSE MULTI-TASK DATA BOTH SCALES AND GENERALIZES}}},
  author = {Kumar, Aviral and Agarwal, Rishabh and Geng, Xinyang and Tucker, George and Levine, Sergey},
  year = {2023},
  abstract = {The potential of offline reinforcement learning (RL) is that high-capacity models trained on large, heterogeneous datasets can lead to agents that generalize broadly, analogously to similar advances in vision and NLP. However, recent works argue that offline RL methods encounter unique challenges to scaling up model capacity. Drawing on the learnings from these works, we re-examine previous design choices and find that with appropriate choices: ResNets, cross-entropy based distributional backups, and feature normalization, offline Q-learning algorithms exhibit strong performance that scales with model capacity. Using multi-task Atari as a testbed for scaling and generalization, we train a single policy on 40 games with near-human performance using up-to 80 million parameter networks, finding that model performance scales favorably with capacity. In contrast to prior work, we extrapolate beyond dataset performance even when trained entirely on a large (400M transitions) but highly suboptimal dataset (51\% human-level performance). Compared to return-conditioned supervised approaches, offline Q-learning scales similarly with model capacity and has better performance, especially when the dataset is suboptimal. Finally, we show that offline Q-learning with a diverse dataset is sufficient to learn powerful representations that facilitate rapid transfer to novel games and fast online learning on new variations of a training game, improving over existing state-of-the-art representation learning approaches.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/3A5GX9ES/Kumar et al. - 2023 - OFFLINE Q-LEARNING ON DIVERSE MULTI-TASK DATA BOTH.pdf}
}

@article{laiHindsightPlanner2020,
  title = {Hindsight {{Planner}}},
  author = {Lai, Yaqing and Wang, Wufan and Yang, Yunjie and Zhu, Jihong and Kuang, Minchi},
  year = {2020},
  journal = {New Zealand},
  pages = {9},
  abstract = {Goal-oriented reinforcement learning capacitates agents to accomplish variant goals, which is crucial for robotic tasks. However, the sparse-reward setting of these tasks aggravates sample inefficiency. Hindsight Experience Replay (HER) was introduced as a technique to elevate sample efficiency by imaging hindsight virtual goals for unsuccessful trajectories, which mitigates long-term domination of negative rewards. Nevertheless, there is still a gap between the distribution of hindsight goals and desired goals of the tasks, which was narrowed by lots of aimless exploration in HER. In this paper, we propose Hindsight Planner(HP) to generate several subgoals guiding the agent to explore towards the desired goal step by step, which allows the agent to exploit its local knowledge learned from achieved goals. The planner uses history trajectories to learn the structure of feasible goal space, then generalizes its knowledge to unseen goals. We have extensively evaluated our framework on a number of robotic tasks and show substantial improvements over the original HER in terms of sample efficiency and converged performance.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/LT85P43X/Lai et al. - 2020 - Hindsight Planner.pdf}
}

@article{lanierCuriosityDrivenMultiCriteriaHindsight2019,
  title = {Curiosity-{{Driven Multi-Criteria Hindsight Experience Replay}}},
  author = {Lanier, John B. and McAleer, Stephen and Baldi, Pierre},
  year = {2019},
  month = jun,
  doi = {10.48550/arXiv.1906.03710},
  urldate = {2023-02-24},
  abstract = {Dealing with sparse rewards is a longstanding challenge in reinforcement learning. The recent use of hindsight methods have achieved success on a variety of sparse-reward tasks, but they fail on complex tasks such as stacking multiple blocks with a robot arm in simulation. Curiosity-driven exploration using the prediction error of a learned dynamics model as an intrinsic reward has been shown to be effective for exploring a number of sparse-reward environments. We present a method that combines hindsight with curiosity-driven exploration and curriculum learning in order to solve the challenging sparse-reward block stacking task. We are the first to stack more than two blocks using only sparse reward without human demonstrations.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/BWIIYEXW/Lanier et al. - 2019 - Curiosity-Driven Multi-Criteria Hindsight Experien.pdf}
}

@article{lenzBoostingLowData2024,
  title = {Boosting {{Low Data PINN Robustness}} with {{Transfer Learning}}},
  author = {Lenz, Cederic and Bause, Maximilian and Henke, Christian and Tr{\"a}chtler, Ansgar},
  year = {2024},
  journal = {Robotics and Mechatronics},
  abstract = {Physics Informed Neural Networks (PINNs) have emerged as a potent tool for integrating domain knowledge of physical systems into the learning process of neural networks. Traditional PINNs dedicate a substantial portion of training effort to encode underlying physical relations and dynamics, while simultaneously mitigating data loss. However, especially under the circumstances of low available data, the performance and accuracy of PINNs are often sensitive to the initial selection of weights and biases, posing a challenge for their widespread application. In this paper, a novel two-phase approach is presented addressing the robustness of PINNs in these situations. The first phase involves an adaptive initialization strategy that sets the stage for enhanced learning efficiency, while the second phase finetunes the network by focusing on the discrepancies between learned and true dynamics. The approach is designed to reduce the dependency on initial values and to foster a more stable learning process, ultimately leading to a PINN solution with improved robustness and reliability. The efficacy of the proposed methodology is demonstrated through a step-bystep guideline showcasing its potential to facilitate the adoption of PINNs in complex physical systems modeling. The validation of the method is conducted on data and physical knowledge of a hot forming line.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/KVHTHJFC/Lenz et al. - 2024 - Boosting Low Data PINN Robustness with Transfer Le.pdf}
}

@article{levineEndtoEndTrainingDeep,
  title = {End-to-{{End Training}} of {{Deep Visuomotor Policies}}},
  author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-toend provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/P67GU6HH/Levine et al. - End-to-End Training of Deep Visuomotor Policies.pdf}
}

@inproceedings{levineLearningContactrichManipulation2015,
  title = {Learning Contact-Rich Manipulation Skills with Guided Policy Search},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Levine, Sergey and Wagener, Nolan and Abbeel, Pieter},
  year = {2015},
  month = may,
  pages = {156--163},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/ICRA.2015.7138994},
  urldate = {2022-09-17},
  abstract = {Autonomous learning of object manipulation skills can enable robots to acquire rich behavioral repertoires that scale to the variety of objects found in the real world. However, current motion skill learning methods typically restrict the behavior to a compact, low-dimensional representation, limiting its expressiveness and generality. In this paper, we extend a recently developed policy search method [1] and use it to learn a range of dynamic manipulation behaviors with highly general policy representations, without using known models or example demonstrations. Our approach learns a set of trajectories for the desired motion skill by using iteratively refitted time-varying linear models, and then unifies these trajectories into a single control policy that can generalize to new situations. To enable this method to run on a real robot, we introduce several improvements that reduce the sample count and automate parameter selection. We show that our method can acquire fast, fluent behaviors after only minutes of interaction time, and can learn robust controllers for complex tasks, including putting together a toy airplane, stacking tight-fitting lego blocks, placing wooden rings onto tight-fitting pegs, inserting a shoe tree into a shoe, and screwing bottle caps onto bottles.},
  isbn = {978-1-4799-6923-4},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/S56R9Y98/Levine et al. - 2015 - Learning contact-rich manipulation skills with gui.pdf}
}

@article{levineLearningNeuralNetwork,
  title = {Learning {{Neural Network Policies}} with {{Guided Policy Search}} under {{Unknown Dynamics}}},
  author = {Levine, Sergey None and Abbeel, Pieter},
  pages = {9},
  abstract = {We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/QCXM5NLA/Levine and Abbeel - Learning Neural Network Policies with Guided Polic.pdf}
}

@misc{lewRoboticTableWiping2022,
  title = {Robotic {{Table Wiping}} via {{Reinforcement Learning}} and {{Whole-body Trajectory Optimization}}},
  author = {Lew, Thomas and Singh, Sumeet and Prats, Mario and Bingham, Jeffrey and Weisz, Jonathan and Holson, Benjie and Zhang, Xiaohan and Sindhwani, Vikas and Lu, Yao and Xia, Fei and Xu, Peng and Zhang, Tingnan and Tan, Jie and Gonzalez, Montserrat},
  year = {2022},
  month = oct,
  number = {arXiv:2210.10865},
  eprint = {2210.10865},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2023-04-10},
  abstract = {We propose a framework to enable multipurpose assistive mobile robots to autonomously wipe tables to clean spills and crumbs. This problem is challenging, as it requires planning wiping actions while reasoning over uncertain latent dynamics of crumbs and spills captured via high-dimensional visual observations. Simultaneously, we must guarantee constraints satisfaction to enable safe deployment in unstructured cluttered environments. To tackle this problem, we first propose a stochastic differential equation to model crumbs and spill dynamics and absorption with a robot wiper. Using this model, we train a vision-based policy for planning wiping actions in simulation using reinforcement learning (RL). To enable zeroshot sim-to-real deployment, we dovetail the RL policy with a whole-body trajectory optimization framework to compute base and arm joint trajectories that execute the desired wiping motions while guaranteeing constraints satisfaction. We extensively validate our approach in simulation and on hardware.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/UFS3CSPZ/Lew et al. - 2022 - Robotic Table Wiping via Reinforcement Learning an.pdf}
}

@misc{liADMAcceleratedDiffusion2024,
  title = {{{ADM}}: {{Accelerated Diffusion Model}} via {{Estimated Priors}} for {{Robust Motion Prediction}} under {{Uncertainties}}},
  shorttitle = {{{ADM}}},
  author = {Li, Jiahui and Shen, Tianle and Gu, Zekai and Sun, Jiawei and Yuan, Chengran and Han, Yuhang and Sun, Shuo and Ang Jr, Marcelo H.},
  year = {2024},
  month = may,
  number = {arXiv:2405.00797},
  eprint = {2405.00797},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-04},
  abstract = {Motion prediction is a challenging problem in autonomous driving as it demands the system to comprehend stochastic dynamics and the multi-modal nature of real-world agent interactions. Diffusion models have recently risen to prominence, and have proven particularly effective in pedestrian motion prediction tasks. However, the significant time consumption and sensitivity to noise have limited the real-time predictive capability of diffusion models. In response to these impediments, we propose a novel diffusion-based, acceleratable framework that adeptly predicts future trajectories of agents with enhanced resistance to noise. The core idea of our model is to learn a coarse-grained prior distribution of trajectory, which can skip a large number of denoise steps. This advancement not only boosts sampling efficiency but also maintains the fidelity of prediction accuracy. Our method meets the rigorous real-time operational standards essential for autonomous vehicles, enabling prompt trajectory generation that is vital for secure and efficient navigation. Through extensive experiments, our method speeds up the inference time to 136ms compared to standard diffusion model, and achieves significant improvement in multi-agent motion prediction on the Argoverse 1 motion forecasting dataset.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/TEWA9YNB/Li et al. - 2024 - ADM Accelerated Diffusion Model via Estimated Pri.pdf;/Users/fangyuan/Zotero/storage/GX2RB3DF/2405.html}
}

@article{liangSelfSupervisedLearningLongHorizon,
  title = {Self-{{Supervised Learning}} of {{Long-Horizon Manipulation Tasks}} with {{Finite-State Task Machines}}},
  author = {Liang, Junchi and Boularias, Abdeslam},
  pages = {14},
  abstract = {We consider the problem of a robot learning to manipulate unknown objects while using them to perform a complex task that is composed of several sub-tasks. The robot receives 6D poses of the objects along with their semantic labels, and executes nonprehensile actions on them. The robot does not receive any feedback regarding the task until the end of an episode, where a binary reward indicates success or failure in performing the task. Moreover, certain attributes of objects cannot be always observed, so the robot needs to learn to remember pertinent past actions that it executed. We propose to solve this problem by simultaneously learning a low-level control policy and a highlevel finite-state task machine that keeps track of the progress made by the robot in solving the various sub-tasks and guides the low-level policy. Several experiments in simulation clearly show that the proposed approach is efficient at solving complex robotic tasks without any supervision.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/HKW49AEQ/Liang and Boularias - Self-Supervised Learning of Long-Horizon Manipulat.pdf}
}

@inproceedings{liangSkillDiffuserInterpretableHierarchical2024,
  title = {{{SkillDiffuser}}: {{Interpretable Hierarchical Planning}} via {{Skill Abstractions}} in {{Diffusion-Based Task Execution}}},
  shorttitle = {{{SkillDiffuser}}},
  booktitle = {2024 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Liang, Zhixuan and Mu, Yao and Ma, Hengbo and Tomizuka, Masayoshi and Ding, Mingyu and Luo, Ping},
  year = {2024},
  month = jun,
  pages = {16467--16476},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR52733.2024.01558},
  urldate = {2024-12-07},
  abstract = {Diffusion models have demonstrated strong potential for robotic trajectory planning. However, generating coherent trajectories from high-level instructions remains challenging, especially for long-range composition tasks requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end hierarchical planning framework integrating interpretable skill learning with conditional diffusion planning to address this problem. At the higher level, the skill abstraction module learns discrete, human-understandable skill representations from visual observations and language instructions. These learned skill embeddings are then used to condition the diffusion model to generate customized latent trajectories aligned with the skills. This allows generating diverse state trajectories that adhere to the learnable skills. By integrating skill learning with conditional trajectory generation, SkillDiffuser produces coherent behavior following abstract instructions across diverse tasks. Experiments on multi-task robotic manipulation benchmarks like Meta-World and LOReL demonstrate state-of-the-art performance and human-interpretable skill representations from SkillDiffuser. More visualization results and information could be found on our website.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-5300-6},
  langid = {english},
  keywords = {Diffusion,Manipulation,Subgoal},
  file = {/Users/fangyuan/Zotero/storage/7ZLJKMPT/Liang et al. - 2024 - SkillDiffuser Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Ex.pdf}
}

@misc{liangSkillDiffuserInterpretableHierarchical2024a,
  title = {{{SkillDiffuser}}: {{Interpretable Hierarchical Planning}} via {{Skill Abstractions}} in {{Diffusion-Based Task Execution}}},
  shorttitle = {{{SkillDiffuser}}},
  author = {Liang, Zhixuan and Mu, Yao and Ma, Hengbo and Tomizuka, Masayoshi and Ding, Mingyu and Luo, Ping},
  year = {2024},
  month = mar,
  number = {arXiv:2312.11598},
  eprint = {2312.11598},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.11598},
  urldate = {2024-03-16},
  abstract = {Diffusion models have demonstrated strong potential for robotic trajectory planning. However, generating coherent trajectories from high-level instructions remains challenging, especially for long-range composition tasks requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end hierarchical planning framework integrating interpretable skill learning with conditional diffusion planning to address this problem. At the higher level, the skill abstraction module learns discrete, human-understandable skill representations from visual observations and language instructions. These learned skill embeddings are then used to condition the diffusion model to generate customized latent trajectories aligned with the skills. This allows generating diverse state trajectories that adhere to the learnable skills. By integrating skill learning with conditional trajectory generation, SkillDiffuser produces coherent behavior following abstract instructions across diverse tasks. Experiments on multi-task robotic manipulation benchmarks like Meta-World and LOReL demonstrate state-of-the-art performance and human-interpretable skill representations from SkillDiffuser. More visualization results and information could be found on our website.},
  archiveprefix = {arXiv},
  keywords = {Diffusion},
  file = {/Users/fangyuan/Zotero/storage/UW5DZKWT/Liang et al. - 2024 - SkillDiffuser Interpretable Hierarchical Planning.pdf;/Users/fangyuan/Zotero/storage/RZ32P6L2/2312.html}
}

@misc{liCogACTFoundationalVisionLanguageAction2024,
  title = {{{CogACT}}: {{A Foundational Vision-Language-Action Model}} for {{Synergizing Cognition}} and {{Action}} in {{Robotic Manipulation}}},
  shorttitle = {{{CogACT}}},
  author = {Li, Qixiu and Liang, Yaobo and Wang, Zeyu and Luo, Lin and Chen, Xi and Liao, Mozheng and Wei, Fangyun and Deng, Yu and Xu, Sicheng and Zhang, Yizhong and Wang, Xiaofan and Liu, Bei and Fu, Jianlong and Bao, Jianmin and Chen, Dong and Shi, Yuanchun and Yang, Jiaolong and Guo, Baining},
  year = {2024},
  month = nov,
  number = {arXiv:2411.19650},
  eprint = {2411.19650},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.19650},
  urldate = {2025-04-07},
  abstract = {The advancement of large Vision-Language-Action (VLA) models has significantly improved robotic manipulation in terms of language-guided task execution and generalization to unseen scenarios. While existing VLAs adapted from pretrained large Vision-Language-Models (VLM) have demonstrated promising generalizability, their task performance is still unsatisfactory as indicated by the low tasks success rates in different environments. In this paper, we present a new advanced VLA architecture derived from VLM. Unlike previous works that directly repurpose VLM for action prediction by simple action quantization, we propose a omponentized VLA architecture that has a specialized action module conditioned on VLM output. We systematically study the design of the action module and demonstrates the strong performance enhancement with diffusion action transformers for action sequence modeling, as well as their favorable scaling behaviors. We also conduct comprehensive experiments and ablation studies to evaluate the efficacy of our models with varied designs. The evaluation on 5 robot embodiments in simulation and real work shows that our model not only significantly surpasses existing VLAs in task performance and but also exhibits remarkable adaptation to new robots and generalization to unseen objects and backgrounds. It exceeds the average success rates of OpenVLA which has similar model size (7B) with ours by over 35\% in simulated evaluation and 55\% in real robot experiments. It also outperforms the large RT-2-X model (55B) by 18\% absolute success rates in simulation. Code and models can be found on our project page (https://cogact.github.io/).},
  archiveprefix = {arXiv},
  keywords = {Diffusion,Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/FZHWHLCS/Li et al. - 2024 - CogACT A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic.pdf;/Users/fangyuan/Zotero/storage/UQ3HNBJS/2411.html}
}

@misc{lidayanIntrinsicallyMotivatedHumansAgents2025,
  title = {Intrinsically-{{Motivated Humans}} and {{Agents}} in {{Open-World Exploration}}},
  author = {Lidayan, Aly and Du, Yuqing and Kosoy, Eliza and Rufova, Maria and Abbeel, Pieter and Gopnik, Alison},
  year = {2025},
  month = mar,
  number = {arXiv:2503.23631},
  eprint = {2503.23631},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.23631},
  urldate = {2025-04-01},
  abstract = {What drives exploration? Understanding intrinsic motivation is a long-standing challenge in both cognitive science and artificial intelligence; numerous objectives have been proposed and used to train agents, yet there remains a gap between human and agent exploration. We directly compare adults, children, and AI agents in a complex open-ended environment, Crafter, and study how common intrinsic objectives: Entropy, Information Gain, and Empowerment, relate to their behavior. We find that only Entropy and Empowerment are consistently positively correlated with human exploration progress, indicating that these objectives may better inform intrinsic reward design for agents. Furthermore, across agents and humans we observe that Entropy initially increases rapidly, then plateaus, while Empowerment increases continuously, suggesting that state diversity may provide more signal in early exploration, while advanced exploration should prioritize control. Finally, we find preliminary evidence that private speech utterances, and particularly goal verbalizations, may aid exploration in children.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/fangyuan/Zotero/storage/NBPGZSLU/Lidayan et al. - 2025 - Intrinsically-Motivated Humans and Agents in Open-World Exploration.pdf;/Users/fangyuan/Zotero/storage/VPIJHQHV/2503.html}
}

@misc{liDerivativeFreeGuidanceContinuous2024,
  title = {Derivative-{{Free Guidance}} in {{Continuous}} and {{Discrete Diffusion Models}} with {{Soft Value-Based Decoding}}},
  author = {Li, Xiner and Zhao, Yulai and Wang, Chenyu and Scalia, Gabriele and Eraslan, Gokcen and Nair, Surag and Biancalani, Tommaso and Regev, Aviv and Levine, Sergey and Uehara, Masatoshi},
  year = {2024},
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-10-07},
  abstract = {Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require ``differentiable'' proxy models ({\textbackslash}textit\{e.g.\}, classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models ({\textbackslash}textit\{e.g.\}, classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation. The code is available at {\textbackslash}href\{https://github.com/masa-ue/SVDD\}\{https://github.com/masa-ue/SVDD\}.},
  howpublished = {https://arxiv.org/abs/2408.08252v4},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/XGAWUB7D/Li et al. - 2024 - Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding.pdf}
}

@misc{liGeneralistRobotPolicies2024,
  title = {Towards {{Generalist Robot Policies}}: {{What Matters}} in {{Building Vision-Language-Action Models}}},
  shorttitle = {Towards {{Generalist Robot Policies}}},
  author = {Li, Xinghang and Li, Peiyan and Liu, Minghuan and Wang, Dong and Liu, Jirong and Kang, Bingyi and Ma, Xiao and Kong, Tao and Zhang, Hanbo and Liu, Huaping},
  year = {2024},
  month = dec,
  number = {arXiv:2412.14058},
  eprint = {2412.14058},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.14058},
  urldate = {2025-04-13},
  abstract = {Foundation Vision Language Models (VLMs) exhibit strong capabilities in multi-modal representation learning, comprehension, and reasoning. By injecting action components into the VLMs, Vision-Language-Action Models (VLAs) can be naturally formed and also show promising performance. Existing work has demonstrated the effectiveness and generalization of VLAs in multiple scenarios and tasks. Nevertheless, the transfer from VLMs to VLAs is not trivial since existing VLAs differ in their backbones, action-prediction formulations, data distributions, and training recipes. This leads to a missing piece for a systematic understanding of the design choices of VLAs. In this work, we disclose the key factors that significantly influence the performance of VLA and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures, and when to add cross-embodiment data. The obtained results convince us firmly to explain why we need VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures, and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets, and toolkits, along with detailed training and evaluation recipes at: robovlms.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/9RCMFTY2/Li et al. - 2024 - Towards Generalist Robot Policies What Matters in Building Vision-Language-Action Models.pdf;/Users/fangyuan/Zotero/storage/4VFF8EC5/2412.html}
}

@misc{liLLaRASuperchargingRobot2025,
  title = {{{LLaRA}}: {{Supercharging Robot Learning Data}} for {{Vision-Language Policy}}},
  shorttitle = {{{LLaRA}}},
  author = {Li, Xiang and Mata, Cristina and Park, Jongwoo and Kahatapitiya, Kumara and Jang, Yoo Sung and Shang, Jinghuan and Ranasinghe, Kanchana and Burgert, Ryan and Cai, Mu and Lee, Yong Jae and Ryoo, Michael S.},
  year = {2025},
  month = jan,
  number = {arXiv:2406.20095},
  eprint = {2406.20095},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.20095},
  urldate = {2025-02-01},
  abstract = {Vision Language Models (VLMs) have recently been leveraged to generate robotic actions, forming Vision-Language-Action (VLA) models. However, directly adapting a pretrained VLM for robotic control remains challenging, particularly when constrained by a limited number of robot demonstrations. In this work, we introduce LLaRA: Large Language and Robotics Assistant, a framework that formulates robot action policy as visuo-textual conversations and enables an efficient transfer of a pretrained VLM into a powerful VLA, motivated by the success of visual instruction tuning in Computer Vision. First, we present an automated pipeline to generate conversation-style instruction tuning data for robots from existing behavior cloning datasets, aligning robotic actions with image pixel coordinates. Further, we enhance this dataset in a self-supervised manner by defining six auxiliary tasks, without requiring any additional action annotations. We show that a VLM finetuned with a limited amount of such datasets can produce meaningful action decisions for robotic control. Through experiments across multiple simulated and real-world tasks, we demonstrate that LLaRA achieves state-of-the-art performance while preserving the generalization capabilities of large language models. The code, datasets, and pretrained models are available at https://github.com/LostXine/LLaRA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/VK35TL6E/Li et al. - 2025 - LLaRA Supercharging Robot Learning Data for Vision-Language Policy.pdf;/Users/fangyuan/Zotero/storage/LTARUXX9/2406.html}
}

@inproceedings{limMultiTaskLearningSequenceConditioned2022,
  title = {Multi-{{Task Learning}} with {{Sequence-Conditioned Transporter Networks}}},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Lim, Michael H. and Zeng, Andy and Ichter, Brian and Bandari, Maryam and Coumans, Erwin and Tomlin, Claire and Schaal, Stefan and Faust, Aleksandra},
  year = {2022},
  month = may,
  pages = {2489--2496},
  doi = {10.1109/ICRA46639.2022.9812096},
  abstract = {Enabling robots to solve multiple manipulation tasks has a wide range of industrial applications. While learning-based approaches enjoy flexibility and generalizability, scaling these approaches to solve such compositional tasks remains a challenge. In this work, we aim to solve multi-task learning through the lens of sequence-conditioning and weighted sampling. First, we propose a new suite of benchmark specifically aimed at compositional tasks, MultiRavens, which allows defining custom task combinations through task modules that are inspired by industrial tasks and exemplify the difficulties in vision-based learning and planning methods. Second, we propose a vision-based end-to-end system architecture, Sequence-Conditioned Transporter Networks, which augments Goal-Conditioned Transporter Networks with sequence-conditioning and weighted sampling and can efficiently learn to solve multi-task long horizon problems. Our analysis suggests that not only the new framework significantly improves pick-and-place performance on novel 10 multi-task benchmark problems, but also the multi-task learning with weighted sampling can vastly improve learning and agent performances on individual tasks.},
  keywords = {Automation,Benchmark testing,Multitasking,Planning,Service robots,Systems architecture,Task analysis},
  file = {/Users/fangyuan/Zotero/storage/KEUYCYZB/Lim et al. - 2022 - Multi-Task Learning with Sequence-Conditioned Tran.pdf;/Users/fangyuan/Zotero/storage/MQ54SLTA/9812096.html}
}

@inproceedings{liOKAMITeachingHumanoid2024,
  title = {{{OKAMI}}: {{Teaching Humanoid Robots Manipulation Skills}} through {{Single Video Imitation}}},
  shorttitle = {{{OKAMI}}},
  booktitle = {8th {{Annual Conference}} on {{Robot Learning}}},
  author = {Li, Jinhan and Zhu, Yifeng and Xie, Yuqi and Jiang, Zhenyu and Seo, Mingyo and Pavlakos, Georgios and Zhu, Yuke},
  year = {2024},
  month = sep,
  urldate = {2024-12-07},
  abstract = {We study the problem of teaching humanoid robots manipulation skills by imitating from single video demonstrations. We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution. At the heart of our approach is object-aware retargeting, which enables the humanoid robot to mimic the human motions in an RGB-D video while adjusting to different object locations during deployment. OKAMI uses open-world vision models to identify task-relevant objects and retarget the body motions and hand poses separately. Our experiments show that OKAMI achieves strong generalizations across varying visual and spatial conditions, outperforming the state-of-the-art baseline on open-world imitation from observation. Furthermore, OKAMI rollout trajectories are leveraged to train closed-loop visuomotor policies, which achieve an average success rate of \$79.2{\textbackslash}\%\$ without the need for labor-intensive teleoperation. More videos can be found on our website https://ut-austin-rpl.github.io/OKAMI/.},
  langid = {english},
  keywords = {FFTAI,Manipulation,Retargeting},
  file = {/Users/fangyuan/Zotero/storage/MUP66L5W/Li et al. - 2024 - OKAMI Teaching Humanoid Robots Manipulation Skills through Single Video Imitation.pdf}
}

@misc{lipmanFlowMatchingGenerative2023,
  title = {Flow {{Matching}} for {{Generative Modeling}}},
  author = {Lipman, Yaron and Chen, Ricky T. Q. and {Ben-Hamu}, Heli and Nickel, Maximilian and Le, Matt},
  year = {2023},
  month = feb,
  number = {arXiv:2210.02747},
  eprint = {2210.02747},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.02747},
  urldate = {2024-12-07},
  abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/9NEZRU6U/Lipman et al. - 2023 - Flow Matching for Generative Modeling.pdf;/Users/fangyuan/Zotero/storage/2FARZKTH/2210.html}
}

@misc{liPracticalMultiObjectManipulation2019,
  title = {Towards {{Practical Multi-Object Manipulation}} Using {{Relational Reinforcement Learning}}},
  author = {Li, Richard and Jabri, Allan and Darrell, Trevor and Agrawal, Pulkit},
  year = {2019},
  month = dec,
  number = {arXiv:1912.11032},
  eprint = {1912.11032},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-02-20},
  abstract = {Learning robotic manipulation tasks using reinforcement learning with sparse rewards is currently impractical due to the outrageous data requirements. Many practical tasks require manipulation of multiple objects, and the complexity of such tasks increases with the number of objects. Learning from a curriculum of increasingly complex tasks appears to be a natural solution, but unfortunately, does not work for many scenarios. We hypothesize that the inability of the stateof-the-art algorithms to effectively utilize a task curriculum stems from the absence of inductive biases for transferring knowledge from simpler to complex tasks. We show that graph-based relational architectures overcome this limitation and enable learning of complex tasks when provided with a simple curriculum of tasks with increasing numbers of objects. We demonstrate the utility of our framework on a simulated block stacking task. Starting from scratch, our agent learns to stack six blocks into a tower. Despite using step-wise sparse rewards, our method is orders of magnitude more dataefficient and outperforms the existing state-of-the-art method that utilizes human demonstrations. Furthermore, the learned policy exhibits zero-shot generalization, successfully stacking blocks into taller towers and previously unseen configurations such as pyramids, without any further training.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/7HVXMJQG/Li et al. - 2019 - Towards Practical Multi-Object Manipulation using .pdf}
}

@misc{liReinforcementLearningVersatile2024,
  title = {Reinforcement {{Learning}} for {{Versatile}}, {{Dynamic}}, and {{Robust Bipedal Locomotion Control}}},
  author = {Li, Zhongyu and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
  year = {2024},
  month = aug,
  number = {arXiv:2401.16889},
  eprint = {2401.16889},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.16889},
  urldate = {2024-12-06},
  abstract = {This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world. The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectively using the robot's I/O history. Additionally, we identify task randomization as another key source of robustness, fostering better task generalization and compliance to disturbances. The resulting control policies can be successfully deployed on Cassie, a torque-controlled human-sized bipedal robot. This work pushes the limits of agility for bipedal robots through extensive real-world experiments. We demonstrate a diverse range of locomotion skills, including: robust standing, versatile walking, fast running with a demonstration of a 400-meter dash, and a diverse set of jumping skills, such as standing long jumps and high jumps.},
  archiveprefix = {arXiv},
  keywords = {Cassie,Locomotion},
  file = {/Users/fangyuan/Zotero/storage/XAJTKVTP/Li et al. - 2024 - Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control.pdf;/Users/fangyuan/Zotero/storage/D3U56NHL/2401.html}
}

@misc{liReinforcementLearningVersatile2024a,
  title = {Reinforcement {{Learning}} for {{Versatile}}, {{Dynamic}}, and {{Robust Bipedal Locomotion Control}}},
  author = {Li, Zhongyu and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
  year = {2024},
  month = aug,
  number = {arXiv:2401.16889},
  eprint = {2401.16889},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.16889},
  urldate = {2024-12-07},
  abstract = {This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world. The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectively using the robot's I/O history. Additionally, we identify task randomization as another key source of robustness, fostering better task generalization and compliance to disturbances. The resulting control policies can be successfully deployed on Cassie, a torque-controlled human-sized bipedal robot. This work pushes the limits of agility for bipedal robots through extensive real-world experiments. We demonstrate a diverse range of locomotion skills, including: robust standing, versatile walking, fast running with a demonstration of a 400-meter dash, and a diverse set of jumping skills, such as standing long jumps and high jumps.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/IJXC9MX4/Li et al. - 2024 - Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control.pdf;/Users/fangyuan/Zotero/storage/L664I4N3/2401.html}
}

@misc{liRobustVersatileBipedal2023,
  title = {Robust and {{Versatile Bipedal Jumping Control}} through {{Reinforcement Learning}}},
  author = {Li, Zhongyu and Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Berseth, Glen and Sreenath, Koushil},
  year = {2023},
  month = jun,
  number = {arXiv:2302.09450},
  eprint = {2302.09450},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.09450},
  urldate = {2024-12-06},
  abstract = {This work aims to push the limits of agility for bipedal robots by enabling a torque-controlled bipedal robot to perform robust and versatile dynamic jumps in the real world. We present a reinforcement learning framework for training a robot to accomplish a large variety of jumping tasks, such as jumping to different locations and directions. To improve performance on these challenging tasks, we develop a new policy structure that encodes the robot's long-term input/output (I/O) history while also providing direct access to a short-term I/O history. In order to train a versatile jumping policy, we utilize a multi-stage training scheme that includes different training stages for different objectives. After multi-stage training, the policy can be directly transferred to a real bipedal Cassie robot. Training on different tasks and exploring more diverse scenarios lead to highly robust policies that can exploit the diverse set of learned maneuvers to recover from perturbations or poor landings during real-world deployment. Such robustness in the proposed policy enables Cassie to succeed in completing a variety of challenging jump tasks in the real world, such as standing long jumps, jumping onto elevated platforms, and multi-axes jumps.},
  archiveprefix = {arXiv},
  keywords = {Cassie,Locomotion},
  file = {/Users/fangyuan/Zotero/storage/VHEJML6R/Li et al. - 2023 - Robust and Versatile Bipedal Jumping Control through Reinforcement Learning.pdf;/Users/fangyuan/Zotero/storage/4MUNZHM2/2302.html}
}

@misc{liSolvingCompositionalReinforcement2021,
  title = {Solving {{Compositional Reinforcement Learning Problems}} via {{Task Reduction}}},
  author = {Li, Yunfei and Wu, Yilin and Xu, Huazhe and Wang, Xiaolong and Wu, Yi},
  year = {2021},
  month = mar,
  number = {arXiv:2103.07607},
  eprint = {2103.07607},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-27},
  abstract = {We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/ view/sir-compositional.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/2JICL5H2/Li et al. - 2021 - Solving Compositional Reinforcement Learning Probl.pdf}
}

@misc{liSystem1System2025,
  title = {From {{System}} 1 to {{System}} 2: {{A Survey}} of {{Reasoning Large Language Models}}},
  shorttitle = {From {{System}} 1 to {{System}} 2},
  author = {Li, Zhong-Zhi and Zhang, Duzhen and Zhang, Ming-Liang and Zhang, Jiaxin and Liu, Zengyan and Yao, Yuxuan and Xu, Haotian and Zheng, Junhao and Wang, Pei-Jie and Chen, Xiuyi and Zhang, Yingying and Yin, Fei and Dong, Jiahua and Guo, Zhijiang and Song, Le and Liu, Cheng-Lin},
  year = {2025},
  month = feb,
  number = {arXiv:2502.17419},
  eprint = {2502.17419},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.17419},
  urldate = {2025-02-26},
  abstract = {Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time {\textbackslash}href\{https://github.com/zzli2022/Awesome-Slow-Reason-System\}\{GitHub Repository\} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/fangyuan/Zotero/storage/2LXHHNBF/Li et al. - 2025 - From System 1 to System 2 A Survey of Reasoning Large Language Models.pdf;/Users/fangyuan/Zotero/storage/R6FIFJYA/2502.html}
}

@misc{liTOPERLTransformerbasedOffPolicy2025,
  title = {{{TOP-ERL}}: {{Transformer-based Off-Policy Episodic Reinforcement Learning}}},
  shorttitle = {{{TOP-ERL}}},
  author = {Li, Ge and Tian, Dong and Zhou, Hongyi and Jiang, Xinkai and Lioutikov, Rudolf and Neumann, Gerhard},
  year = {2025},
  month = feb,
  number = {arXiv:2410.09536},
  eprint = {2410.09536},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.09536},
  urldate = {2025-02-18},
  abstract = {This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/63B2F4J9/Li et al. - 2025 - TOP-ERL Transformer-based Off-Policy Episodic Reinforcement Learning.pdf;/Users/fangyuan/Zotero/storage/RBYKFB9W/2410.html}
}

@misc{liuGoalConditionedReinforcementLearning2022,
  title = {Goal-{{Conditioned Reinforcement Learning}}: {{Problems}} and {{Solutions}}},
  shorttitle = {Goal-{{Conditioned Reinforcement Learning}}},
  author = {Liu, Minghuan and Zhu, Menghui and Zhang, Weinan},
  year = {2022},
  month = sep,
  number = {arXiv:2201.08299},
  eprint = {2201.08299},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-11-16},
  abstract = {Goal-conditioned reinforcement learning (GCRL), related to a set of complex RL problems, trains an agent to achieve different goals under particular scenarios. Compared to the standard RL solutions that learn a policy solely depending on the states or observations, GCRL additionally requires the agent to make decisions according to different goals. In this survey, we provide a comprehensive overview of the challenges and algorithms for GCRL. Firstly, we answer what the basic problems are studied in this field. Then, we explain how goals are represented and present how existing solutions are designed from different points of view. Finally, we make the conclusion and discuss potential future prospects that recent researches focus on.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/2VS4LLCW/Liu et al. - 2022 - Goal-Conditioned Reinforcement Learning Problems .pdf}
}

@misc{liuHumanHumanoidRobotsCrossEmbodiment2024,
  title = {Human-{{Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning}} from {{Demonstration}}},
  author = {Liu, Junjia and Li, Zhuo and Yu, Minghao and Dong, Zhipeng and Calinon, Sylvain and Caldwell, Darwin and Chen, Fei},
  year = {2024},
  month = dec,
  number = {arXiv:2412.15166},
  eprint = {2412.15166},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.15166},
  urldate = {2024-12-22},
  abstract = {Humanoid robots are envisioned as embodied intelligent agents capable of performing a wide range of human-level loco-manipulation tasks, particularly in scenarios requiring strenuous and repetitive labor. However, learning these skills is challenging due to the high degrees of freedom of humanoid robots, and collecting sufficient training data for humanoid is a laborious process. Given the rapid introduction of new humanoid platforms, a cross-embodiment framework that allows generalizable skill transfer is becoming increasingly critical. To address this, we propose a transferable framework that reduces the data bottleneck by using a unified digital human model as a common prototype and bypassing the need for re-training on every new robot platform. The model learns behavior primitives from human demonstrations through adversarial imitation, and the complex robot structures are decomposed into functional components, each trained independently and dynamically coordinated. Task generalization is achieved through a human-object interaction graph, and skills are transferred to different robots via embodiment-specific kinematic motion retargeting and dynamic fine-tuning. Our framework is validated on five humanoid robots with diverse configurations, demonstrating stable loco-manipulation and highlighting its effectiveness in reducing data requirements and increasing the efficiency of skill transfer across platforms.},
  archiveprefix = {arXiv},
  keywords = {Retargeting,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/VHNFZC3H/Liu et al. - 2024 - Human-Humanoid Robots Cross-Embodiment Behavior-Skill Transfer Using Decomposed Adversarial Learning.pdf;/Users/fangyuan/Zotero/storage/GAFGTNPY/2412.html}
}

@misc{liuHybridVLACollaborativeDiffusion2025,
  title = {{{HybridVLA}}: {{Collaborative Diffusion}} and {{Autoregression}} in a {{Unified Vision-Language-Action Model}}},
  shorttitle = {{{HybridVLA}}},
  author = {Liu, Jiaming and Chen, Hao and An, Pengju and Liu, Zhuoyang and Zhang, Renrui and Gu, Chenyang and Li, Xiaoqi and Guo, Ziyu and Chen, Sixiang and Liu, Mengzhen and Hou, Chengkai and Zhao, Mengdi and alex Zhou, {\relax KC} and Heng, Pheng-Ann and Zhang, Shanghang},
  year = {2025},
  month = mar,
  number = {arXiv:2503.10631},
  eprint = {2503.10631},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.10631},
  urldate = {2025-03-17},
  abstract = {Recent advancements in vision-language models (VLMs) for common-sense reasoning have led to the development of vision-language-action (VLA) models, enabling robots to perform generalized manipulation. Although existing autoregressive VLA methods leverage large-scale pretrained knowledge, they disrupt the continuity of actions. Meanwhile, some VLA methods incorporate an additional diffusion head to predict continuous actions, relying solely on VLM-extracted features, which limits their reasoning capabilities. In this paper, we introduce HybridVLA, a unified framework that seamlessly integrates the strengths of both autoregressive and diffusion policies within a single large language model, rather than simply connecting them. To bridge the generation gap, a collaborative training recipe is proposed that injects the diffusion modeling directly into the next-token prediction. With this recipe, we find that these two forms of action prediction not only reinforce each other but also exhibit varying performance across different tasks. Therefore, we design a collaborative action ensemble mechanism that adaptively fuses these two predictions, leading to more robust control. In experiments, HybridVLA outperforms previous state-of-the-art VLA methods across various simulation and real-world tasks, including both single-arm and dual-arm robots, while demonstrating stable manipulation in previously unseen configurations.},
  archiveprefix = {arXiv},
  keywords = {Autoregression,Diffusion,Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/VJS4SQ9F/Liu et al. - 2025 - HybridVLA Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model.pdf;/Users/fangyuan/Zotero/storage/X5WPS5SA/2503.html}
}

@misc{liuMAPPERMultiAgentPath2020,
  title = {{{MAPPER}}: {{Multi-Agent Path Planning}} with {{Evolutionary Reinforcement Learning}} in {{Mixed Dynamic Environments}}},
  shorttitle = {{{MAPPER}}},
  author = {Liu, Zuxin and Chen, Baiming and Zhou, Hongyi and Koushik, Guru and Hebert, Martial and Zhao, Ding},
  year = {2020},
  month = jul,
  number = {arXiv:2007.15724},
  eprint = {2007.15724},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-08},
  abstract = {Multi-agent navigation in dynamic environments is of great industrial value when deploying a large scale fleet of robot to real-world applications. This paper proposes a decentralized partially observable multi-agent path planning with evolutionary reinforcement learning (MAPPER) method to learn an effective local planning policy in mixed dynamic environments. Reinforcement learning-based methods usually suffer performance degradation on long-horizon tasks with goal-conditioned sparse rewards, so we decompose the longrange navigation task into many easier sub-tasks under the guidance of a global planner, which increases agents' performance in large environments. Moreover, most existing multiagent planning approaches assume either perfect information of the surrounding environment or homogeneity of nearby dynamic agents, which may not hold in practice. Our approach models dynamic obstacles' behavior with an image-based representation and trains a policy in mixed dynamic environments without homogeneity assumption. To ensure multi-agent training stability and performance, we propose an evolutionary training approach that can be easily scaled to large and complex environments. Experiments show that MAPPER is able to achieve higher success rates and more stable performance when exposed to a large number of non-cooperative dynamic obstacles compared with traditional reaction-based planner LRA* and the state-of-the-art learning-based method.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Multiagent Systems},
  file = {/Users/fangyuan/Zotero/storage/MC7ZL7VY/Liu et al. - 2020 - MAPPER Multi-Agent Path Planning with Evolutionar.pdf}
}

@misc{liuMimickingBenchBenchmarkGeneralizable2024,
  title = {Mimicking-{{Bench}}: {{A Benchmark}} for {{Generalizable Humanoid-Scene Interaction Learning}} via {{Human Mimicking}}},
  shorttitle = {Mimicking-{{Bench}}},
  author = {Liu, Yun and Yang, Bowen and Zhong, Licheng and Wang, He and Yi, Li},
  year = {2024},
  month = dec,
  number = {arXiv:2412.17730},
  eprint = {2412.17730},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.17730},
  urldate = {2024-12-27},
  abstract = {Learning generic skills for humanoid robots interacting with 3D scenes by mimicking human data is a key research challenge with significant implications for robotics and real-world applications. However, existing methodologies and benchmarks are constrained by the use of small-scale, manually collected demonstrations, lacking the general dataset and benchmark support necessary to explore scene geometry generalization effectively. To address this gap, we introduce Mimicking-Bench, the first comprehensive benchmark designed for generalizable humanoid-scene interaction learning through mimicking large-scale human animation references. Mimicking-Bench includes six household full-body humanoid-scene interaction tasks, covering 11K diverse object shapes, along with 20K synthetic and 3K real-world human interaction skill references. We construct a complete humanoid skill learning pipeline and benchmark approaches for motion retargeting, motion tracking, imitation learning, and their various combinations. Extensive experiments highlight the value of human mimicking for skill learning, revealing key challenges and research directions.},
  archiveprefix = {arXiv},
  keywords = {Benchmark},
  file = {/Users/fangyuan/Zotero/storage/ZWVDEPK6/Liu et al. - 2024 - Mimicking-Bench A Benchmark for Generalizable Humanoid-Scene Interaction Learning via Human Mimicki.pdf;/Users/fangyuan/Zotero/storage/P4YCTY7G/2412.html}
}

@misc{liuOKRobotWhatReally2024,
  title = {{{OK-Robot}}: {{What Really Matters}} in {{Integrating Open-Knowledge Models}} for {{Robotics}}},
  shorttitle = {{{OK-Robot}}},
  author = {Liu, Peiqi and Orru, Yaswanth and Paxton, Chris and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
  year = {2024},
  month = jan,
  number = {arXiv:2401.12202},
  eprint = {2401.12202},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-26},
  abstract = {Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5\% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot's performance increases to 82\%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules. Videos of our experiments are available on our website: https://ok-robot.github.io},
  archiveprefix = {arXiv},
  keywords = {Manipulation,Mobile Manipulation,Open Vocabulary},
  file = {/Users/fangyuan/Zotero/storage/TINFIH7P/Liu et al. - 2024 - OK-Robot What Really Matters in Integrating Open-.pdf;/Users/fangyuan/Zotero/storage/W3XN5PUC/2401.html}
}

@misc{liuOpt2SkillImitatingDynamicallyfeasible2024,
  title = {{{Opt2Skill}}: {{Imitating Dynamically-feasible Whole-Body Trajectories}} for {{Versatile Humanoid Loco-Manipulation}}},
  shorttitle = {{{Opt2Skill}}},
  author = {Liu, Fukang and Gu, Zhaoyuan and Cai, Yilin and Zhou, Ziyi and Zhao, Shijie and Jung, Hyunyoung and Ha, Sehoon and Chen, Yue and Xu, Danfei and Zhao, Ye},
  year = {2024},
  month = oct,
  number = {arXiv:2409.20514},
  eprint = {2409.20514},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.20514},
  urldate = {2024-12-06},
  abstract = {Humanoid robots are designed to perform diverse loco-manipulation tasks. However, they face challenges due to their high-dimensional and unstable dynamics, as well as the complex contact-rich nature of the tasks. Model-based optimal control methods offer precise and systematic control but are limited by high computational complexity and accurate contact sensing. On the other hand, reinforcement learning (RL) provides robustness and handles high-dimensional spaces but suffers from inefficient learning, unnatural motion, and sim-to-real gaps. To address these challenges, we introduce Opt2Skill, an end-to-end pipeline that combines model-based trajectory optimization with RL to achieve robust whole-body loco-manipulation. We generate reference motions for the Digit humanoid robot using differential dynamic programming (DDP) and train RL policies to track these trajectories. Our results demonstrate that Opt2Skill outperforms pure RL methods in both training efficiency and task performance, with optimal trajectories that account for torque limits enhancing trajectory tracking. We successfully transfer our approach to real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Digit,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/XBZNFBPU/Liu et al. - 2024 - Opt2Skill Imitating Dynamically-feasible Whole-Body Trajectories for Versatile Humanoid Loco-Manipu.pdf;/Users/fangyuan/Zotero/storage/PJ2Q975N/2409.html}
}

@misc{liuRoboMambaEfficientVisionLanguageAction2024,
  title = {{{RoboMamba}}: {{Efficient Vision-Language-Action Model}} for {{Robotic Reasoning}} and {{Manipulation}}},
  shorttitle = {{{RoboMamba}}},
  author = {Liu, Jiaming and Liu, Mengzhen and Wang, Zhenyu and An, Pengju and Li, Xiaoqi and Zhou, Kaichen and Yang, Senqiao and Zhang, Renrui and Guo, Yandong and Zhang, Shanghang},
  year = {2024},
  month = dec,
  number = {arXiv:2406.04339},
  eprint = {2406.04339},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.04339},
  urldate = {2025-01-16},
  abstract = {A fundamental objective in robot manipulation is to enable models to comprehend visual scenes and execute actions. Although existing Vision-Language-Action (VLA) models for robots can handle a range of basic tasks, they still face challenges in two areas: (1) insufficient reasoning ability to tackle complex tasks, and (2) high computational costs for VLA model fine-tuning and inference. The recently proposed state space model (SSM) known as Mamba demonstrates promising capabilities in non-trivial sequence modeling with linear inference complexity. Inspired by this, we introduce RoboMamba, an end-to-end robotic VLA model that leverages Mamba to deliver both robotic reasoning and action capabilities, while maintaining efficient fine-tuning and inference. Specifically, we first integrate the vision encoder with Mamba, aligning visual tokens with language embedding through co-training, empowering our model with visual common sense and robotic-related reasoning. To further equip RoboMamba with SE(3) pose prediction abilities, we explore an efficient fine-tuning strategy with a simple policy head. We find that once RoboMamba possesses sufficient reasoning capability, it can acquire manipulation skills with minimal fine-tuning parameters (0.1{\textbackslash}\% of the model) and time. In experiments, RoboMamba demonstrates outstanding reasoning capabilities on general and robotic evaluation benchmarks. Meanwhile, our model showcases impressive pose prediction results in both simulation and real-world experiments, achieving inference speeds 3 times faster than existing VLA models. Our project web page: https://sites.google.com/view/robomamba-web},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/Q9JQZ94E/Liu et al. - 2024 - RoboMamba Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation.pdf;/Users/fangyuan/Zotero/storage/C3H5S5LY/2406.html}
}

@article{liuTACOBenchmarkingGeneralizable,
  title = {{{TACO}}: {{Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding}}},
  author = {Liu, Yun and Yang, Haolin and Si, Xu and Liu, Ling and Li, Zipeng and Zhang, Yuxiang and Liu, Yebin and Yi, Li},
  langid = {english},
  keywords = {Dataset,Manipulation},
  file = {/Users/fangyuan/Zotero/storage/3F7ZG7HP/Liu et al. - TACO Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding.pdf}
}

@misc{liVisionLanguageFoundationModels2024,
  title = {Vision-{{Language Foundation Models}} as {{Effective Robot Imitators}}},
  author = {Li, Xinghang and Liu, Minghuan and Zhang, Hanbo and Yu, Cunjun and Xu, Jie and Wu, Hongtao and Cheang, Chilam and Jing, Ya and Zhang, Weinan and Liu, Huaping and Li, Hang and Kong, Tao},
  year = {2024},
  month = feb,
  number = {arXiv:2311.01378},
  eprint = {2311.01378},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.01378},
  urldate = {2025-03-11},
  abstract = {Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models (VLMs) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed RoboFlamingo, built upon the open-source VLMs, OpenFlamingo. Unlike prior works, RoboFlamingo utilizes pre-trained VLMs for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides RoboFlamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show RoboFlamingo can be an effective and competitive alternative to adapt VLMs to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained VLMs on manipulation tasks. We believe RoboFlamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/EYNE2GDY/Li et al. - 2024 - Vision-Language Foundation Models as Effective Robot Imitators.pdf;/Users/fangyuan/Zotero/storage/LDXMKR53/2311.html}
}

@misc{longLearningHumanoidLocomotion2024,
  title = {Learning {{Humanoid Locomotion}} with {{Perceptive Internal Model}}},
  author = {Long, Junfeng and Ren, Junli and Shi, Moji and Wang, Zirui and Huang, Tao and Luo, Ping and Pang, Jiangmiao},
  year = {2024},
  month = nov,
  number = {arXiv:2411.14386},
  eprint = {2411.14386},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.14386},
  urldate = {2024-12-06},
  abstract = {In contrast to quadruped robots that can navigate diverse terrains using a "blind" policy, humanoid robots require accurate perception for stable locomotion due to their high degrees of freedom and inherently unstable morphology. However, incorporating perceptual signals often introduces additional disturbances to the system, potentially reducing its robustness, generalizability, and efficiency. This paper presents the Perceptive Internal Model (PIM), which relies on onboard, continuously updated elevation maps centered around the robot to perceive its surroundings. We train the policy using ground-truth obstacle heights surrounding the robot in simulation, optimizing it based on the Hybrid Internal Model (HIM), and perform inference with heights sampled from the constructed elevation map. Unlike previous methods that directly encode depth maps or raw point clouds, our approach allows the robot to perceive the terrain beneath its feet clearly and is less affected by camera movement or noise. Furthermore, since depth map rendering is not required in simulation, our method introduces minimal additional computational costs and can train the policy in 3 hours on an RTX 4090 GPU. We verify the effectiveness of our method across various humanoid robots, various indoor and outdoor terrains, stairs, and various sensor configurations. Our method can enable a humanoid robot to continuously climb stairs and has the potential to serve as a foundational algorithm for the development of future humanoid control methods.},
  archiveprefix = {arXiv},
  keywords = {Locomotion,Unitree},
  file = {/Users/fangyuan/Zotero/storage/79QNZ4AI/Long et al. - 2024 - Learning Humanoid Locomotion with Perceptive Internal Model.pdf;/Users/fangyuan/Zotero/storage/VXUYSFQH/2411.html}
}

@misc{luMobileTeleVisionPredictiveMotion2024,
  title = {Mobile-{{TeleVision}}: {{Predictive Motion Priors}} for {{Humanoid Whole-Body Control}}},
  shorttitle = {Mobile-{{TeleVision}}},
  author = {Lu, Chenhao and Cheng, Xuxin and Li, Jialong and Yang, Shiqi and Ji, Mazeyu and Yuan, Chengjing and Yang, Ge and Yi, Sha and Wang, Xiaolong},
  year = {2024},
  month = dec,
  number = {arXiv:2412.07773},
  eprint = {2412.07773},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.07773},
  urldate = {2024-12-18},
  abstract = {Humanoid robots require both robust lower-body locomotion and precise upper-body manipulation. While recent Reinforcement Learning (RL) approaches provide whole-body loco-manipulation policies, they lack precise manipulation with high DoF arms. In this paper, we propose decoupling upper-body control from locomotion, using inverse kinematics (IK) and motion retargeting for precise manipulation, while RL focuses on robust lower-body locomotion. We introduce PMP (Predictive Motion Priors), trained with Conditional Variational Autoencoder (CVAE) to effectively represent upper-body motions. The locomotion policy is trained conditioned on this upper-body motion representation, ensuring that the system remains robust with both manipulation and locomotion. We show that CVAE features are crucial for stability and robustness, and significantly outperforms RL-based whole-body control in precise manipulation. With precise upper-body motion and robust lower-body locomotion control, operators can remotely control the humanoid to walk around and explore different environments, while performing diverse manipulation tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/Q9VRKT3N/Lu et al. - 2024 - Mobile-TeleVision Predictive Motion Priors for Humanoid Whole-Body Control.pdf;/Users/fangyuan/Zotero/storage/WT4XLBU6/2412.html}
}

@misc{luoGraspingDiverseObjects2024,
  title = {Grasping {{Diverse Objects}} with {{Simulated Humanoids}}},
  author = {Luo, Zhengyi and Cao, Jinkun and Christen, Sammy and Winkler, Alexander and Kitani, Kris and Xu, Weipeng},
  year = {2024},
  month = jul,
  number = {arXiv:2407.11385},
  eprint = {2407.11385},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.11385},
  urldate = {2024-09-30},
  abstract = {We present a method for controlling a simulated humanoid to grasp an object and move it to follow an object trajectory. Due to the challenges in controlling a humanoid with dexterous hands, prior methods often use a disembodied hand and only consider vertical lifts or short trajectories. This limited scope hampers their applicability for object manipulation required for animation and simulation. To close this gap, we learn a controller that can pick up a large number ({$>$}1200) of objects and carry them to follow randomly generated trajectories. Our key insight is to leverage a humanoid motion representation that provides human-like motor skills and significantly speeds up training. Using only simplistic reward, state, and object representations, our method shows favorable scalability on diverse object and trajectories. For training, we do not need dataset of paired full-body motion and object trajectories. At test time, we only require the object mesh and desired trajectories for grasping and transporting. To demonstrate the capabilities of our method, we show state-of-the-art success rates in following object trajectories and generalizing to unseen objects. Code and models will be released.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Graphics},
  file = {/Users/fangyuan/Zotero/storage/4D8JCNII/Luo et al. - 2024 - Grasping Diverse Objects with Simulated Humanoids.pdf;/Users/fangyuan/Zotero/storage/G5CJILAM/2407.html}
}

@misc{luoUnderstandingDiffusionModels2022,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  year = {2022},
  month = aug,
  number = {arXiv:2208.11970},
  eprint = {2208.11970},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2208.11970},
  urldate = {2025-01-19},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/CQQRMFIL/Luo - 2022 - Understanding Diffusion Models A Unified Perspective.pdf;/Users/fangyuan/Zotero/storage/9UHUI547/2208.html}
}

@misc{luoUniversalHumanoidMotion2024,
  title = {Universal {{Humanoid Motion Representations}} for {{Physics-Based Control}}},
  author = {Luo, Zhengyi and Cao, Jinkun and Merel, Josh and Winkler, Alexander and Huang, Jing and Kitani, Kris and Xu, Weipeng},
  year = {2024},
  month = apr,
  number = {arXiv:2310.04582},
  eprint = {2310.04582},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-06},
  abstract = {We present a universal motion representation that encompasses a comprehensive range of motor skills for physics-based humanoid control. Due to the high dimensionality of humanoids and the inherent difficulties in reinforcement learning, prior methods have focused on learning skill embeddings for a narrow range of movement styles (e.g. locomotion, game characters) from specialized motion datasets. This limited scope hampers their applicability in complex tasks. We close this gap by significantly increasing the coverage of our motion representation space. To achieve this, we first learn a motion imitator that can imitate all of human motion from a large, unstructured motion dataset. We then create our motion representation by distilling skills directly from the imitator. This is achieved by using an encoder-decoder structure with a variational information bottleneck. Additionally, we jointly learn a prior conditioned on proprioception (humanoid's own pose and velocities) to improve model expressiveness and sampling efficiency for downstream tasks. By sampling from the prior, we can generate long, stable, and diverse human motions. Using this latent space for hierarchical RL, we show that our policies solve tasks using human-like behavior. We demonstrate the effectiveness of our motion representation by solving generative tasks (e.g. strike, terrain traversal) and motion tracking using VR controllers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Graphics},
  file = {/Users/fangyuan/Zotero/storage/TAMDHAGI/Luo et al. - 2024 - Universal Humanoid Motion Representations for Physics-Based Control.pdf}
}

@misc{luUGGUnifiedGenerative2023,
  title = {{{UGG}}: {{Unified Generative Grasping}}},
  shorttitle = {{{UGG}}},
  author = {Lu, Jiaxin and Kang, Hao and Li, Haoxiang and Liu, Bo and Yang, Yiding and Huang, Qixing and Hua, Gang},
  year = {2023},
  month = nov,
  journal = {arXiv.org},
  urldate = {2024-10-03},
  abstract = {Dexterous grasping aims to produce diverse grasping postures with a high grasping success rate. Regression-based methods that directly predict grasping parameters given the object may achieve a high success rate but often lack diversity. Generation-based methods that generate grasping postures conditioned on the object can often produce diverse grasping, but they are insufficient for high grasping success due to lack of discriminative information. To mitigate, we introduce a unified diffusion-based dexterous grasp generation model, dubbed the name UGG, which operates within the object point cloud and hand parameter spaces. Our all-transformer architecture unifies the information from the object, the hand, and the contacts, introducing a novel representation of contact points for improved contact modeling. The flexibility and quality of our model enable the integration of a lightweight discriminator, benefiting from simulated discriminative data, which pushes for a high success rate while preserving high diversity. Beyond grasp generation, our model can also generate objects based on hand information, offering valuable insights into object design and studying how the generative model perceives objects. Our model achieves state-of-the-art dexterous grasping on the large-scale DexGraspNet dataset while facilitating human-centric object design, marking a significant advancement in dexterous grasping research. Our project page is https://jiaxin-lu.github.io/ugg/.},
  howpublished = {https://arxiv.org/abs/2311.16917v2},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/HWP4Y2ZG/Lu et al. - 2023 - UGG Unified Generative Grasping.pdf}
}

@misc{lynchInteractiveLanguageTalking2022,
  title = {Interactive {{Language}}: {{Talking}} to {{Robots}} in {{Real Time}}},
  shorttitle = {Interactive {{Language}}},
  author = {Lynch, Corey and Wahid, Ayzaan and Tompson, Jonathan and Ding, Tianli and Betker, James and Baruch, Robert and Armstrong, Travis and Florence, Pete},
  year = {2022},
  month = oct,
  number = {arXiv:2210.06407},
  eprint = {2210.06407},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-21},
  abstract = {We present a framework for building interactive, realtime, natural language-instructable robots in the real world, and we open source related assets (dataset, environment, benchmark, and policies). Trained with behavioral cloning on a dataset of hundreds of thousands of language-annotated trajectories, a produced policy can proficiently execute an order of magnitude more commands than previous works: specifically we estimate a 93.5\% success rate on a set of 87,000 unique natural language strings specifying raw end-to-end visuo-linguo-motor skills in the real world. We find that the same policy is capable of being guided by a human via real-time language to address a wide range of precise long-horizon rearrangement goals, e.g. ``make a smiley face out of blocks''. The dataset we release comprises nearly 600,000 language-labeled trajectories, an order of magnitude larger than prior available datasets. We hope the demonstrated results and associated assets enable further advancement of helpful, capable, natural-language-interactable robots. See videos at https://interactive-language.github.io.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/KNF9HEXG/Lynch et al. - 2022 - Interactive Language Talking to Robots in Real Time.pdf}
}

@misc{maoLearningMassiveHuman2024,
  title = {Learning from {{Massive Human Videos}} for {{Universal Humanoid Pose Control}}},
  author = {Mao, Jiageng and Zhao, Siheng and Song, Siqi and Shi, Tianheng and Ye, Junjie and Zhang, Mingtong and Geng, Haoran and Malik, Jitendra and Guizilini, Vitor and Wang, Yue},
  year = {2024},
  month = dec,
  number = {arXiv:2412.14172},
  eprint = {2412.14172},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.14172},
  urldate = {2024-12-19},
  abstract = {Scalable learning of humanoid robots is crucial for their deployment in real-world applications. While traditional approaches primarily rely on reinforcement learning or teleoperation to achieve whole-body control, they are often limited by the diversity of simulated environments and the high costs of demonstration collection. In contrast, human videos are ubiquitous and present an untapped source of semantic and motion information that could significantly enhance the generalization capabilities of humanoid robots. This paper introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot poses with corresponding text-based motion descriptions, designed to leverage this abundant data. Humanoid-X is curated through a comprehensive pipeline: data mining from the Internet, video caption generation, motion retargeting of humans to humanoid robots, and policy learning for real-world deployment. With Humanoid-X, we further train a large humanoid model, UH-1, which takes text instructions as input and outputs corresponding actions to control a humanoid robot. Extensive simulated and real-world experiments validate that our scalable training approach leads to superior generalization in text-based humanoid control, marking a significant step toward adaptable, real-world-ready humanoid robots.},
  archiveprefix = {arXiv},
  keywords = {Retargeting,Sim2Real,Unitree,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/2Z98QW9X/Mao et al. - 2024 - Learning from Massive Human Videos for Universal Humanoid Pose Control.pdf;/Users/fangyuan/Zotero/storage/ANNDME8E/2412.html}
}

@misc{maSoftDiffusionActorCritic2025,
  title = {Soft {{Diffusion Actor-Critic}}: {{Efficient Online Reinforcement Learning}} for {{Diffusion Policy}}},
  shorttitle = {Soft {{Diffusion Actor-Critic}}},
  author = {Ma, Haitong and Chen, Tianyi and Wang, Kai and Li, Na and Dai, Bo},
  year = {2025},
  month = feb,
  number = {arXiv:2502.00361},
  eprint = {2502.00361},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.00361},
  urldate = {2025-02-14},
  abstract = {Diffusion policies have achieved superior performance in imitation learning and offline reinforcement learning (RL) due to their rich expressiveness. However, the vanilla diffusion training procedure requires samples from target distribution, which is impossible in online RL since we cannot sample from the optimal policy, making training diffusion policies highly non-trivial in online RL. Backpropagating policy gradient through the diffusion process incurs huge computational costs and instability, thus being expensive and impractical. To enable efficient diffusion policy training for online RL, we propose Soft Diffusion Actor-Critic (SDAC), exploiting the viewpoint of diffusion models as noise-perturbed energy-based models. The proposed SDAC relies solely on the state-action value function as the energy functions to train diffusion policies, bypassing sampling from the optimal policy while maintaining lightweight computations. We conducted comprehensive comparisons on MuJoCo benchmarks. The empirical results show that SDAC outperforms all recent diffusion-policy online RLs on most tasks, and improves more than 120\% over soft actor-critic on complex locomotion tasks such as Humanoid and Ant.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/I7ZRVZ42/Ma et al. - 2025 - Soft Diffusion Actor-Critic Efficient Online Reinforcement Learning for Diffusion Policy.pdf;/Users/fangyuan/Zotero/storage/M69BW4CZ/2502.html}
}

@misc{maSurveyVisionLanguageActionModels2024,
  title = {A {{Survey}} on {{Vision-Language-Action Models}} for {{Embodied AI}}},
  author = {Ma, Yueen and Song, Zixing and Zhuang, Yuzheng and Hao, Jianye and King, Irwin},
  year = {2024},
  month = may,
  number = {arXiv:2405.14093},
  eprint = {2405.14093},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-22},
  abstract = {Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural networks, Transformers, and deep Q-networks. Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and speech recognition. The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal models known as vision-language-action models (VLAs). Their multimodality capability has become a foundational element in robot learning. Various methods have been proposed to enhance traits such as versatility, dexterity, and generalizability. Some models focus on refining specific components through pretraining. Others aim to develop control policies adept at predicting low-level actions. Certain VLAs serve as high-level task planners capable of decomposing long-horizon tasks into executable subtasks. Over the past few years, a myriad of VLAs have emerged, reflecting the rapid advancement of embodied AI. Therefore, it is imperative to capture the evolving landscape through a comprehensive survey.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Survey,VLA},
  file = {/Users/fangyuan/Zotero/storage/JET9HDAB/Ma et al. - 2024 - A Survey on Vision-Language-Action Models for Embodied AI.pdf}
}

@misc{maWhenLLMsStep2024,
  title = {When {{LLMs}} Step into the {{3D World}}: {{A Survey}} and {{Meta-Analysis}} of {{3D Tasks}} via {{Multi-modal Large Language Models}}},
  shorttitle = {When {{LLMs}} Step into the {{3D World}}},
  author = {Ma, Xianzheng and Bhalgat, Yash and Smart, Brandon and Chen, Shuai and Li, Xinghui and Ding, Jian and Gu, Jindong and Chen, Dave Zhenyu and Peng, Songyou and Bian, Jia-Wang and Torr, Philip H. and Pollefeys, Marc and Nie{\ss}ner, Matthias and Reid, Ian D. and Chang, Angel X. and Laina, Iro and Prisacariu, Victor Adrian},
  year = {2024},
  month = may,
  number = {arXiv:2405.10255},
  eprint = {2405.10255},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.10255},
  urldate = {2024-12-07},
  abstract = {As large language models (LLMs) evolve, their integration with 3D spatial data (3D-LLMs) has seen rapid progress, offering unprecedented capabilities for understanding and interacting with physical spaces. This survey provides a comprehensive overview of the methodologies enabling LLMs to process, understand, and generate 3D data. Highlighting the unique advantages of LLMs, such as in-context learning, step-by-step reasoning, open-vocabulary capabilities, and extensive world knowledge, we underscore their potential to significantly advance spatial comprehension and interaction within embodied Artificial Intelligence (AI) systems. Our investigation spans various 3D data representations, from point clouds to Neural Radiance Fields (NeRFs). It examines their integration with LLMs for tasks such as 3D scene understanding, captioning, question-answering, and dialogue, as well as LLM-based agents for spatial reasoning, planning, and navigation. The paper also includes a brief review of other methods that integrate 3D and language. The meta-analysis presented in this paper reveals significant progress yet underscores the necessity for novel approaches to harness the full potential of 3D-LLMs. Hence, with this paper, we aim to chart a course for future research that explores and expands the capabilities of 3D-LLMs in understanding and interacting with the complex 3D world. To support this survey, we have established a project page where papers related to our topic are organized and listed: https://github.com/ActiveVisionLab/Awesome-LLM-3D.},
  archiveprefix = {arXiv},
  keywords = {Robotic Foundation Model,Survey},
  file = {/Users/fangyuan/Zotero/storage/V8FISLZS/Ma et al. - 2024 - When LLMs step into the 3D World A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Langu.pdf;/Users/fangyuan/Zotero/storage/NWDUX3CE/2405.html}
}

@misc{mccandlishEmpiricalModelLargeBatch2018,
  title = {An {{Empirical Model}} of {{Large-Batch Training}}},
  author = {McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  year = {2018},
  month = dec,
  number = {arXiv:1812.06162},
  eprint = {1812.06162},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.06162},
  urldate = {2024-10-10},
  abstract = {In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/YLK9E28V/McCandlish et al. - 2018 - An Empirical Model of Large-Batch Training.pdf;/Users/fangyuan/Zotero/storage/9Z5YBWR3/1812.html}
}

@misc{meesCALVINBenchmarkLanguageConditioned2022,
  title = {{{CALVIN}}: {{A Benchmark}} for {{Language-Conditioned Policy Learning}} for {{Long-Horizon Robot Manipulation Tasks}}},
  shorttitle = {{{CALVIN}}},
  author = {Mees, Oier and Hermann, Lukas and {Rosete-Beas}, Erick and Burgard, Wolfram},
  year = {2022},
  month = jul,
  number = {arXiv:2112.03227},
  eprint = {2112.03227},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.03227},
  urldate = {2025-03-11},
  abstract = {General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks. Moreover, they need to acquire a diverse repertoire of general-purpose skills that allow composing long-horizon tasks by following unconstrained language instructions. In this paper, we present CALVIN (Composing Actions from Language and Vision), an open-source simulated benchmark to learn long-horizon language-conditioned tasks. Our aim is to make it possible to develop agents that can solve many robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. CALVIN tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets and supports flexible specification of sensor suites. We evaluate the agents in zero-shot to novel language instructions and to novel environments and objects. We show that a baseline model based on multi-context imitation learning performs poorly on CALVIN, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/ULUN8RR8/Mees et al. - 2022 - CALVIN A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tas.pdf;/Users/fangyuan/Zotero/storage/CZDQ5F8R/2112.html}
}

@misc{meesGroundingLanguageVisual2023,
  title = {Grounding {{Language}} with {{Visual Affordances}} over {{Unstructured Data}}},
  author = {Mees, Oier and {Borja-Diaz}, Jessica and Burgard, Wolfram},
  year = {2023},
  month = mar,
  number = {arXiv:2210.01911},
  eprint = {2210.01911},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.01911},
  urldate = {2025-04-23},
  abstract = {Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1\% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/9SWBW3TR/Mees et al. - 2023 - Grounding Language with Visual Affordances over Unstructured Data.pdf;/Users/fangyuan/Zotero/storage/VYFDK54X/2210.html}
}

@misc{meesWhatMattersLanguage2022,
  title = {What {{Matters}} in {{Language Conditioned Robotic Imitation Learning}}},
  author = {Mees, Oier and Hermann, Lukas and Burgard, Wolfram},
  year = {2022},
  month = apr,
  number = {arXiv:2204.06252},
  eprint = {2204.06252},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-08-29},
  abstract = {A long-standing goal in robotics is to build robots that can perform a wide range of daily tasks from perceptions obtained with their onboard sensors and specified only via natural language. While recently substantial advances have been achieved in language-driven robotics by leveraging end-to-end learning from pixels, there is no clear and well-understood process for making various design choices due to the underlying variation in setups. In this paper, we conduct an extensive study of the most critical challenges in learning language conditioned policies from offline free-form imitation datasets. We further identify architectural and algorithmic techniques that improve performance, such as a hierarchical decomposition of the robot control learning, a multimodal transformer encoder, discrete latent plans and a self-supervised contrastive loss that aligns video and language representations. By combining the results of our investigation with our improved model components, we are able to present a novel approach that significantly outperforms the state of the art on the challenging language conditioned long-horizon robot manipulation CALVIN benchmark. We have open-sourced our implementation to facilitate future research in learning to perform many complex manipulation skills in a row specified with natural language. Codebase and trained models available at http://hulc.cs.uni-freiburg.de},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/3EHMAXGM/Mees et al. - 2022 - What Matters in Language Conditioned Robotic Imita.pdf}
}

@article{michauxLetsMakeSplan2024,
  title = {Let's {{Make}} a {{Splan}}: {{Risk-Aware Trajectory Optimization}} in a {{Normalized Gaussian Splat}}},
  author = {Michaux, Jonathan and Isaacson, Seth and Adu, Challen Enninful and Li, Adam and Swayampakula, Rahul Kashyap and Ewen, Parker and Rice, Sean and Skinner, Katherine A and Vasudevan, Ram},
  year = {2024},
  number = {24},
  abstract = {Neural Radiance Fields and Gaussian Splatting have recently transformed computer vision by enabling photo-realistic representations of complex scenes. However, they have seen limited application in real-world robotics tasks such as trajectory optimization. This is due to the difficulty in reasoning about collisions in radiance models and the computational complexity associated with operating in dense models. This paper addresses these challenges by proposing SPLANNING, a risk-aware trajectory optimizer operating in a Gaussian Splatting model. This paper first derives a method to rigorously upper-bound the probability of collision between a robot and a radiance field. Then, this paper introduces a normalized reformulation of Gaussian Splatting that enables efficient computation of this collision bound. Finally, this paper presents a method to optimize trajectories that avoid collisions in a Gaussian Splat. Experiments show that SPLANNING outperforms state-of-the-art methods in generating collision-free trajectories in cluttered environments. The proposed system is also tested on a real-world robot manipulator. A project page is available at https://roahmlab.github.io/splanning.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/DMXSFZYN/Michaux et al. - 2024 - Let's Make a Splan Risk-Aware Trajectory Optimization in a Normalized Gaussian Splat.pdf}
}

@misc{mindererSimpleOpenVocabularyObject2022,
  title = {Simple {{Open-Vocabulary Object Detection}} with {{Vision Transformers}}},
  author = {Minderer, Matthias and Gritsenko, Alexey and Stone, Austin and Neumann, Maxim and Weissenborn, Dirk and Dosovitskiy, Alexey and Mahendran, Aravindh and Arnab, Anurag and Dehghani, Mostafa and Shen, Zhuoran and Wang, Xiao and Zhai, Xiaohua and Kipf, Thomas and Houlsby, Neil},
  year = {2022},
  month = jul,
  number = {arXiv:2205.06230},
  eprint = {2205.06230},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.06230},
  urldate = {2024-01-26},
  abstract = {Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub.},
  archiveprefix = {arXiv},
  keywords = {Open Vocabulary},
  file = {/Users/fangyuan/Zotero/storage/3D9RJZPR/Minderer et al. - 2022 - Simple Open-Vocabulary Object Detection with Visio.pdf;/Users/fangyuan/Zotero/storage/8M9QRADG/2205.html}
}

@misc{minRecentAdvancesNatural2021,
  title = {Recent {{Advances}} in {{Natural Language Processing}} via {{Large Pre-Trained Language Models}}: {{A Survey}}},
  shorttitle = {Recent {{Advances}} in {{Natural Language Processing}} via {{Large Pre-Trained Language Models}}},
  author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heinz, Ilana and Roth, Dan},
  year = {2021},
  month = nov,
  number = {arXiv:2111.01243},
  eprint = {2111.01243},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-16},
  abstract = {Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/WI48J87I/Min et al. - 2021 - Recent Advances in Natural Language Processing via.pdf;/Users/fangyuan/Zotero/storage/SGXRMX8K/2111.html}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2022-12-20},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computer science}
}

@article{mondalRiskAwareEnergyConstrainedUAVUGV,
  title = {Risk-{{Aware Energy-Constrained UAV-UGV Cooperative Routing}} Using {{Attention-Guided Reinforcement Learning}}},
  author = {Mondal, Mohammad Safwan and Ramasamy, Subramanian and Rownak, Ragib and Bhounsule, Pranav},
  abstract = {Maximizing the endurance of unmanned aerial vehicles (UAVs) in large-scale monitoring missions spanning over large areas requires addressing their limited battery capacity. Deploying unmanned ground vehicles (UGVs) as mobile recharging stations offers a practical solution, extending UAVs' operational range. This introduces the challenge of optimizing UAV-UGV routes for efficient mission point coverage and seamless recharging coordination. In this paper, we present a risk-aware deep reinforcement learning (Ra-DRL) framework with a multi-head attention mechanism within an encoderdecoder transformer architecture to solve this cooperative routing problem. Our model minimizes mission time while accounting for the stochastic fuel consumption of UAV, influenced by environmental factors like wind velocity, ensuring adherence to a risk threshold to avoid mid-mission energy depletion. Extensive evaluations on various problem sizes show that our method significantly outperforms nearest-neighbor heuristics in both solution quality and risk management. We validate the RaDRL policy in a Gazebo-ROS SITL environment with a PX4based custom UAV and Clearpath Husky UGV. The results demonstrate the robustness and adaptability of our policy, making it highly effective for mission planning in dynamic, uncertain scenarios.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/TVMQMMQ8/Mondal et al. - Risk-Aware Energy-Constrained UAV-UGV Cooperative Routing using Attention-Guided Reinforcement Learn.pdf}
}

@misc{myersGoalRepresentationsInstruction2023,
  title = {Goal {{Representations}} for {{Instruction Following}}: {{A Semi-Supervised Language Interface}} to {{Control}}},
  shorttitle = {Goal {{Representations}} for {{Instruction Following}}},
  author = {Myers, Vivek and He, Andre and Fang, Kuan and Walke, Homer and {Hansen-Estruch}, Philippe and Cheng, Ching-An and Jalobeanu, Mihai and Kolobov, Andrey and Dragan, Anca and Levine, Sergey},
  year = {2023},
  month = aug,
  number = {arXiv:2307.00117},
  eprint = {2307.00117},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-19},
  abstract = {Our goal is for robots to follow natural language instructions like "put the towel next to the microwave." But getting large amounts of labeled data, i.e. data that contains demonstrations of tasks labeled with the language instruction, is prohibitive. In contrast, obtaining policies that respond to image goals is much easier, because any autonomous trial or demonstration can be labeled in hindsight with its final state as the goal. In this work, we contribute a method that taps into joint image- and goal- conditioned policies with language using only a small amount of language data. Prior work has made progress on this using vision-language models or by jointly training language-goal-conditioned policies, but so far neither method has scaled effectively to real-world robot tasks without significant human annotation. Our method achieves robust performance in the real world by learning an embedding from the labeled data that aligns language not to the goal image, but rather to the desired change between the start and goal images that the instruction corresponds to. We then train a policy on this embedding: the policy benefits from all the unlabeled data, but the aligned embedding provides an interface for language to steer the policy. We show instruction following across a variety of manipulation tasks in different scenes, with generalization to language instructions outside of the labeled data. Videos and code for our approach can be found on our website: https://rail-berkeley.github.io/grif/ .},
  archiveprefix = {arXiv},
  keywords = {Goal-Conditioned RL,Manipulation,Subgoal},
  file = {/Users/fangyuan/Zotero/storage/VCFNYX2J/Myers et al. - 2023 - Goal Representations for Instruction Following A .pdf;/Users/fangyuan/Zotero/storage/YN44XPM9/2307.html}
}

@misc{myersLearningAssistHumans2024,
  title = {Learning to {{Assist Humans}} without {{Inferring Rewards}}},
  author = {Myers, Vivek and Ellis, Evan and Levine, Sergey and Eysenbach, Benjamin and Dragan, Anca},
  year = {2024},
  month = nov,
  number = {arXiv:2411.02623},
  eprint = {2411.02623},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.02623},
  urldate = {2024-11-10},
  abstract = {Assistive agents should make humans' lives easier. Classically, such assistance is studied through the lens of inverse reinforcement learning, where an assistive agent (e.g., a chatbot, a robot) infers a human's intention and then selects actions to help the human reach that goal. This approach requires inferring intentions, which can be difficult in high-dimensional settings. We build upon prior work that studies assistance through the lens of empowerment: an assistive agent aims to maximize the influence of the human's actions such that they exert a greater control over the environmental outcomes and can solve tasks in fewer steps. We lift the major limitation of prior work in this area--scalability to high-dimensional settings--with contrastive successor representations. We formally prove that these representations estimate a similar notion of empowerment to that studied by prior work and provide a ready-made mechanism for optimizing it. Empirically, our proposed method outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a cooperative game setting. Theoretically, our work connects ideas from information theory, neuroscience, and reinforcement learning, and charts a path for representations to play a critical role in solving assistive problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/fangyuan/Zotero/storage/9YJWAMIB/Myers et al. - 2024 - Learning to Assist Humans without Inferring Rewards.pdf}
}

@misc{myersPolicyAdaptationLanguage2024,
  title = {Policy {{Adaptation}} via {{Language Optimization}}: {{Decomposing Tasks}} for {{Few-Shot Imitation}}},
  shorttitle = {Policy {{Adaptation}} via {{Language Optimization}}},
  author = {Myers, Vivek and Zheng, Bill Chunyuan and Mees, Oier and Levine, Sergey and Fang, Kuan},
  year = {2024},
  month = aug,
  number = {arXiv:2408.16228},
  eprint = {2408.16228},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-31},
  abstract = {Learned language-conditioned robot policies often struggle to effectively adapt to new real-world tasks even when pre-trained across a diverse set of instructions. We propose a novel approach for few-shot adaptation to unseen tasks that exploits the semantic understanding of task decomposition provided by vision-language models (VLMs). Our method, Policy Adaptation via Language Optimization (PALO), combines a handful of demonstrations of a task with proposed language decompositions sampled from a VLM to quickly enable rapid nonparametric adaptation, avoiding the need for a larger fine-tuning dataset. We evaluate PALO on extensive real-world experiments consisting of challenging unseen, long-horizon robot manipulation tasks. We find that PALO is able of consistently complete long-horizon, multi-tier tasks in the real world, outperforming state of the art pre-trained generalist policies, and methods that have access to the same demonstrations.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/HT4ZS4LM/Myers et al. - 2024 - Policy Adaptation via Language Optimization Decomposing Tasks for Few-Shot Imitation.pdf}
}

@misc{myersTemporalRepresentationAlignment2025,
  title = {Temporal {{Representation Alignment}}: {{Successor Features Enable Emergent Compositionality}} in {{Robot Instruction Following}}},
  shorttitle = {Temporal {{Representation Alignment}}},
  author = {Myers, Vivek and Zheng, Bill Chunyuan and Dragan, Anca and Fang, Kuan and Levine, Sergey},
  year = {2025},
  month = feb,
  number = {arXiv:2502.05454},
  eprint = {2502.05454},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.05454},
  urldate = {2025-02-15},
  abstract = {Effective task representations should facilitate compositionality, such that after learning a variety of basic tasks, an agent can perform compound tasks consisting of multiple steps simply by composing the representations of the constituent steps together. While this is conceptually simple and appealing, it is not clear how to automatically learn representations that enable this sort of compositionality. We show that learning to associate the representations of current and future states with a temporal alignment loss can improve compositional generalization, even in the absence of any explicit subtask planning or reinforcement learning. We evaluate our approach across diverse robotic manipulation tasks as well as in simulation, showing substantial improvements for tasks specified with either language or goal images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/HTCPMUQ4/Myers et al. - 2025 - Temporal Representation Alignment Successor Features Enable Emergent Compositionality in Robot Inst.pdf;/Users/fangyuan/Zotero/storage/7KYQ4HCM/2502.html}
}

@misc{nachumNearOptimalRepresentationLearning2019,
  title = {Near-{{Optimal Representation Learning}} for {{Hierarchical Reinforcement Learning}}},
  author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  year = {2019},
  month = jan,
  number = {arXiv:1810.01257},
  eprint = {1810.01257},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-27},
  abstract = {We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods (see videos at https://sites.google.com/view/representation-hrl).},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/GXNAUKNB/Nachum et al. - 2019 - Near-Optimal Representation Learning for Hierarchi.pdf}
}

@article{nairContextualImaginedGoals,
  title = {Contextual {{Imagined Goals}} for {{Self-Supervised Robotic Learning}}},
  author = {Nair, Ashvin and Bahl, Shikhar and Khazatsky, Alexander and Pong, Vitchyr and Berseth, Glen and Levine, Sergey},
  pages = {10},
  abstract = {While reinforcement learning provides an appealing formalism for learning individual skills, a general-purpose robotic system must be able to master an extensive repertoire of behaviors. Instead of learning a large collection of skills individually, can we instead enable a robot to propose and practice its own behaviors automatically, learning about the affordances and behaviors that it can perform in its environment, such that it can then repurpose this knowledge once a new task is commanded by the user? In this paper, we study this question in the context of self-supervised goal-conditioned reinforcement learning. A central challenge in this learning regime is the problem of goal setting: in order to practice useful skills, the robot must be able to autonomously set goals that are feasible but diverse. When the robot's environment and available objects vary, as they do in most open-world settings, the robot must propose to itself only those goals that it can accomplish in its present setting with the objects that are at hand. Previous work only studies self-supervised goal-conditioned RL in a single-environment setting, where goal proposals come from the robot's past experience or a generative model are sufficient. In more diverse settings, this frequently leads to impossible goals and, as we show experimentally, prevents effective learning. We propose a conditional goal-setting model that aims to propose goals that are feasible from the robot's current state. We demonstrate that this enables self-supervised goalconditioned off-policy learning with raw image observations in the real world, enabling a robot to manipulate a variety of objects and generalize to new objects that were not seen during training.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/9JVC6G8A/Nair et al. - Contextual Imagined Goals for Self-Supervised Robo.pdf}
}

@misc{nairHierarchicalForesightSelfSupervised2019,
  title = {Hierarchical {{Foresight}}: {{Self-Supervised Learning}} of {{Long-Horizon Tasks}} via {{Visual Subgoal Generation}}},
  shorttitle = {Hierarchical {{Foresight}}},
  author = {Nair, Suraj and Finn, Chelsea},
  year = {2019},
  month = sep,
  number = {arXiv:1909.05829},
  eprint = {1909.05829},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.05829},
  urldate = {2022-12-16},
  abstract = {Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision, reaching novel goals in cluttered scenes with unseen objects. However, due to the compounding uncertainty in long horizon video prediction and poor scalability of sampling-based planning optimizers, one significant limitation of these approaches is the ability to plan over long horizons to reach distant goals. To that end, we propose a framework for subgoal generation and planning, hierarchical visual foresight (HVF), which generates subgoal images conditioned on a goal image, and uses them for planning. The subgoal images are directly optimized to decompose the task into easy to plan segments, and as a result, we observe that the method naturally identifies semantically meaningful states as subgoals. Across three out of four simulated vision-based manipulation tasks, we find that our method achieves nearly a 200\% performance improvement over planning without subgoals and model-free RL approaches. Further, our experiments illustrate that our approach extends to real, cluttered visual scenes. Project page: https://sites.google.com/stanford.edu/hvf},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/EGUD44KF/Nair and Finn - 2019 - Hierarchical Foresight Self-Supervised Learning o.pdf;/Users/fangyuan/Zotero/storage/PN3MD49P/1909.html}
}

@misc{nairLearningJobSelfRewarding2022,
  title = {Learning on the {{Job}}: {{Self-Rewarding Offline-to-Online Finetuning}} for {{Industrial Insertion}} of {{Novel Connectors}} from {{Vision}}},
  shorttitle = {Learning on the {{Job}}},
  author = {Nair, Ashvin and Zhu, Brian and Narayanan, Gokul and Solowjow, Eugen and Levine, Sergey},
  year = {2022},
  month = oct,
  number = {arXiv:2210.15206},
  eprint = {2210.15206},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-10-28},
  abstract = {Learning-based methods in robotics hold the promise of generalization, but what can be done if a learned policy does not generalize to a new situation? In principle, if an agent can at least evaluate its own success (i.e., with a reward classifier that generalizes well even when the policy does not), it could actively practice the task and finetune the policy in this situation. We study this problem in the setting of industrial insertion tasks, such as inserting connectors in sockets and setting screws. Existing algorithms rely on precise localization of the connector or socket and carefully managed physical setups, such as assembly lines, to succeed at the task. But in unstructured environments such as homes or even some industrial settings, robots cannot rely on precise localization and may be tasked with previously unseen connectors. Offline reinforcement learning on a variety of connector insertion tasks is a potential solution, but what if the robot is tasked with inserting previously unseen connector? In such a scenario, we will still need methods that can robustly solve such tasks with online practice. One of the main observations we make in this work is that, with a suitable representation learning and domain generalization approach, it can be significantly easier for the reward function to generalize to a new but structurally similar task (e.g., inserting a new type of connector) than for the policy. This means that a learned reward function can be used to facilitate the finetuning of the robot's policy in situations where the policy fails to generalize in zero shot, but the reward function generalizes successfully. We show that such an approach can be instantiated in the real world, pretrained on 50 different connectors, and successfully finetuned to new connectors via the learned reward function. Videos can be viewed at https://sites.google.com/view/learningonthejob},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/PK77C7QH/Nair et al. - 2022 - Learning on the Job Self-Rewarding Offline-to-Onl.pdf}
}

@inproceedings{nairOvercomingExplorationReinforcement2018,
  title = {Overcoming {{Exploration}} in {{Reinforcement Learning}} with {{Demonstrations}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  year = {2018},
  month = may,
  pages = {6292--6299},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8463162},
  abstract = {Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.},
  keywords = {Games,Learning (artificial intelligence),Mathematical model,Robots,Stacking,Task analysis,Training},
  file = {/Users/fangyuan/Zotero/storage/79M5HXNS/Nair et al. - 2018 - Overcoming Exploration in Reinforcement Learning w.pdf;/Users/fangyuan/Zotero/storage/XBY2JNRZ/stamp.html}
}

@article{nairVisualReinforcementLearning,
  title = {Visual {{Reinforcement Learning}} with {{Imagined Goals}}},
  author = {Nair, Ashvin V and Pong, Vitchyr and Dalal, Murtaza and Bahl, Shikhar and Lin, Steven and Levine, Sergey},
  pages = {10},
  abstract = {For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised ``practice'' phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/AZ8RTQDR/Nair et al. - Visual Reinforcement Learning with Imagined Goals.pdf}
}

@misc{nakamotoSteeringYourGeneralists2024,
  title = {Steering {{Your Generalists}}: {{Improving Robotic Foundation Models}} via {{Value Guidance}}},
  shorttitle = {Steering {{Your Generalists}}},
  author = {Nakamoto, Mitsuhiko and Mees, Oier and Kumar, Aviral and Levine, Sergey},
  year = {2024},
  month = oct,
  number = {arXiv:2410.13816},
  eprint = {2410.13816},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13816},
  urldate = {2024-10-18},
  abstract = {Large, general-purpose robotic policies trained on diverse demonstration datasets have been shown to be remarkably effective both for controlling a variety of robots in a range of different scenes, and for acquiring broad repertoires of manipulation skills. However, the data that such policies are trained on is generally of mixed quality -- not only are human-collected demonstrations unlikely to perform the task perfectly, but the larger the dataset is, the harder it is to curate only the highest quality examples. It also remains unclear how optimal data from one embodiment is for training on another embodiment. In this paper, we present a general and broadly applicable approach that enhances the performance of such generalist robot policies at deployment time by re-ranking their actions according to a value function learned via offline RL. This approach, which we call Value-Guided Policy Steering (V-GPS), is compatible with a wide range of different generalist policies, without needing to fine-tune or even access the weights of the policy. We show that the same value function can improve the performance of five different state-of-the-art policies with different architectures, even though they were trained on distinct datasets, attaining consistent performance improvement on multiple robotic platforms across a total of 12 tasks. Code and videos can be found at: https://nakamotoo.github.io/V-GPS},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/T3C9N4B9/Nakamoto et al. - 2024 - Steering Your Generalists Improving Robotic Foundation Models via Value Guidance.pdf;/Users/fangyuan/Zotero/storage/UHX3HT68/2410.html}
}

@misc{nakamotoSteeringYourGeneralists2025,
  title = {Steering {{Your Generalists}}: {{Improving Robotic Foundation Models}} via {{Value Guidance}}},
  shorttitle = {Steering {{Your Generalists}}},
  author = {Nakamoto, Mitsuhiko and Mees, Oier and Kumar, Aviral and Levine, Sergey},
  year = {2025},
  month = feb,
  number = {arXiv:2410.13816},
  eprint = {2410.13816},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.13816},
  urldate = {2025-02-26},
  abstract = {Large, general-purpose robotic policies trained on diverse demonstration datasets have been shown to be remarkably effective both for controlling a variety of robots in a range of different scenes, and for acquiring broad repertoires of manipulation skills. However, the data that such policies are trained on is generally of mixed quality -- not only are human-collected demonstrations unlikely to perform the task perfectly, but the larger the dataset is, the harder it is to curate only the highest quality examples. It also remains unclear how optimal data from one embodiment is for training on another embodiment. In this paper, we present a general and broadly applicable approach that enhances the performance of such generalist robot policies at deployment time by re-ranking their actions according to a value function learned via offline RL. This approach, which we call Value-Guided Policy Steering (V-GPS), is compatible with a wide range of different generalist policies, without needing to fine-tune or even access the weights of the policy. We show that the same value function can improve the performance of five different state-of-the-art policies with different architectures, even though they were trained on distinct datasets, attaining consistent performance improvement on multiple robotic platforms across a total of 12 tasks. Code and videos can be found at: https://nakamotoo.github.io/V-GPS},
  archiveprefix = {arXiv},
  keywords = {Manipulation,Robotic Foundation Model,VLA},
  file = {/Users/fangyuan/Zotero/storage/PATFPYUA/Nakamoto et al. - 2025 - Steering Your Generalists Improving Robotic Foundation Models via Value Guidance.pdf;/Users/fangyuan/Zotero/storage/BY7NMDFN/2410.html}
}

@misc{nasirianyPlanningGoalConditionedPolicies2019,
  title = {Planning with {{Goal-Conditioned Policies}}},
  author = {Nasiriany, Soroush and Pong, Vitchyr H. and Lin, Steven and Levine, Sergey},
  year = {2019},
  month = nov,
  number = {arXiv:1911.08453},
  eprint = {1911.08453},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-12-05},
  abstract = {Planning methods can solve temporally extended sequential decision making problems by composing simple behaviors. However, planning requires suitable abstractions for the states and transitions, which typically need to be designed by hand. In contrast, model-free reinforcement learning (RL) can acquire behaviors from low-level inputs directly, but often struggles with temporally extended tasks. Can we utilize reinforcement learning to automatically form the abstractions needed for planning, thus obtaining the best of both approaches? We show that goalconditioned policies learned with RL can be incorporated into planning, so that a planner can focus on which states to reach, rather than how those states are reached. However, with complex state observations such as images, not all inputs represent valid states. We therefore also propose using a latent variable model to compactly represent the set of valid states for the planner, so that the policies provide an abstraction of actions, and the latent variable model provides an abstraction of states. We compare our method with planning-based and model-free methods and find that our method significantly outperforms prior work when evaluated on image-based robot navigation and manipulation tasks that require non-greedy, multi-staged behavior.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/RTGMZ3NJ/Nasiriany et al. - 2019 - Planning with Goal-Conditioned Policies.pdf}
}

@misc{nasirianyRoboCasaLargeScaleSimulation2024,
  title = {{{RoboCasa}}: {{Large-Scale Simulation}} of {{Everyday Tasks}} for {{Generalist Robots}}},
  shorttitle = {{{RoboCasa}}},
  author = {Nasiriany, Soroush and Maddukuri, Abhiram and Zhang, Lance and Parikh, Adeet and Lo, Aaron and Joshi, Abhishek and Mandlekar, Ajay and Zhu, Yuke},
  year = {2024},
  month = jun,
  number = {arXiv:2406.02523},
  eprint = {2406.02523},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.02523},
  urldate = {2024-12-06},
  abstract = {Recent advancements in Artificial Intelligence (AI) have largely been propelled by scaling. In Robotics, scaling is hindered by the lack of access to massive robot datasets. We advocate using realistic physical simulation as a means to scale environments, tasks, and datasets for robot learning methods. We present RoboCasa, a large-scale simulation framework for training generalist robots in everyday environments. RoboCasa features realistic and diverse scenes focusing on kitchen environments. We provide thousands of 3D assets across over 150 object categories and dozens of interactable furniture and appliances. We enrich the realism and diversity of our simulation with generative AI tools, such as object assets from text-to-3D models and environment textures from text-to-image models. We design a set of 100 tasks for systematic evaluation, including composite tasks generated by the guidance of large language models. To facilitate learning, we provide high-quality human demonstrations and integrate automated trajectory generation methods to substantially enlarge our datasets with minimal human burden. Our experiments show a clear scaling trend in using synthetically generated robot data for large-scale imitation learning and show great promise in harnessing simulation data in real-world tasks. Videos and open-source code are available at https://robocasa.ai/},
  archiveprefix = {arXiv},
  keywords = {Benchmark},
  file = {/Users/fangyuan/Zotero/storage/LTLGYAWA/Nasiriany et al. - 2024 - RoboCasa Large-Scale Simulation of Everyday Tasks for Generalist Robots.pdf;/Users/fangyuan/Zotero/storage/3U8QP5WV/2406.html}
}

@misc{nvidiaGR00TN1Open2025,
  title = {{{GR00T N1}}: {{An Open Foundation Model}} for {{Generalist Humanoid Robots}}},
  shorttitle = {{{GR00T N1}}},
  author = {NVIDIA and Bjorck, Johan and Casta{\~n}eda, Fernando and Cherniadev, Nikita and Da, Xingye and Ding, Runyu and Fan, Linxi "Jim" and Fang, Yu and Fox, Dieter and Hu, Fengyuan and Huang, Spencer and Jang, Joel and Jiang, Zhenyu and Kautz, Jan and Kundalia, Kaushil and Lao, Lawrence and Li, Zhiqi and Lin, Zongyu and Lin, Kevin and Liu, Guilin and Llontop, Edith and Magne, Loic and Mandlekar, Ajay and Narayan, Avnish and Nasiriany, Soroush and Reed, Scott and Tan, You Liang and Wang, Guanzhi and Wang, Zu and Wang, Jing and Wang, Qi and Xiang, Jiannan and Xie, Yuqi and Xu, Yinzhen and Xu, Zhenjia and Ye, Seonghyeon and Yu, Zhiding and Zhang, Ao and Zhang, Hao and Zhao, Yizhou and Zheng, Ruijie and Zhu, Yuke},
  year = {2025},
  month = mar,
  number = {arXiv:2503.14734},
  eprint = {2503.14734},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.14734},
  urldate = {2025-03-29},
  abstract = {General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation model, trained on massive and diverse data sources, is essential for enabling the robots to reason about novel situations, robustly handle real-world variability, and rapidly learn new tasks. To this end, we introduce GR00T N1, an open foundation model for humanoid robots. GR00T N1 is a Vision-Language-Action (VLA) model with a dual-system architecture. The vision-language module (System 2) interprets the environment through vision and language instructions. The subsequent diffusion transformer module (System 1) generates fluid motor actions in real time. Both modules are tightly coupled and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture of real-robot trajectories, human videos, and synthetically generated datasets. We show that our generalist robot model GR00T N1 outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments. Furthermore, we deploy our model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks, achieving strong performance with high data efficiency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/2WPJQAZC/NVIDIA et al. - 2025 - GR00T N1 An Open Foundation Model for Generalist Humanoid Robots.pdf;/Users/fangyuan/Zotero/storage/UR36V52G/2503.html}
}

@article{ohZeroShotTaskGeneralization,
  title = {Zero-{{Shot Task Generalization}} with {{Multi-Task Deep Reinforcement Learning}}},
  author = {Oh, Junhyuk and Singh, Satinder and Lee, Honglak and Kohli, Pushmeet},
  pages = {10},
  abstract = {As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/IM4Z3V63/Oh et al. - Zero-Shot Task Generalization with Multi-Task Deep.pdf}
}

@misc{panHumanInteractiveSubgoalSupervision2018,
  title = {Human-{{Interactive Subgoal Supervision}} for {{Efficient Inverse Reinforcement Learning}}},
  author = {Pan, Xinlei and {Ohn-Bar}, Eshed and Rhinehart, Nicholas and Xu, Yan and Shen, Yilin and Kitani, Kris M.},
  year = {2018},
  month = jun,
  number = {arXiv:1806.08479},
  eprint = {1806.08479},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-08},
  abstract = {Humans are able to understand and perform complex tasks by strategically structuring the tasks into incremental steps or subgoals. For a robot attempting to learn to perform a sequential task with critical subgoal states, such states can provide a natural opportunity for interaction with a human expert. This paper analyzes the benefit of incorporating a notion of subgoals into Inverse Reinforcement Learning (IRL) with a Human-In-The-Loop (HITL) framework. The learning process is interactive, with a human expert first providing input in the form of full demonstrations along with some subgoal states. These subgoal states define a set of subtasks for the learning agent to complete in order to achieve the final goal. The learning agent queries for partial demonstrations corresponding to each subtask as needed when the agent struggles with the subtask. The proposed Human Interactive IRL (HI-IRL) framework is evaluated on several discrete path-planning tasks. We demonstrate that subgoal-based interactive structuring of the learning task results in significantly more efficient learning, requiring only a fraction of the demonstration data needed for learning the underlying reward function with the baseline IRL model.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/PHEJ8SQT/Pan et al. - 2018 - Human-Interactive Subgoal Supervision for Efficien.pdf}
}

@misc{parkDexHubDARTInternet2024,
  title = {{{DexHub}} and {{DART}}: {{Towards Internet Scale Robot Data Collection}}},
  shorttitle = {{{DexHub}} and {{DART}}},
  author = {Park, Younghyo and Bhatia, Jagdeep Singh and Ankile, Lars and Agrawal, Pulkit},
  year = {2024},
  month = nov,
  number = {arXiv:2411.02214},
  eprint = {2411.02214},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.02214},
  urldate = {2024-12-06},
  abstract = {The quest to build a generalist robotic system is impeded by the scarcity of diverse and high-quality data. While real-world data collection effort exist, requirements for robot hardware, physical environment setups, and frequent resets significantly impede the scalability needed for modern learning frameworks. We introduce DART, a teleoperation platform designed for crowdsourcing that reimagines robotic data collection by leveraging cloud-based simulation and augmented reality (AR) to address many limitations of prior data collection efforts. Our user studies highlight that DART enables higher data collection throughput and lower physical fatigue compared to real-world teleoperation. We also demonstrate that policies trained using DART-collected datasets successfully transfer to reality and are robust to unseen visual disturbances. All data collected through DART is automatically stored in our cloud-hosted database, DexHub, which will be made publicly available upon curation, paving the path for DexHub to become an ever-growing data hub for robot learning. Videos are available at: https://dexhub.ai/project},
  archiveprefix = {arXiv},
  keywords = {Manipulation,Teleoperation,Vision Pro},
  file = {/Users/fangyuan/Zotero/storage/SKM6N7UE/Park et al. - 2024 - DexHub and DART Towards Internet Scale Robot Data Collection.pdf;/Users/fangyuan/Zotero/storage/NPY6UID8/2411.html}
}

@misc{parkFlowQLearning2025,
  title = {Flow {{Q-Learning}}},
  author = {Park, Seohong and Li, Qiyang and Levine, Sergey},
  year = {2025},
  month = feb,
  number = {arXiv:2502.02538},
  eprint = {2502.02538},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.02538},
  urldate = {2025-02-07},
  abstract = {We present flow Q-learning (FQL), a simple and performant offline reinforcement learning (RL) method that leverages an expressive flow-matching policy to model arbitrarily complex action distributions in data. Training a flow policy with RL is a tricky problem, due to the iterative nature of the action generation process. We address this challenge by training an expressive one-step policy with RL, rather than directly guiding an iterative flow policy to maximize values. This way, we can completely avoid unstable recursive backpropagation, eliminate costly iterative action generation at test time, yet still mostly maintain expressivity. We experimentally show that FQL leads to strong performance across 73 challenging state- and pixel-based OGBench and D4RL tasks in offline RL and offline-to-online RL. Project page: https://seohong.me/projects/fql/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/UZS6BH2X/Park et al. - 2025 - Flow Q-Learning.pdf;/Users/fangyuan/Zotero/storage/Y269AT2S/2502.html}
}

@misc{parkHIQLOfflineGoalConditioned2023,
  title = {{{HIQL}}: {{Offline Goal-Conditioned RL}} with {{Latent States}} as {{Actions}}},
  shorttitle = {{{HIQL}}},
  author = {Park, Seohong and Ghosh, Dibya and Eysenbach, Benjamin and Levine, Sergey},
  year = {2023},
  month = jul,
  number = {arXiv:2307.11949},
  eprint = {2307.11949},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-09-13},
  abstract = {Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at https://seohong.me/projects/hiql/},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/BEICLK7J/Park et al. - 2023 - HIQL Offline Goal-Conditioned RL with Latent Stat.pdf;/Users/fangyuan/Zotero/storage/7SAPEWPJ/2307.html}
}

@misc{parmarChallengesEnsuringAI2025,
  title = {Challenges in {{Ensuring AI Safety}} in {{DeepSeek-R1 Models}}: {{The Shortcomings}} of {{Reinforcement Learning Strategies}}},
  shorttitle = {Challenges in {{Ensuring AI Safety}} in {{DeepSeek-R1 Models}}},
  author = {Parmar, Manojkumar and Govindarajulu, Yuvaraj},
  year = {2025},
  month = jan,
  number = {arXiv:2501.17030},
  eprint = {2501.17030},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.17030},
  urldate = {2025-02-01},
  abstract = {Large Language Models (LLMs) have achieved remarkable progress in reasoning, alignment, and task-specific performance. However, ensuring harmlessness in these systems remains a critical challenge, particularly in advanced models like DeepSeek-R1. This paper examines the limitations of Reinforcement Learning (RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and compares it with Supervised Fine-Tuning (SFT). While RL improves reasoning capabilities, it faces challenges such as reward hacking, generalization failures, language mixing, and high computational costs. We propose hybrid training approaches combining RL and SFT to achieve robust harmlessness reduction. Usage recommendations and future directions for deploying DeepSeek-R1 responsibly are also presented.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/NTWRZBFU/Parmar and Govindarajulu - 2025 - Challenges in Ensuring AI Safety in DeepSeek-R1 Models The Shortcomings of Reinforcement Learning S.pdf;/Users/fangyuan/Zotero/storage/3UW5KWPW/2501.html}
}

@misc{patelGETZeroGraphEmbodiment2024,
  title = {{{GET-Zero}}: {{Graph Embodiment Transformer}} for {{Zero-shot Embodiment Generalization}}},
  shorttitle = {{{GET-Zero}}},
  author = {Patel, Austin and Song, Shuran},
  year = {2024},
  month = sep,
  number = {arXiv:2407.15002},
  eprint = {2407.15002},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.15002},
  urldate = {2024-09-12},
  abstract = {This paper introduces GET-Zero, a model architecture and training procedure for learning an embodiment-aware control policy that can immediately adapt to new hardware changes without retraining. To do so, we present Graph Embodiment Transformer (GET), a transformer model that leverages the embodiment graph connectivity as a learned structural bias in the attention mechanism. We use behavior cloning to distill demonstration data from embodiment-specific expert policies into an embodiment-aware GET model that conditions on the hardware configuration of the robot to make control decisions. We conduct a case study on a dexterous in-hand object rotation task using different configurations of a four-fingered robot hand with joints removed and with link length extensions. Using the GET model along with a self-modeling loss enables GET-Zero to zero-shot generalize to unseen variation in graph structure and link length, yielding a 20\% improvement over baseline methods. All code and qualitative video results are on https://get-zero-paper.github.io},
  archiveprefix = {arXiv},
  keywords = {Dexterous Hand,GNN},
  file = {/Users/fangyuan/Zotero/storage/TWH7YVMR/Patel and Song - 2024 - GET-Zero Graph Embodiment Transformer for Zero-shot Embodiment Generalization.pdf;/Users/fangyuan/Zotero/storage/7FVJHEUC/2407.html}
}

@article{paulLearningTrajectoriesSubgoal,
  title = {Learning from {{Trajectories}} via {{Subgoal Discovery}}},
  author = {Paul, Sujoy and Vanbaar, Jeroen and {Roy-Chowdhury}, Amit},
  pages = {11},
  abstract = {Learning to solve complex goal-oriented tasks with sparse terminal-only rewards often requires an enormous number of samples. In such cases, using a set of expert trajectories could help to learn faster. However, Imitation Learning (IL) via supervised pre-training with these trajectories may not perform as well and generally requires additional finetuning with expert-in-the-loop. In this paper, we propose an approach which uses the expert trajectories and learns to decompose the complex main task into smaller sub-goals. We learn a function which partitions the state-space into sub-goals, which can then be used to design an extrinsic reward function. We follow a strategy where the agent first learns from the trajectories using IL and then switches to Reinforcement Learning (RL) using the identified sub-goals, to alleviate the errors in the IL step. To deal with states which are underrepresented by the trajectory set, we also learn a function to modulate the sub-goal predictions. We show that our method is able to solve complex goal-oriented tasks, which other RL, IL or their combinations in literature are not able to solve.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/UA5WHZRK/Paul et al. - Learning from Trajectories via Subgoal Discovery.pdf}
}

@misc{peeblesScalableDiffusionModels2023,
  title = {Scalable {{Diffusion Models}} with {{Transformers}}},
  author = {Peebles, William and Xie, Saining},
  year = {2023},
  month = mar,
  number = {arXiv:2212.09748},
  eprint = {2212.09748},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09748},
  urldate = {2025-04-13},
  abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/9KYCWLNR/Peebles and Xie - 2023 - Scalable Diffusion Models with Transformers.pdf;/Users/fangyuan/Zotero/storage/8RJVB3GS/2212.html}
}

@misc{pertschFASTEfficientAction2025,
  title = {{{FAST}}: {{Efficient Action Tokenization}} for {{Vision-Language-Action Models}}},
  shorttitle = {{{FAST}}},
  author = {Pertsch, Karl and Stachowicz, Kyle and Ichter, Brian and Driess, Danny and Nair, Suraj and Vuong, Quan and Mees, Oier and Finn, Chelsea and Levine, Sergey},
  year = {2025},
  month = jan,
  number = {arXiv:2501.09747},
  eprint = {2501.09747},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.09747},
  urldate = {2025-01-17},
  abstract = {Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/J36SS47W/Pertsch et al. - 2025 - FAST Efficient Action Tokenization for Vision-Language-Action Models.pdf;/Users/fangyuan/Zotero/storage/XXL8XGSN/2501.html}
}

@article{pertschLongHorizonVisualPlanning,
  title = {Long-{{Horizon Visual Planning}} with {{Goal-Conditioned Hierarchical Predictors}}},
  author = {Pertsch, Karl and Rybkin, Oleh and Ebert, Frederik and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
  pages = {13},
  abstract = {The ability to predict and plan into the future is fundamental for agents acting in the world. To reach a faraway goal, we predict trajectories at multiple timescales, first devising a coarse plan towards the goal and then gradually filling in details. In contrast, current learning approaches for visual prediction and planning fail on longhorizon tasks as they generate predictions (1) without considering goal information, and (2) at the finest temporal resolution, one step at a time. In this work we propose a framework for visual prediction and planning that is able to overcome both of these limitations. First, we formulate the problem of predicting towards a goal and propose the corresponding class of latent space goal-conditioned predictors (GCPs). GCPs significantly improve planning efficiency by constraining the search space to only those trajectories that reach the goal. Further, we show how GCPs can be naturally formulated as hierarchical models that, given two observations, predict an observation between them, and by recursively subdividing each part of the trajectory generate complete sequences. This divide-and-conquer strategy is effective at long-term prediction, and enables us to design an effective hierarchical planning algorithm that optimizes trajectories in a coarse-to-fine manner. We show that by using both goal-conditioning and hierarchical prediction, GCPs enable us to solve visual planning tasks with much longer horizon than previously possible.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/98MQ6IJZ/Pertsch et al. - Long-Horizon Visual Planning with Goal-Conditioned.pdf}
}

@article{pitisMaximumEntropyGain,
  title = {Maximum {{Entropy Gain Exploration}} for {{Long Horizon Multi-goal Reinforcement Learning}}},
  author = {Pitis, Silviu and Chan, Harris and Zhao, Stephen and Stadie, Bradly},
  pages = {12},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/H3TH5V9I/Pitis et al. - Maximum Entropy Gain Exploration for Long Horizon .pdf}
}

@misc{pongSkewFitStateCoveringSelfSupervised2020,
  title = {Skew-{{Fit}}: {{State-Covering Self-Supervised Reinforcement Learning}}},
  shorttitle = {Skew-{{Fit}}},
  author = {Pong, Vitchyr H. and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  year = {2020},
  month = aug,
  number = {arXiv:1903.03698},
  eprint = {1903.03698},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-11-18},
  abstract = {Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills. Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden. Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions. In this paper, we propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing goal reaching performance together with the entropy of the goal distribution, where goals correspond to full state observations. To instantiate this principle, we present an algorithm called Skew-Fit for learning a maximum-entropy goal distributions and prove that, under regularity conditions, Skew-Fit converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand. Our experiments show that combining Skew-Fit for learning goal distributions with existing goal-reaching methods outperforms a variety of prior methods on open-sourced visual goal-reaching tasks and that Skew-Fit enables a real-world robot to learn to open a door, entirely from scratch, from pixels, and without any manually-designed reward function.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/T2IJMFDL/Pong et al. - 2020 - Skew-Fit State-Covering Self-Supervised Reinforce.pdf}
}

@book{porterHandbookKnowledgeRepresentation2008,
  title = {Handbook of Knowledge Representation},
  editor = {Porter, Bruce and Lifschitz, Vladimir and Van Harmelen, Frank},
  year = {2008},
  series = {Foundations of Artificial Intelligence},
  edition = {1st ed},
  publisher = {Elsevier},
  address = {Amsterdam ; Boston},
  isbn = {978-0-444-52211-5},
  langid = {english},
  lccn = {Q387 .H35 2008},
  keywords = {Knowledge representation (Information theory),Representation des connaissances},
  annotation = {OCLC: ocn171550939},
  file = {/Users/fangyuan/Zotero/storage/R962XHSS/Porter et al. - 2008 - Handbook of knowledge representation.pdf}
}

@misc{psenkaLearningDiffusionModel2025,
  title = {Learning a {{Diffusion Model Policy}} from {{Rewards}} via {{Q-Score Matching}}},
  author = {Psenka, Michael and Escontrela, Alejandro and Abbeel, Pieter and Ma, Yi},
  year = {2025},
  month = feb,
  number = {arXiv:2312.11752},
  eprint = {2312.11752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.11752},
  urldate = {2025-02-17},
  abstract = {Diffusion models have become a popular choice for representing actor policies in behavior cloning and offline reinforcement learning. This is due to their natural ability to optimize an expressive class of distributions over a continuous space. However, previous works fail to exploit the score-based structure of diffusion models, and instead utilize a simple behavior cloning term to train the actor, limiting their ability in the actor-critic setting. In this paper, we present a theoretical framework linking the structure of diffusion model policies to a learned Q-function, by linking the structure between the score of the policy to the action gradient of the Q-function. We focus on off-policy reinforcement learning and propose a new policy update method from this theory, which we denote Q-score matching. Notably, this algorithm only needs to differentiate through the denoising model rather than the entire diffusion model evaluation, and converged policies through Q-score matching are implicitly multi-modal and explorative in continuous domains. We conduct experiments in simulated environments to demonstrate the viability of our proposed method and compare to popular baselines. Source code is available from the project website: https://michaelpsenka.io/qsm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/YPEXMUED/Psenka et al. - 2025 - Learning a Diffusion Model Policy from Rewards via Q-Score Matching.pdf;/Users/fangyuan/Zotero/storage/5X66SRJ9/2312.html}
}

@misc{qiLearningGeneralizableTooluse2024,
  title = {Learning {{Generalizable Tool-use Skills}} through {{Trajectory Generation}}},
  author = {Qi, Carl and Wu, Yilin and Yu, Lifan and Liu, Haoyue and Jiang, Bowen and Lin, Xingyu and Held, David},
  year = {2024},
  month = apr,
  number = {arXiv:2310.00156},
  eprint = {2310.00156},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-27},
  abstract = {Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of tool point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model on four different challenging deformable object manipulation tasks, using demonstration data from only one tool per task. The model generalizes to various novel tools, significantly outperforming baselines. We further test our trained policy in the real world with unseen tools, where it achieves the performance comparable to human. Additional materials can be found on our project website: https://sites.google.com/view/toolgen.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {I.2.9},
  file = {/Users/fangyuan/Zotero/storage/M6LZFIRG/Qi et al. - 2024 - Learning Generalizable Tool-use Skills through Trajectory Generation.pdf}
}

@inproceedings{qinAnyTeleopGeneralVisionBased2023,
  title = {{{AnyTeleop}}: {{A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System}}},
  shorttitle = {{{AnyTeleop}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIX}}},
  author = {Qin, Yuzhe and Yang, Wei and Huang, Binghao and Wyk, Karl and Su, Hao and Wang, Xiaolong and Chao, Yu-Wei and Fox, Dieter},
  year = {2023},
  month = jul,
  publisher = {{Robotics: Science and Systems Foundation}},
  doi = {10.15607/RSS.2023.XIX.015},
  urldate = {2024-12-07},
  isbn = {978-0-9923747-9-2},
  langid = {english},
  keywords = {Dexterous Hand,Manipulation,Teleoperation},
  file = {/Users/fangyuan/Zotero/storage/Y9M9QS3E/Qin et al. - 2023 - AnyTeleop A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System.pdf}
}

@misc{qiuIdentifyingSelectionsUnsupervised2024,
  title = {Identifying {{Selections}} for {{Unsupervised Subtask Discovery}}},
  author = {Qiu, Yiwen and Zheng, Yujia and Zhang, Kun},
  year = {2024},
  month = oct,
  number = {arXiv:2410.21616},
  eprint = {2410.21616},
  publisher = {arXiv},
  urldate = {2024-11-10},
  abstract = {When solving long-horizon tasks, it is intriguing to decompose the high-level task into subtasks. Decomposing experiences into reusable subtasks can improve data efficiency, accelerate policy generalization, and in general provide promising solutions to multi-task reinforcement learning and imitation learning problems. However, the concept of subtasks is not sufficiently understood and modeled yet, and existing works often overlook the true structure of the data generation process: subtasks are the results of a \${\textbackslash}textit\{selection\}\$ mechanism on actions, rather than possible underlying confounders or intermediates. Specifically, we provide a theory to identify, and experiments to verify the existence of selection variables in such data. These selections serve as subgoals that indicate subtasks and guide policy. In light of this idea, we develop a sequential non-negative matrix factorization (seq- NMF) method to learn these subgoals and extract meaningful behavior patterns as subtasks. Our empirical results on a challenging Kitchen environment demonstrate that the learned subtasks effectively enhance the generalization to new tasks in multi-task imitation learning scenarios. The codes are provided at https://anonymous.4open.science/r/Identifying{\textbackslash}\_Selections{\textbackslash}\_for{\textbackslash}\_Unsupervised{\textbackslash}\_Subtask{\textbackslash}\_Discovery/README.md.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/MFT4ZPYE/Qiu et al. - 2024 - Identifying Selections for Unsupervised Subtask Discovery.pdf;/Users/fangyuan/Zotero/storage/AP393N5R/2410.html}
}

@misc{qiuLearningGeneralizableFeature2024,
  title = {Learning {{Generalizable Feature Fields}} for {{Mobile Manipulation}}},
  author = {Qiu, Ri-Zhao and Hu, Yafei and Song, Yuchen and Yang, Ge and Fu, Yang and Ye, Jianglong and Mu, Jiteng and Yang, Ruihan and Atanasov, Nikolay and Scherer, Sebastian and Wang, Xiaolong},
  year = {2024},
  month = nov,
  number = {arXiv:2403.07563},
  eprint = {2403.07563},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.07563},
  urldate = {2024-11-28},
  abstract = {An open problem in mobile manipulation is how to represent objects and scenes in a unified manner so that robots can use both for navigation and manipulation. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherent at an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We quantitatively evaluate GeFF's ability for open-vocabulary object-/part-level manipulation and show that GeFF outperforms point-based baselines in runtime and storage-accuracy trade-offs, with qualitative examples of semantics-aware navigation and articulated object manipulation.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,Mobile Manipulation},
  file = {/Users/fangyuan/Zotero/storage/FKE8DZZU/Qiu et al. - 2024 - Learning Generalizable Feature Fields for Mobile Manipulation.pdf;/Users/fangyuan/Zotero/storage/VYMDVPM4/2403.html}
}

@misc{qiuOpenvocabularyMobileManipulation2024,
  title = {Open-Vocabulary {{Mobile Manipulation}} in {{Unseen Dynamic Environments}} with {{3D Semantic Maps}}},
  author = {Qiu, Dicong and Ma, Wenzong and Pan, Zhenfu and Xiong, Hui and Liang, Junwei},
  year = {2024},
  month = jun,
  number = {arXiv:2406.18115},
  eprint = {2406.18115},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-27},
  abstract = {Open-Vocabulary Mobile Manipulation (OVMM) is a crucial capability for autonomous robots, especially when faced with the challenges posed by unknown and dynamic environments. This task requires robots to explore and build a semantic understanding of their surroundings, generate feasible plans to achieve manipulation goals, adapt to environmental changes, and comprehend natural language instructions from humans. To address these challenges, we propose a novel framework that leverages the zero-shot detection and grounded recognition capabilities of pretraining visual-language models (VLMs) combined with dense 3D entity reconstruction to build 3D semantic maps. Additionally, we utilize large language models (LLMs) for spatial region abstraction and online planning, incorporating human instructions and spatial semantic context. We have built a 10-DoF mobile manipulation robotic platform JSR-1 and demonstrated in real-world robot experiments that our proposed framework can effectively capture spatial semantics and process natural language user instructions for zero-shot OVMM tasks under dynamic environment settings, with an overall navigation and task success rate of 80.95\% and 73.33\% over 105 episodes, and better SFT and SPL by 157.18\% and 19.53\% respectively compared to the baseline. Furthermore, the framework is capable of replanning towards the next most probable candidate location based on the spatial semantic context derived from the 3D semantic map when initial plans fail, keeping an average success rate of 76.67\%.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {LLM,Manipulation,Mobile Manipulation,Open Vocabulary,VLM},
  file = {/Users/fangyuan/Zotero/storage/NR39I5VQ/Qiu et al. - 2024 - Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps.pdf}
}

@misc{qiuWildLMaLongHorizon2024,
  title = {{{WildLMa}}: {{Long Horizon Loco-Manipulation}} in the {{Wild}}},
  shorttitle = {{{WildLMa}}},
  author = {Qiu, Ri-Zhao and Song, Yuchen and Peng, Xuanbin and Suryadevara, Sai Aneesh and Yang, Ge and Liu, Minghuan and Ji, Mazeyu and Jia, Chengzhe and Yang, Ruihan and Zou, Xueyan and Wang, Xiaolong},
  year = {2024},
  month = nov,
  number = {arXiv:2411.15131},
  eprint = {2411.15131},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.15131},
  urldate = {2024-11-25},
  abstract = {`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world environments, which requires the robot to (1) have skills that generalize across object configurations; (2) be capable of long-horizon task execution in diverse environments; and (3) perform complex manipulation beyond pick-and-place. Quadruped robots with manipulators hold promise for extending the workspace and enabling robust locomotion, but existing results do not investigate such a capability. This paper proposes WildLMa with three components to address these issues: (1) adaptation of learned low-level controller for VR-enabled whole-body teleoperation and traversability; (2) WildLMa-Skill -- a library of generalizable visuomotor skills acquired via imitation learning or heuristics and (3) WildLMa-Planner -- an interface of learned skills that allow LLM planners to coordinate skills for long-horizon tasks. We demonstrate the importance of high-quality training data by achieving higher grasping success rate over existing RL baselines using only tens of demonstrations. WildLMa exploits CLIP for language-conditioned imitation learning that empirically generalizes to objects unseen in training demonstrations. Besides extensive quantitative evaluation, we qualitatively demonstrate practical robot applications, such as cleaning up trash in university hallways or outdoor terrains, operating articulated objects, and rearranging items on a bookshelf.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/8A3JXZN7/Qiu et al. - 2024 - WildLMa Long Horizon Loco-Manipulation in the Wild.pdf;/Users/fangyuan/Zotero/storage/QM63KZLM/2411.html}
}

@misc{quSpatialVLAExploringSpatial2025,
  title = {{{SpatialVLA}}: {{Exploring Spatial Representations}} for {{Visual-Language-Action Model}}},
  shorttitle = {{{SpatialVLA}}},
  author = {Qu, Delin and Song, Haoming and Chen, Qizhi and Yao, Yuanqi and Ye, Xinyi and Ding, Yan and Wang, Zhigang and Gu, JiaYuan and Zhao, Bin and Wang, Dong and Li, Xuelong},
  year = {2025},
  month = jan,
  number = {arXiv:2501.15830},
  eprint = {2501.15830},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.15830},
  urldate = {2025-02-01},
  abstract = {In this paper, we claim that spatial understanding is the keypoint in robot manipulation, and propose SpatialVLA to explore effective spatial representations for the robot foundation model. Specifically, we introduce Ego3D Position Encoding to inject 3D information into the input observations of the visual-language-action model, and propose Adaptive Action Grids to represent spatial robot movement actions with adaptive discretized action grids, facilitating learning generalizable and transferrable spatial action knowledge for cross-robot control. SpatialVLA is first pre-trained on top of a vision-language model with 1.1 Million real-world robot episodes, to learn a generalist manipulation policy across multiple robot environments and tasks. After pre-training, SpatialVLA is directly applied to perform numerous tasks in a zero-shot manner. The superior results in both simulation and real-world robots demonstrate its advantage of inferring complex robot motion trajectories and its strong in-domain multi-task generalization ability. We further show the proposed Adaptive Action Grids offer a new and effective way to fine-tune the pre-trained SpatialVLA model for new simulation and real-world setups, where the pre-learned action grids are re-discretized to capture robot-specific spatial action movements of new setups. The superior results from extensive evaluations demonstrate the exceptional in-distribution generalization and out-of-distribution adaptation capability, highlighting the crucial benefit of the proposed spatial-aware representations for generalist robot policy learning. All the details and codes will be open-sourced.},
  archiveprefix = {arXiv},
  keywords = {Action Tokenizer,Manipulation,Spatial,VLA},
  file = {/Users/fangyuan/Zotero/storage/XTHNHWXG/Qu et al. - 2025 - SpatialVLA Exploring Spatial Representations for Visual-Language-Action Model.pdf;/Users/fangyuan/Zotero/storage/AC5RF28E/2501.html}
}

@misc{radosavovicHumanoidLocomotionNext2024,
  title = {Humanoid {{Locomotion}} as {{Next Token Prediction}}},
  author = {Radosavovic, Ilija and Zhang, Bike and Shi, Baifeng and Rajasegaran, Jathushan and Kamat, Sarthak and Darrell, Trevor and Sreenath, Koushil and Malik, Jitendra},
  year = {2024},
  month = feb,
  number = {arXiv:2402.19469},
  eprint = {2402.19469},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.19469},
  urldate = {2024-12-06},
  abstract = {We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.},
  archiveprefix = {arXiv},
  keywords = {Digit,Locomotion},
  file = {/Users/fangyuan/Zotero/storage/S9E6UPUH/Radosavovic et al. - 2024 - Humanoid Locomotion as Next Token Prediction.pdf;/Users/fangyuan/Zotero/storage/QNPAYCLM/2402.html}
}

@misc{radosavovicLearningHumanoidLocomotion2024,
  title = {Learning {{Humanoid Locomotion}} over {{Challenging Terrain}}},
  author = {Radosavovic, Ilija and Kamat, Sarthak and Darrell, Trevor and Malik, Jitendra},
  year = {2024},
  month = oct,
  number = {arXiv:2410.03654},
  eprint = {2410.03654},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.03654},
  urldate = {2024-12-06},
  abstract = {Humanoid robots can, in principle, use their legs to go almost anywhere. Developing controllers capable of traversing diverse terrains, however, remains a considerable challenge. Classical controllers are hard to generalize broadly while the learning-based methods have primarily focused on gentle terrains. Here, we present a learning-based approach for blind humanoid locomotion capable of traversing challenging natural and man-made terrain. Our method uses a transformer model to predict the next action based on the history of proprioceptive observations and actions. The model is first pre-trained on a dataset of flat-ground trajectories with sequence modeling, and then fine-tuned on uneven terrain using reinforcement learning. We evaluate our model on a real humanoid robot across a variety of terrains, including rough, deformable, and sloped surfaces. The model demonstrates robust performance, in-context adaptation, and emergent terrain representations. In real-world case studies, our humanoid robot successfully traversed over 4 miles of hiking trails in Berkeley and climbed some of the steepest streets in San Francisco.},
  archiveprefix = {arXiv},
  keywords = {Digit,Locomotion},
  file = {/Users/fangyuan/Zotero/storage/EIBIFSV7/Radosavovic et al. - 2024 - Learning Humanoid Locomotion over Challenging Terrain.pdf;/Users/fangyuan/Zotero/storage/ZHXXYXK3/2410.html}
}

@misc{radosavovicRealWorldHumanoidLocomotion2023,
  title = {Real-{{World Humanoid Locomotion}} with {{Reinforcement Learning}}},
  author = {Radosavovic, Ilija and Xiao, Tete and Zhang, Bike and Darrell, Trevor and Malik, Jitendra and Sreenath, Koushil},
  year = {2023},
  month = dec,
  number = {arXiv:2303.03381},
  eprint = {2303.03381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.03381},
  urldate = {2024-12-06},
  abstract = {Humanoid robots that can autonomously operate in diverse environments have the potential to help address labour shortages in factories, assist elderly at homes, and colonize new planets. While classical controllers for humanoid robots have shown impressive results in a number of settings, they are challenging to generalize and adapt to new environments. Here, we present a fully learning-based approach for real-world humanoid locomotion. Our controller is a causal transformer that takes the history of proprioceptive observations and actions as input and predicts the next action. We hypothesize that the observation-action history contains useful information about the world that a powerful transformer model can use to adapt its behavior in-context, without updating its weights. We train our model with large-scale model-free reinforcement learning on an ensemble of randomized environments in simulation and deploy it to the real world zero-shot. Our controller can walk over various outdoor terrains, is robust to external disturbances, and can adapt in context.},
  archiveprefix = {arXiv},
  keywords = {Digit,Locomotion},
  file = {/Users/fangyuan/Zotero/storage/YYWAGVP3/Radosavovic et al. - 2023 - Real-World Humanoid Locomotion with Reinforcement Learning.pdf;/Users/fangyuan/Zotero/storage/EBMFWUG7/2303.html}
}

@misc{rajeswaranLearningComplexDexterous2018,
  title = {Learning {{Complex Dexterous Manipulation}} with {{Deep Reinforcement Learning}} and {{Demonstrations}}},
  author = {Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  year = {2018},
  month = jun,
  number = {arXiv:1709.10087},
  eprint = {1709.10087},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1709.10087},
  urldate = {2022-12-20},
  abstract = {Dexterous multi-fingered hands are extremely versatile and provide a generic way to perform a multitude of tasks in human-centric environments. However, effectively controlling them remains challenging due to their high dimensionality and large number of potential contacts. Deep reinforcement learning (DRL) provides a model-agnostic approach to control complex dynamical systems, but has not been shown to scale to high-dimensional dexterous manipulation. Furthermore, deployment of DRL on physical systems remains challenging due to sample inefficiency. Consequently, the success of DRL in robotics has thus far been limited to simpler manipulators and tasks. In this work, we show that model-free DRL can effectively scale up to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve them from scratch in simulated experiments. Furthermore, with the use of a small number of human demonstrations, the sample complexity can be significantly reduced, which enables learning with sample sizes equivalent to a few hours of robot experience. The use of demonstrations result in policies that exhibit very natural movements and, surprisingly, are also substantially more robust.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/ES5I6LUG/Rajeswaran et al. - 2018 - Learning Complex Dexterous Manipulation with Deep .pdf;/Users/fangyuan/Zotero/storage/38VSX7GM/1709.html}
}

@misc{renDINOXUnifiedVision2024,
  title = {{{DINO-X}}: {{A Unified Vision Model}} for {{Open-World Object Detection}} and {{Understanding}}},
  shorttitle = {{{DINO-X}}},
  author = {Ren, Tianhe and Chen, Yihao and Jiang, Qing and Zeng, Zhaoyang and Xiong, Yuda and Liu, Wenlong and Ma, Zhengyu and Shen, Junyi and Gao, Yuan and Jiang, Xiaoke and Chen, Xingyu and Song, Zhuheng and Zhang, Yuhong and Huang, Hongjie and Gao, Han and Liu, Shilong and Zhang, Hao and Li, Feng and Yu, Kent and Zhang, Lei},
  year = {2024},
  month = nov,
  number = {arXiv:2411.14347},
  eprint = {2411.14347},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.14347},
  urldate = {2024-11-24},
  abstract = {In this paper, we introduce DINO-X, which is a unified object-centric vision model developed by IDEA Research with the best open-world object detection performance to date. DINO-X employs the same Transformer-based encoder-decoder architecture as Grounding DINO 1.5 to pursue an object-level representation for open-world object understanding. To make long-tailed object detection easy, DINO-X extends its input options to support text prompt, visual prompt, and customized prompt. With such flexible prompt options, we develop a universal object prompt to support prompt-free open-world detection, making it possible to detect anything in an image without requiring users to provide any prompt. To enhance the model's core grounding capability, we have constructed a large-scale dataset with over 100 million high-quality grounding samples, referred to as Grounding-100M, for advancing the model's open-vocabulary detection performance. Pre-training on such a large-scale grounding dataset leads to a foundational object-level representation, which enables DINO-X to integrate multiple perception heads to simultaneously support multiple object perception and understanding tasks, including detection, segmentation, pose estimation, object captioning, object-based QA, etc. Experimental results demonstrate the superior performance of DINO-X. Specifically, the DINO-X Pro model achieves 56.0 AP, 59.8 AP, and 52.4 AP on the COCO, LVIS-minival, and LVIS-val zero-shot object detection benchmarks, respectively. Notably, it scores 63.3 AP and 56.5 AP on the rare classes of LVIS-minival and LVIS-val benchmarks, both improving the previous SOTA performance by 5.8 AP. Such a result underscores its significantly improved capacity for recognizing long-tailed objects.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/B8CKZ8CI/Ren et al. - 2024 - DINO-X A Unified Vision Model for Open-World Object Detection and Understanding.pdf;/Users/fangyuan/Zotero/storage/NCAJ3RHQ/2411.html}
}

@article{renExplorationHindsightGoal,
  title = {Exploration via {{Hindsight Goal Generation}}},
  author = {Ren, Zhizhou and Dong, Kefan and Zhou, Yuan and Liu, Qiang and Peng, Jian},
  pages = {12},
  abstract = {Goal-oriented reinforcement learning has recently been a practical framework for robotic manipulation tasks, in which an agent is required to reach a certain goal defined by a function on the state space. However, the sparsity of such reward definition makes traditional reinforcement learning algorithms very inefficient. Hindsight Experience Replay (HER), a recent advance, has greatly improved sample efficiency and practical applicability for such problems. It exploits previous replays by constructing imaginary goals in a simple heuristic way, acting like an implicit curriculum to alleviate the challenge of sparse reward signal. In this paper, we introduce Hindsight Goal Generation (HGG), a novel algorithmic framework that generates valuable hindsight goals which are easy for an agent to achieve in the short term and are also potential for guiding the agent to reach the actual goal in the long term. We have extensively evaluated our goal generation algorithm on a number of robotic manipulation tasks and demonstrated substantially improvement over the original HER in terms of sample efficiency.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/A8PW3DCK/Ren et al. - Exploration via Hindsight Goal Generation.pdf}
}

@misc{rhoLanguageGuidedSkill2024,
  title = {Language {{Guided Skill Discovery}}},
  author = {Rho, Seungeun and Smith, Laura and Li, Tianyu and Levine, Sergey and Peng, Xue Bin and Ha, Sehoon},
  year = {2024},
  month = jun,
  number = {arXiv:2406.06615},
  eprint = {2406.06615},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.06615},
  urldate = {2024-06-16},
  abstract = {Skill discovery methods enable agents to learn diverse emergent behaviors without explicit rewards. To make learned skills useful for unknown downstream tasks, obtaining a semantically diverse repertoire of skills is essential. While some approaches introduce a discriminator to distinguish skills and others aim to increase state coverage, no existing work directly addresses the "semantic diversity" of skills. We hypothesize that leveraging the semantic knowledge of large language models (LLMs) can lead us to improve semantic diversity of resulting behaviors. In this sense, we introduce Language Guided Skill Discovery (LGSD), a skill discovery framework that aims to directly maximize the semantic diversity between skills. LGSD takes user prompts as input and outputs a set of semantically distinctive skills. The prompts serve as a means to constrain the search space into a semantically desired subspace, and the generated LLM outputs guide the agent to visit semantically diverse states within the subspace. We demonstrate that LGSD enables legged robots to visit different user-intended areas on a plane by simply changing the prompt. Furthermore, we show that language guidance aids in discovering more diverse skills compared to five existing skill discovery methods in robot-arm manipulation environments. Lastly, LGSD provides a simple way of utilizing learned skills via natural language.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/N4YACA7G/Rho et al. - 2024 - Language Guided Skill Discovery.pdf;/Users/fangyuan/Zotero/storage/WV82DUJB/2406.html}
}

@misc{rhoLanguageGuidedSkill2025,
  title = {Language {{Guided Skill Discovery}}},
  author = {Rho, Seungeun and Smith, Laura and Li, Tianyu and Levine, Sergey and Peng, Xue Bin and Ha, Sehoon},
  year = {2025},
  month = mar,
  number = {arXiv:2406.06615},
  eprint = {2406.06615},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.06615},
  urldate = {2025-03-10},
  abstract = {Skill discovery methods enable agents to learn diverse emergent behaviors without explicit rewards. To make learned skills useful for unknown downstream tasks, obtaining a semantically diverse repertoire of skills is essential. While some approaches introduce a discriminator to distinguish skills and others aim to increase state coverage, no existing work directly addresses the "semantic diversity" of skills. We hypothesize that leveraging the semantic knowledge of large language models (LLMs) can lead us to improve semantic diversity of resulting behaviors. In this sense, we introduce Language Guided Skill Discovery (LGSD), a skill discovery framework that aims to directly maximize the semantic diversity between skills. LGSD takes user prompts as input and outputs a set of semantically distinctive skills. The prompts serve as a means to constrain the search space into a semantically desired subspace, and the generated LLM outputs guide the agent to visit semantically diverse states within the subspace. We demonstrate that LGSD enables legged robots to visit different user-intended areas on a plane by simply changing the prompt. Furthermore, we show that language guidance aids in discovering more diverse skills compared to five existing skill discovery methods in robot-arm manipulation environments. Lastly, LGSD provides a simple way of utilizing learned skills via natural language.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/E6U35SZ7/Rho et al. - 2025 - Language Guided Skill Discovery.pdf;/Users/fangyuan/Zotero/storage/U5HM9PV3/2406.html}
}

@misc{rojasBurningREDUnlocking2024,
  title = {Burning {{RED}}: {{Unlocking Subtask-Driven Reinforcement Learning}} and {{Risk-Awareness}} in {{Average-Reward Markov Decision Processes}}},
  shorttitle = {Burning {{RED}}},
  author = {Rojas, Juan Sebastian and Lee, Chi-Guhn},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10578},
  eprint = {2410.10578},
  publisher = {arXiv},
  urldate = {2024-11-10},
  abstract = {Average-reward Markov decision processes (MDPs) provide a foundational framework for sequential decision-making under uncertainty. However, average-reward MDPs have remained largely unexplored in reinforcement learning (RL) settings, with the majority of RL-based efforts having been allocated to episodic and discounted MDPs. In this work, we study a unique structural property of average-reward MDPs and utilize it to introduce Reward-Extended Differential (or RED) reinforcement learning: a novel RL framework that can be used to effectively and efficiently solve various subtasks simultaneously in the average-reward setting. We introduce a family of RED learning algorithms for prediction and control, including proven-convergent algorithms for the tabular case. We then showcase the power of these algorithms by demonstrating how they can be used to learn a policy that optimizes, for the first time, the well-known conditional value-at-risk (CVaR) risk measure in a fully-online manner, without the use of an explicit bi-level optimization scheme or an augmented state-space.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/SH25U7KL/Rojas and Lee - 2024 - Burning RED Unlocking Subtask-Driven Reinforcement Learning and Risk-Awareness in Average-Reward Ma.pdf;/Users/fangyuan/Zotero/storage/B6BYLYU8/2410.html}
}

@misc{rosenfeldLeveragingHumanKnowledge2018,
  title = {Leveraging Human Knowledge in Tabular Reinforcement Learning: {{A}} Study of Human Subjects},
  shorttitle = {Leveraging Human Knowledge in Tabular Reinforcement Learning},
  author = {Rosenfeld, Ariel and Cohen, Moshe and Taylor, Matthew E. and Kraus, Sarit},
  year = {2018},
  month = may,
  number = {arXiv:1805.05769},
  eprint = {1805.05769},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-08},
  abstract = {Reinforcement Learning (RL) can be extremely effective in solving complex, real-world problems. However, injecting human knowledge into an RL agent may require extensive effort and expertise on the human designer's part. To date, human factors are generally not considered in the development and evaluation of possible RL approaches. In this article, we set out to investigate how different methods for injecting human knowledge are applied, in practice, by human designers of varying levels of knowledge and skill. We perform the first empirical evaluation of several methods, including a newly proposed method named SASS which is based on the notion of similarities in the agent's state-action space. Through this human study, consisting of 51 human participants, we shed new light on the human factors that play a key role in RL. We find that the classical reward shaping technique seems to be the most natural method for most designers, both expert and non-expert, to speed up RL. However, we further find that our proposed method SASS can be effectively and efficiently combined with reward shaping, and provides a beneficial alternative to using only a single speedup method with minimal human designer effort overhead.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/WEE4MKVS/Rosenfeld et al. - 2018 - Leveraging human knowledge in tabular reinforcemen.pdf}
}

@misc{rouxelFlowMatchingImitation2024,
  title = {Flow {{Matching Imitation Learning}} for {{Multi-Support Manipulation}}},
  author = {Rouxel, Quentin and Ferrari, Andrea and Ivaldi, Serena and Mouret, Jean-Baptiste},
  year = {2024},
  month = oct,
  number = {arXiv:2407.12381},
  eprint = {2407.12381},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.12381},
  urldate = {2024-12-06},
  abstract = {Humanoid robots could benefit from using their upper bodies for support contacts, enhancing their workspace, stability, and ability to perform contact-rich and pushing tasks. In this paper, we propose a unified approach that combines an optimization-based multi-contact whole-body controller with Flow Matching, a recently introduced method capable of generating multi-modal trajectory distributions for imitation learning. In simulation, we show that Flow Matching is more appropriate for robotics than Diffusion and traditional behavior cloning. On a real full-size humanoid robot (Talos), we demonstrate that our approach can learn a whole-body non-prehensile box-pushing task and that the robot can close dishwasher drawers by adding contacts with its free hand when needed for balance. We also introduce a shared autonomy mode for assisted teleoperation, providing automatic contact placement for tasks not covered in the demonstrations. Full experimental videos are available at: https://hucebot.github.io/flow\_multisupport\_website/},
  archiveprefix = {arXiv},
  keywords = {Flow Matching,Manipulation,Talos},
  file = {/Users/fangyuan/Zotero/storage/TQBMGIHN/Rouxel et al. - 2024 - Flow Matching Imitation Learning for Multi-Support Manipulation.pdf;/Users/fangyuan/Zotero/storage/G4MEEDN5/2407.html}
}

@misc{rozlivekHARMONIOUSHumanlikeReactive2024,
  title = {{{HARMONIOUS}} -- {{Human-like}} Reactive Motion Control and Multimodal Perception for Humanoid Robots},
  author = {Rozlivek, Jakub and Roncone, Alessandro and Pattacini, Ugo and Hoffmann, Matej},
  year = {2024},
  month = dec,
  number = {arXiv:2312.02711},
  eprint = {2312.02711},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.02711},
  urldate = {2024-12-18},
  abstract = {For safe and effective operation of humanoid robots in human-populated environments, the problem of commanding a large number of Degrees of Freedom (DoF) while simultaneously considering dynamic obstacles and human proximity has still not been solved. We present a new reactive motion controller that commands two arms of a humanoid robot and three torso joints (17 DoF in total). We formulate a quadratic program that seeks joint velocity commands respecting multiple constraints while minimizing the magnitude of the velocities. We introduce a new unified treatment of obstacles that dynamically maps visual and proximity (pre-collision) and tactile (post-collision) obstacles as additional constraints to the motion controller, in a distributed fashion over the surface of the upper body of the iCub robot (with 2000 pressure-sensitive receptors). This results in a bio-inspired controller that: (i) gives rise to a robot with whole-body visuo-tactile awareness, resembling peripersonal space representations, and (ii) produces human-like minimum jerk movement profiles. The controller was extensively experimentally validated, including a physical human-robot interaction scenario.},
  archiveprefix = {arXiv},
  keywords = {Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/4LESHNWH/Rozlivek et al. - 2024 - HARMONIOUS -- Human-like reactive motion control and multimodal perception for humanoid robots.pdf;/Users/fangyuan/Zotero/storage/ES3FFBKB/2312.html}
}

@article{schaulUniversalValueFunction,
  title = {Universal {{Value Function Approximators}}},
  author = {Schaul, Tom and Horgan, Dan and Gregor, Karol and Silver, David},
  pages = {9},
  abstract = {Value functions are a core component of reinforcement learning systems. The main idea is to to construct a single function approximator V (s; {\texttheta}) that estimates the long-term reward from any state s, using parameters {\texttheta}. In this paper we introduce universal value function approximators (UVFAs) V (s, g; {\texttheta}) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/HUKLGDL9/Schaul et al. - Universal Value Function Approximators.pdf}
}

@misc{schickToolformerLanguageModels2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04761},
  eprint = {2302.04761},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-15},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/XC6DKLFM/Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves t.pdf}
}

@misc{schickToolformerLanguageModels2023a,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04761},
  eprint = {2302.04761},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.04761},
  urldate = {2024-02-16},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/FP269W9I/Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves t.pdf;/Users/fangyuan/Zotero/storage/YQWNLNJ8/2302.html}
}

@misc{schulmanHighDimensionalContinuousControl2018,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  year = {2018},
  month = oct,
  number = {arXiv:1506.02438},
  eprint = {1506.02438},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-06},
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/XMB7N73S/Schulman et al. - 2018 - High-Dimensional Continuous Control Using Generali.pdf;/Users/fangyuan/Zotero/storage/BJSMJBFE/1506.html}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  eprint = {1707.06347},
  primaryclass = {cs},
  urldate = {2022-07-19},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/LRVAVEJF/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf}
}

@misc{schulmanTrustRegionPolicy2017,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2017},
  month = apr,
  eprint = {1502.05477},
  primaryclass = {cs},
  urldate = {2022-07-19},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/244GLLVE/Schulman et al. - 2017 - Trust Region Policy Optimization.pdf}
}

@misc{senanayakeRolePredictiveUncertainty2024,
  title = {The {{Role}} of {{Predictive Uncertainty}} and {{Diversity}} in {{Embodied AI}} and {{Robot Learning}}},
  author = {Senanayake, Ransalu},
  year = {2024},
  month = may,
  number = {arXiv:2405.03164},
  eprint = {2405.03164},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.03164},
  urldate = {2024-05-20},
  abstract = {Uncertainty has long been a critical area of study in robotics, particularly when robots are equipped with analytical models. As we move towards the widespread use of deep neural networks in robots, which have demonstrated remarkable performance in research settings, understanding the nuances of uncertainty becomes crucial for their real-world deployment. This guide offers an overview of the importance of uncertainty and provides methods to quantify and evaluate it from an applications perspective.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/PDHHG7D8/Senanayake - 2024 - The Role of Predictive Uncertainty and Diversity i.pdf;/Users/fangyuan/Zotero/storage/LY3X88FB/2405.html}
}

@misc{seoCoarsetofineQNetworkAction2025,
  title = {Coarse-to-Fine {{Q-Network}} with {{Action Sequence}} for {{Data-Efficient Robot Learning}}},
  author = {Seo, Younggyo and Abbeel, Pieter},
  year = {2025},
  month = feb,
  number = {arXiv:2411.12155},
  eprint = {2411.12155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.12155},
  urldate = {2025-02-07},
  abstract = {In reinforcement learning (RL), we train a value function to understand the long-term consequence of executing a single action. However, the value of taking each action can be ambiguous in robotics as robot movements are typically the aggregate result of executing multiple small actions. Moreover, robotic training data often consists of noisy trajectories, in which each action is noisy but executing a series of actions results in a meaningful robot movement. This further makes it difficult for the value function to understand the effect of individual actions. To address this, we introduce Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that learns a critic network that outputs Q-values over a sequence of actions, i.e., explicitly training the value function to learn the consequence of executing action sequences. We study our algorithm on 53 robotic tasks with sparse and dense rewards, as well as with and without demonstrations, from BiGym, HumanoidBench, and RLBench. We find that CQN-AS outperforms various baselines, in particular on humanoid control tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/3KP5ERLY/Seo and Abbeel - 2025 - Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot Learning.pdf;/Users/fangyuan/Zotero/storage/VJM2TUB3/2411.html}
}

@misc{seoContinuousControlCoarsetofine2024,
  title = {Continuous {{Control}} with {{Coarse-to-fine Reinforcement Learning}}},
  author = {Seo, Younggyo and Uru{\c c}, Jafar and James, Stephen},
  year = {2024},
  month = jul,
  number = {arXiv:2407.07787},
  eprint = {2407.07787},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.07787},
  urldate = {2024-11-22},
  abstract = {Despite recent advances in improving the sample-efficiency of reinforcement learning (RL) algorithms, designing an RL algorithm that can be practically deployed in real-world environments remains a challenge. In this paper, we present Coarse-to-fine Reinforcement Learning (CRL), a framework that trains RL agents to zoom-into a continuous action space in a coarse-to-fine manner, enabling the use of stable, sample-efficient value-based RL algorithms for fine-grained continuous control tasks. Our key idea is to train agents that output actions by iterating the procedure of (i) discretizing the continuous action space into multiple intervals and (ii) selecting the interval with the highest Q-value to further discretize at the next level. We then introduce a concrete, value-based algorithm within the CRL framework called Coarse-to-fine Q-Network (CQN). Our experiments demonstrate that CQN significantly outperforms RL and behavior cloning baselines on 20 sparsely-rewarded RLBench manipulation tasks with a modest number of environment interactions and expert demonstrations. We also show that CQN robustly learns to solve real-world manipulation tasks within a few minutes of online training.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/E7B5W5ZK/Seo et al. - 2024 - Continuous Control with Coarse-to-fine Reinforcement Learning.pdf;/Users/fangyuan/Zotero/storage/XRXSNU72/2407.html}
}

@misc{seoDeepImitationLearning2023,
  title = {Deep {{Imitation Learning}} for {{Humanoid Loco-manipulation}} through {{Human Teleoperation}}},
  author = {Seo, Mingyo and Han, Steve and Sim, Kyutae and Bang, Seung Hyeon and Gonzalez, Carlos and Sentis, Luis and Zhu, Yuke},
  year = {2023},
  month = nov,
  number = {arXiv:2309.01952},
  eprint = {2309.01952},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.01952},
  urldate = {2024-12-06},
  abstract = {We tackle the problem of developing humanoid loco-manipulation skills with deep imitation learning. The difficulty of collecting task demonstrations and training policies for humanoids with a high degree of freedom presents substantial challenges. We introduce TRILL, a data-efficient framework for training humanoid loco-manipulation policies from human demonstrations. In this framework, we collect human demonstration data through an intuitive Virtual Reality (VR) interface. We employ the whole-body control formulation to transform task-space commands by human operators into the robot's joint-torque actuation while stabilizing its dynamics. By employing high-level action abstractions tailored for humanoid loco-manipulation, our method can efficiently learn complex sensorimotor skills. We demonstrate the effectiveness of TRILL in simulation and on a real-world robot for performing various loco-manipulation tasks. Videos and additional materials can be found on the project page: https://ut-austin-rpl.github.io/TRILL.},
  archiveprefix = {arXiv},
  keywords = {Imitation Learning,Manipulation,Teleoperation},
  file = {/Users/fangyuan/Zotero/storage/GS8W9GBL/Seo et al. - 2023 - Deep Imitation Learning for Humanoid Loco-manipulation through Human Teleoperation.pdf;/Users/fangyuan/Zotero/storage/WC5BNJM2/2309.html}
}

@misc{sferrazzaHumanoidBenchSimulatedHumanoid2024,
  title = {{{HumanoidBench}}: {{Simulated Humanoid Benchmark}} for {{Whole-Body Locomotion}} and {{Manipulation}}},
  shorttitle = {{{HumanoidBench}}},
  author = {Sferrazza, Carmelo and Huang, Dun-Ming and Lin, Xingyu and Lee, Youngwoon and Abbeel, Pieter},
  year = {2024},
  month = jun,
  number = {arXiv:2403.10506},
  eprint = {2403.10506},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.10506},
  urldate = {2024-12-06},
  abstract = {Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning approach achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitating prompt verification of algorithms and ideas. The open-source code is available at https://humanoid-bench.github.io.},
  archiveprefix = {arXiv},
  keywords = {Benchmark},
  file = {/Users/fangyuan/Zotero/storage/GZM5WVEP/Sferrazza et al. - 2024 - HumanoidBench Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation.pdf;/Users/fangyuan/Zotero/storage/5VQKMUUC/2403.html}
}

@inproceedings{shawBimanualDexterityComplex2024,
  title = {Bimanual {{Dexterity}} for {{Complex Tasks}}},
  booktitle = {8th {{Annual Conference}} on {{Robot Learning}}},
  author = {Shaw, Kenneth and Li, Yulong and Yang, Jiahui and Srirama, Mohan Kumar and Liu, Ray and Xiong, Haoyu and Mendonca, Russell and Pathak, Deepak},
  year = {2024},
  month = sep,
  urldate = {2024-12-06},
  abstract = {To train generalist robot policies, machine learning methods often require a substantial amount of expert human teleoperation data. An ideal robot for humans collecting data is one that closely mimics them: bimanual arms and dexterous hands. However, creating such a bimanual teleoperation system with over 50 DoF is a significant challenge. To address this, we introduce Bidex, an extremely dexterous, low-cost, low-latency and portable bimanual dexterous teleoperation system which relies on motion capture gloves and teacher arms. We compare Bidex to a Vision Pro teleoperation system and a SteamVR system and find Bidex to produce better quality data for more complex tasks at a faster rate. Additionally, we show Bidex operating a mobile bimanual robot for in the wild tasks. Please refer to https://bidex-teleop.github.io for video results and instructions to recreate Bidex. The robot hands (5k USD) and teleoperation system (7k USD) is readily reproducible and can be used on many robot arms including two xArms (\$16k USD).},
  langid = {english},
  keywords = {Dexterous Hand,Manipulation,Teleoperation},
  file = {/Users/fangyuan/Zotero/storage/7VBS89GN/Shaw et al. - 2024 - Bimanual Dexterity for Complex Tasks.pdf}
}

@misc{shentuLLMsActionsLatent2024,
  title = {From {{LLMs}} to {{Actions}}: {{Latent Codes}} as {{Bridges}} in {{Hierarchical Robot Control}}},
  shorttitle = {From {{LLMs}} to {{Actions}}},
  author = {Shentu, Yide and Wu, Philipp and Rajeswaran, Aravind and Abbeel, Pieter},
  year = {2024},
  month = jul,
  number = {arXiv:2405.04798},
  eprint = {2405.04798},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-14},
  abstract = {Hierarchical control for robotics has long been plagued by the need to have a well defined interface layer to communicate between high-level task planners and low-level policies. With the advent of LLMs, language has been emerging as a prospective interface layer. However, this has several limitations. Not all tasks can be decomposed into steps that are easily expressible in natural language (e.g. performing a dance routine). Further, it makes end-to-end finetuning on embodied data challenging due to domain shift and catastrophic forgetting. We introduce our method -- Learnable Latent Codes as Bridges (LCB) -- as an alternate architecture to overcome these limitations. {\textbackslash}method{\textasciitilde}uses a learnable latent code to act as a bridge between LLMs and low-level policies. This enables LLMs to flexibly communicate goals in the task plan without being entirely constrained by language limitations. Additionally, it enables end-to-end finetuning without destroying the embedding space of word tokens learned during pre-training. Through experiments on Language Table and Calvin, two common language based benchmarks for embodied agents, we find that {\textbackslash}method{\textasciitilde}outperforms baselines (including those w/ GPT-4V) that leverage pure language as the interface layer on tasks that require reasoning and multi-step behaviors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Action Token,Manipulation,Robotic Foundation Model,VLA},
  file = {/Users/fangyuan/Zotero/storage/L9SI7VR5/Shentu et al. - 2024 - From LLMs to Actions Latent Codes as Bridges in Hierarchical Robot Control.pdf}
}

@misc{shiHiRobotOpenEnded2025,
  title = {Hi {{Robot}}: {{Open-Ended Instruction Following}} with {{Hierarchical Vision-Language-Action Models}}},
  shorttitle = {Hi {{Robot}}},
  author = {Shi, Lucy Xiaoyang and Ichter, Brian and Equi, Michael and Ke, Liyiming and Pertsch, Karl and Vuong, Quan and Tanner, James and Walling, Anna and Wang, Haohuan and Fusai, Niccolo and {Li-Bell}, Adrian and Driess, Danny and Groom, Lachy and Levine, Sergey and Finn, Chelsea},
  year = {2025},
  month = feb,
  number = {arXiv:2502.19417},
  eprint = {2502.19417},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.19417},
  urldate = {2025-03-17},
  abstract = {Generalist robots that can perform a range of different tasks in open-world settings must be able to not only reason about the steps needed to accomplish their goals, but also process complex instructions, prompts, and even feedback during task execution. Intricate instructions (e.g., "Could you make me a vegetarian sandwich?" or "I don't like that one") require not just the ability to physically perform the individual steps, but the ability to situate complex commands and feedback in the physical world. In this work, we describe a system that uses vision-language models in a hierarchical structure, first reasoning over complex prompts and user feedback to deduce the most appropriate next step to fulfill the task, and then performing that step with low-level actions. In contrast to direct instruction following methods that can fulfill simple commands ("pick up the cup"), our system can reason through complex prompts and incorporate situated feedback during task execution ("that's not trash"). We evaluate our system across three robotic platforms, including single-arm, dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks such as cleaning messy tables, making sandwiches, and grocery shopping.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/9AL4PZSE/Shi et al. - 2025 - Hi Robot Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models.pdf;/Users/fangyuan/Zotero/storage/7SRD5TTR/2502.html}
}

@misc{shiRoboCookLongHorizonElastoPlastic2023,
  title = {{{RoboCook}}: {{Long-Horizon Elasto-Plastic Object Manipulation}} with {{Diverse Tools}}},
  shorttitle = {{{RoboCook}}},
  author = {Shi, Haochen and Xu, Huazhe and Clarke, Samuel and Li, Yunzhu and Wu, Jiajun},
  year = {2023},
  month = jun,
  number = {arXiv:2306.14447},
  eprint = {2306.14447},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-08},
  abstract = {Humans excel in complex long-horizon soft body manipulation tasks via flexible tool use: bread baking requires a knife to slice the dough and a rolling pin to flatten it. Often regarded as a hallmark of human cognition, tool use in autonomous robots remains limited due to challenges in understanding tool-object interactions. Here we develop an intelligent robotic system, RoboCook, which perceives, models, and manipulates elasto-plastic objects with various tools. RoboCook uses point cloud scene representations, models tool-object interactions with Graph Neural Networks (GNNs), and combines tool classification with self-supervised policy learning to devise manipulation plans. We demonstrate that from just 20 minutes of real-world interaction data per tool, a general-purpose robot arm can learn complex long-horizon soft object manipulation tasks, such as making dumplings and alphabet letter cookies. Extensive evaluations show that RoboCook substantially outperforms state-of-the-art approaches, exhibits robustness against severe external disturbances, and demonstrates adaptability to different materials.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/72XTSFHU/Shi et al. - 2023 - RoboCook Long-Horizon Elasto-Plastic Object Manip.pdf}
}

@misc{shiToddlerBotOpenSourceMLCompatible2025,
  title = {{{ToddlerBot}}: {{Open-Source ML-Compatible Humanoid Platform}} for {{Loco-Manipulation}}},
  shorttitle = {{{ToddlerBot}}},
  author = {Shi, Haochen and Wang, Weizhuo and Song, Shuran and Liu, C. Karen},
  year = {2025},
  month = feb,
  number = {arXiv:2502.00893},
  eprint = {2502.00893},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.00893},
  urldate = {2025-02-07},
  abstract = {Learning-based robotics research driven by data demands a new approach to robot hardware design-one that serves as both a platform for policy execution and a tool for embodied data collection to train policies. We introduce ToddlerBot, a low-cost, open-source humanoid robot platform designed for scalable policy learning and research in robotics and AI. ToddlerBot enables seamless acquisition of high-quality simulation and real-world data. The plug-and-play zero-point calibration and transferable motor system identification ensure a high-fidelity digital twin, enabling zero-shot policy transfer from simulation to the real world. A user-friendly teleoperation interface facilitates streamlined real-world data collection for learning motor skills from human demonstrations. Utilizing its data collection ability and anthropomorphic design, ToddlerBot is an ideal platform to perform whole-body loco-manipulation. Additionally, ToddlerBot's compact size (0.56m, 3.4kg) ensures safe operation in real-world environments. Reproducibility is achieved with an entirely 3D-printed, open-source design and commercially available components, keeping the total cost under 6,000 USD. Comprehensive documentation allows assembly and maintenance with basic technical expertise, as validated by a successful independent replication of the system. We demonstrate ToddlerBot's capabilities through arm span, payload, endurance tests, loco-manipulation tasks, and a collaborative long-horizon scenario where two robots tidy a toy session together. By advancing ML-compatibility, capability, and reproducibility, ToddlerBot provides a robust platform for scalable learning and dynamic policy execution in robotics research.},
  archiveprefix = {arXiv},
  keywords = {Benchmark,Locomotion,Manipulation},
  file = {/Users/fangyuan/Zotero/storage/9GBTWC8K/Shi et al. - 2025 - ToddlerBot Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation.pdf;/Users/fangyuan/Zotero/storage/MWDTIUJS/2502.html}
}

@misc{shorinwaSplatMOVERMultiStageOpenVocabulary2024,
  title = {Splat-{{MOVER}}: {{Multi-Stage}}, {{Open-Vocabulary Robotic Manipulation}} via {{Editable Gaussian Splatting}}},
  shorttitle = {Splat-{{MOVER}}},
  author = {Shorinwa, Ola and Tucker, Johnathan and Smith, Aliyah and Swann, Aiden and Chen, Timothy and Firoozi, Roya and Kennedy III, Monroe and Schwager, Mac},
  year = {2024},
  month = may,
  number = {arXiv:2405.04378},
  eprint = {2405.04378},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.04378},
  urldate = {2024-05-21},
  abstract = {We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic manipulation, which leverages the editability of Gaussian Splatting (GSplat) scene representations to enable multi-stage manipulation tasks. Splat-MOVER consists of: (i) ASK-Splat, a GSplat representation that distills latent codes for language semantics and grasp affordance into the 3D scene. ASK-Splat enables geometric, semantic, and affordance understanding of 3D scenes, which is critical for many robotics tasks; (ii) SEE-Splat, a real-time scene-editing module using 3D semantic masking and infilling to visualize the motions of objects that result from robot interactions in the real-world. SEE-Splat creates a "digital twin" of the evolving environment throughout the manipulation task; and (iii) Grasp-Splat, a grasp generation module that uses ASK-Splat and SEE-Splat to propose candidate grasps for open-world objects. ASK-Splat is trained in real-time from RGB images in a brief scanning phase prior to operation, while SEE-Splat and Grasp-Splat run in real-time during operation. We demonstrate the superior performance of Splat-MOVER in hardware experiments on a Kinova robot compared to two recent baselines in four single-stage, open-vocabulary manipulation tasks, as well as in four multi-stage manipulation tasks using the edited scene to reflect scene changes due to prior manipulation stages, which is not possible with the existing baselines. Code for this project and a link to the project page will be made available soon.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/AS99K4U2/Shorinwa et al. - 2024 - Splat-MOVER Multi-Stage, Open-Vocabulary Robotic .pdf;/Users/fangyuan/Zotero/storage/CK39ZW9K/2405.html}
}

@misc{shorinwaSplatMOVERMultiStageOpenVocabulary2024a,
  title = {Splat-{{MOVER}}: {{Multi-Stage}}, {{Open-Vocabulary Robotic Manipulation}} via {{Editable Gaussian Splatting}}},
  shorttitle = {Splat-{{MOVER}}},
  author = {Shorinwa, Ola and Tucker, Johnathan and Smith, Aliyah and Swann, Aiden and Chen, Timothy and Firoozi, Roya and Kennedy III, Monroe and Schwager, Mac},
  year = {2024},
  month = may,
  number = {arXiv:2405.04378},
  eprint = {2405.04378},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.04378},
  urldate = {2024-05-20},
  abstract = {We present Splat-MOVER, a modular robotics stack for open-vocabulary robotic manipulation, which leverages the editability of Gaussian Splatting (GSplat) scene representations to enable multi-stage manipulation tasks. Splat-MOVER consists of: (i) ASK-Splat, a GSplat representation that distills latent codes for language semantics and grasp affordance into the 3D scene. ASK-Splat enables geometric, semantic, and affordance understanding of 3D scenes, which is critical for many robotics tasks; (ii) SEE-Splat, a real-time scene-editing module using 3D semantic masking and infilling to visualize the motions of objects that result from robot interactions in the real-world. SEE-Splat creates a "digital twin" of the evolving environment throughout the manipulation task; and (iii) Grasp-Splat, a grasp generation module that uses ASK-Splat and SEE-Splat to propose candidate grasps for open-world objects. ASK-Splat is trained in real-time from RGB images in a brief scanning phase prior to operation, while SEE-Splat and Grasp-Splat run in real-time during operation. We demonstrate the superior performance of Splat-MOVER in hardware experiments on a Kinova robot compared to two recent baselines in four single-stage, open-vocabulary manipulation tasks, as well as in four multi-stage manipulation tasks using the edited scene to reflect scene changes due to prior manipulation stages, which is not possible with the existing baselines. Code for this project and a link to the project page will be made available soon.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/LBYAESU2/Shorinwa et al. - 2024 - Splat-MOVER Multi-Stage, Open-Vocabulary Robotic .pdf;/Users/fangyuan/Zotero/storage/JUZLWBMC/2405.html}
}

@misc{shumAutomaticPromptAugmentation2023,
  title = {Automatic {{Prompt Augmentation}} and {{Selection}} with {{Chain-of-Thought}} from {{Labeled Data}}},
  author = {Shum, KaShun and Diao, Shizhe and Zhang, Tong},
  year = {2023},
  month = feb,
  number = {arXiv:2302.12822},
  eprint = {2302.12822},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.12822},
  urldate = {2024-02-16},
  abstract = {Chain-of-thought prompting (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in arithmetic, commonsense, and symbolic reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt the language model, which poses challenges for real-world applications where labeled training data is available without human-annotated rational chains. This creates barriers to applications of CoT prompting to these general tasks. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoTs by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example in a black-box language model. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where state-of-the-art results are achieved on arithmetic reasoning (+2.7{\textbackslash}\%), commonsense reasoning (+3.4{\textbackslash}\%), symbolic reasoning (+3.2{\textbackslash}\%), and non-reasoning tasks (+2.5{\textbackslash}\%). Our code will be available at https://github.com/shizhediao/automate-cot.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/T5K3W9Y8/Shum et al. - 2023 - Automatic Prompt Augmentation and Selection with C.pdf;/Users/fangyuan/Zotero/storage/DG3HJBG2/2302.html}
}

@article{silverHighPerformanceOutdoor,
  title = {High {{Performance Outdoor Navigation}} from {{Overhead Data Using Imitation Learning}}},
  author = {Silver, David and Bagnell, J Andrew and Stentz, Andrew},
  abstract = {High performance, long-distance autonomous navigation is a central problem for field robotics. Efficient navigation relies not only upon intelligent onboard systems for perception and planning, but also the effective use of prior maps and knowledge. While the availability and quality of low cost, high resolution satellite and aerial terrain data continues to rapidly improve, automated interpretation appropriate for robot planning and navigation remains difficult. Recently, a class of machine learning techniques have been developed that rely upon expert human demonstration to develop a function mapping overhead data to traversal cost. These algorithms choose the cost function so that planner behavior mimics an expert's demonstration as closely as possible. In this work, we extend these methods to automate interpretation of overhead data. We address key challenges, including interpolation-based planners, non-linear approximation techniques, and imperfect expert demonstration, necessary to apply these methods for learning to search for effective terrain interpretations. We validate our approach on a large scale outdoor robot during over 300 kilometers of autonomous traversal through complex natural environments.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/3R3RWAGE/Silver et al. - High Performance Outdoor Navigation from Overhead .pdf}
}

@article{silverMasteringGameGo2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  urldate = {2022-12-20},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/fangyuan/Zotero/storage/WWMD9L6Q/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@misc{singhCRISPCurriculumInducing2024,
  title = {{{CRISP}}: {{Curriculum Inducing Primitive Informed Subgoal Prediction}} for {{Hierarchical Reinforcement Learning}}},
  shorttitle = {{{CRISP}}},
  author = {Singh, Utsav and Namboodiri, Vinay P.},
  year = {2024},
  month = sep,
  number = {arXiv:2304.03535},
  eprint = {2304.03535},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-29},
  abstract = {Hierarchical reinforcement learning (HRL) is a promising approach that uses temporal abstraction to solve complex long horizon problems. However, simultaneously learning a hierarchy of policies is unstable as it is challenging to train higher-level policy when the lower-level primitive is non-stationary. In this paper, we present CRISP, a novel HRL algorithm that effectively generates a curriculum of achievable subgoals for evolving lower-level primitives using reinforcement learning and imitation learning. CRISP uses the lower level primitive to periodically perform data relabeling on a handful of expert demonstrations, using a novel primitive informed parsing (PIP) approach, thereby mitigating non-stationarity. Since our approach only assumes access to a handful of expert demonstrations, it is suitable for most robotic control tasks. Experimental evaluations on complex robotic maze navigation and robotic manipulation tasks demonstrate that inducing hierarchical curriculum learning significantly improves sample efficiency, and results in efficient goal conditioned policies for solving temporally extended tasks. Additionally, we perform real world robotic experiments on complex manipulation tasks and demonstrate that CRISP demonstrates impressive generalization in real world scenarios.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/P6UESLIA/Singh and Namboodiri - 2024 - CRISP Curriculum Inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Lear.pdf}
}

@article{singhMemoryBasedGoalPlanning,
  title = {Memory-{{Based Goal Planning}} for {{Cognitive Agents}}: {{An Episodic Learning Approach}}},
  author = {Singh, Shweta and Seshadri, Shraddha},
  abstract = {As intelligent robots and cognitive agents increasingly operate in human-centric environments, the ability to retain and utilize past experiences through episodic memory is crucial for autonomous decision-making. This paper introduces Episodic Memory for Cognitive Agents (EMCA), a novel framework designed to learn from real-world interactions without requiring pretraining on predefined scenarios. Unlike conventional approaches that rely on fixed temporal patterns, recurrent architectures, or instruction-based navigation, EMCA employs a graph-based memory structure to incrementally store and retrieve experiences, facilitating flexible reasoning across diverse contexts.A key innovation of EMCA is its landmarkbased memory representation, which enables agents to retain and utilize spatial knowledge from exploration, even in the absence of predefined goals. This capability is particularly advantageous for outdoor navigation, where existing models primarily focus on constrained indoor environments. Furthermore, in contrast to Visual Language Navigation (VLN) systems that depend on structured textual commands, EMCA dynamically extracts task-relevant and environment-related information from natural human conversations.Additionally, EMCA introduces a context-sensitive retrieval mechanism, allowing memory access to adapt based on situational demands rather than predefined queries. This supports continuous learning, adaptive decision-making, and real-time updates, enhancing autonomy in unstructured environments. EMCA also incorporates an internal question-generation module, enabling agents to autonomously formulate goals based on episodic memory. By integrating deep learning architectures with memory graphs, EMCA ensures efficient memory retrieval and decision-making in dynamic scenarios, demonstrating superior adaptability, reasoning, and robustness in empirical evaluations. These contributions establish EMCA as a versatile framework for nextgeneration cognitive agents operating in complex and evolving real-world settings.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/3ZBBY5YL/Singh and Seshadri - Memory-Based Goal Planning for Cognitive Agents An Episodic Learning Approach.pdf}
}

@misc{singhOfflineRLRealistic2022,
  title = {Offline {{RL With Realistic Datasets}}: {{Heteroskedasticity}} and {{Support Constraints}}},
  shorttitle = {Offline {{RL With Realistic Datasets}}},
  author = {Singh, Anikait and Kumar, Aviral and Vuong, Quan and Chebotar, Yevgen and Levine, Sergey},
  year = {2022},
  month = nov,
  number = {arXiv:2211.01052},
  eprint = {2211.01052},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-11-04},
  abstract = {Offline reinforcement learning (RL) learns policies entirely from static datasets, thereby avoiding the challenges associated with online data collection. Practical applications of offline RL will inevitably require learning from datasets where the variability of demonstrated behaviors changes non-uniformly across the state space. For example, at a red light, nearly all human drivers behave similarly by stopping, but when merging onto a highway, some drivers merge quickly, efficiently, and safely, while many hesitate or merge dangerously. Both theoretically and empirically, we show that typical offline RL methods, which are based on distribution constraints fail to learn from data with such non-uniform variability, due to the requirement to stay close to the behavior policy to the same extent across the state space. Ideally, the learned policy should be free to choose per state how closely to follow the behavior policy to maximize long-term return, as long as the learned policy stays within the support of the behavior policy. To instantiate this principle, we reweight the data distribution in conservative Q-learning (CQL) to obtain an approximate support constraint formulation. The reweighted distribution is a mixture of the current policy and an additional policy trained to mine poor actions that are likely under the behavior policy. Our method, CQL (ReDS), is simple, theoretically motivated, and improves performance across a wide range of offline RL problems in Atari games, navigation, and pixel-based manipulation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/TBYK9T8P/Singh et al. - 2022 - Offline RL With Realistic Datasets Heteroskedasti.pdf}
}

@article{singhTransferLearningComposing,
  title = {Transfer of Learning by Composing Solutions of Elemental Sequential Tasks},
  author = {Singh, Satinder Pal},
  pages = {17},
  abstract = {Although building sophisticated learning agents that operate in complex environments will require learning to perform multiple tasks, most applications of reinforcement learning have focused on single tasks. In this paper I consider a class of sequential decision tasks (SDTs), called composite sequential decision tasks, formed by temporally concatenating a number of elemental sequential decision tasks. Elemental SDTs cannot be decomposed into simpler SDTs. I consider a learning agent that has to learn to solve a set of elemental and composite SDTs. I assume that the structure of the composite tasks is unknown to the learning agent. The straightforward application of reinforcement learning to multiple tasks requires learning the tasks separately, which can waste computational resources, both memory and time. I present a new learning algorithm and a modular architecture that learns the decomposition of composite SDTs, and achieves transfer of learning by sharing the solutions of elemental SDTs across multiple composite SDTs. The solution of a composite SDT is constructed by computationally inexpensive modifications of the solutions of its constituent elemental SDTs. I provide a proof of one aspect of the learning algorithm.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/4BLBZL9G/Singh - Transfer of learning by composing solutions of ele.pdf}
}

@misc{smithConvNetsMatchVision2023,
  title = {{{ConvNets Match Vision Transformers}} at {{Scale}}},
  author = {Smith, Samuel L. and Brock, Andrew and Berrada, Leonard and De, Soham},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16764},
  eprint = {2310.16764},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.16764},
  urldate = {2023-10-28},
  abstract = {Many researchers believe that ConvNets perform well on small or moderately sized datasets, but are not competitive with Vision Transformers when given access to datasets on the web-scale. We challenge this belief by evaluating a performant ConvNet architecture pre-trained on JFT-4B, a large labelled dataset of images often used for training foundation models. We consider pre-training compute budgets between 0.4k and 110k TPU-v4 core compute hours, and train a series of networks of increasing depth and width from the NFNet model family. We observe a log-log scaling law between held out loss and compute budget. After fine-tuning on ImageNet, NFNets match the reported performance of Vision Transformers with comparable compute budgets. Our strongest fine-tuned model achieves a Top-1 accuracy of 90.4\%.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/fangyuan/Zotero/storage/6LZFUIYT/Smith et al. - 2023 - ConvNets Match Vision Transformers at Scale.pdf;/Users/fangyuan/Zotero/storage/WF4CG28B/2310.html}
}

@misc{snellOfflineRLNatural2022,
  title = {Offline {{RL}} for {{Natural Language Generation}} with {{Implicit Language Q Learning}}},
  author = {Snell, Charlie and Kostrikov, Ilya and Su, Yi and Yang, Mengjiao and Levine, Sergey},
  year = {2022},
  month = jun,
  eprint = {2206.11871},
  primaryclass = {cs},
  urldate = {2022-07-12},
  abstract = {Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL motivated method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility optimization framework of traditional RL algorithms with supervised learning's ability to leverage existing data and its simplicity and stability. Our method, based on dynamic programming, employs a blend of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing utility. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language generation settings, demonstrating how it can be a more effective utility optimizer than prior approaches for end-to-end dialogue, and how it can effectively optimize high variance reward functions based on subjective judgement, such as whether to label a comment as an example of toxic speech or not.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/RKMVJGLT/Snell et al. - 2022 - Offline RL for Natural Language Generation with Implicit Language Q Learning.pdf}
}

@article{sohnHierarchicalReinforcementLearning,
  title = {Hierarchical {{Reinforcement Learning}} for {{Zero-shot Generalization}} with {{Subtask Dependencies}}},
  author = {Sohn, Sungryull and Oh, Junhyuk and Lee, Honglak},
  pages = {11},
  abstract = {We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradientbased policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/2CJHLMIG/Sohn et al. - Hierarchical Reinforcement Learning for Zero-shot .pdf}
}

@article{sohnHierarchicalReinforcementLearninga,
  title = {Hierarchical {{Reinforcement Learning}} for {{Zero-shot Generalization}} with {{Subtask Dependencies}}},
  author = {Sohn, Sungryull and Oh, Junhyuk and Lee, Honglak},
  pages = {11},
  abstract = {We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradientbased policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/MT4VJ5N3/Sohn et al. - Hierarchical Reinforcement Learning for Zero-shot .pdf}
}

@article{sohnLearningStructuredOutput,
  title = {Learning {{Structured Output Representation}} Using {{Deep Conditional Generative Models}}},
  author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
  pages = {9},
  abstract = {Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/73PFFGUA/Sohn et al. - Learning Structured Output Representation using De.pdf}
}

@misc{sohnMetaReinforcementLearning2020,
  title = {Meta {{Reinforcement Learning}} with {{Autonomous Inference}} of {{Subtask Dependencies}}},
  author = {Sohn, Sungryull and Woo, Hyunjae and Choi, Jongwook and Lee, Honglak},
  year = {2020},
  month = apr,
  number = {arXiv:2001.00248},
  eprint = {2001.00248},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-09-08},
  abstract = {We propose and address a novel few-shot RL problem, where a task is characterized by a subtask graph which describes a set of subtasks and their dependencies that are unknown to the agent. The agent needs to quickly adapt to the task over few episodes during adaptation phase to maximize the return in the test phase. Instead of directly learning a meta-policy, we develop a Meta-learner with Subtask Graph Inference (MSGI), which infers the latent parameter of the task by interacting with the environment and maximizes the return given the latent parameter. To facilitate learning, we adopt an intrinsic reward inspired by upper confidence bound (UCB) that encourages efficient exploration. Our experiment results on two grid-world domains and StarCraft II environments show that the proposed method is able to accurately infer the latent task parameter, and to adapt more efficiently than existing meta RL and hierarchical RL methods 1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/3YVKA397/Sohn et al. - 2020 - Meta Reinforcement Learning with Autonomous Infere.pdf}
}

@misc{songAcceleratingVisionLanguageActionModel2025,
  title = {Accelerating {{Vision-Language-Action Model Integrated}} with {{Action Chunking}} via {{Parallel Decoding}}},
  author = {Song, Wenxuan and Chen, Jiayi and Ding, Pengxiang and Zhao, Han and Zhao, Wei and Zhong, Zhide and Ge, Zongyuan and Ma, Jun and Li, Haoang},
  year = {2025},
  month = mar,
  number = {arXiv:2503.02310},
  eprint = {2503.02310},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.02310},
  urldate = {2025-03-11},
  abstract = {Vision-Language-Action (VLA) models demonstrate remarkable potential for generalizable robotic manipulation. The performance of VLA models can be improved by integrating with action chunking, a critical technique for effective control. However, action chunking linearly scales up action dimensions in VLA models with increased chunking sizes. This reduces the inference efficiency. To tackle this problem, we propose PD-VLA, the first parallel decoding framework for VLA models integrated with action chunking. Our framework reformulates autoregressive decoding as a nonlinear system solved by parallel fixed-point iterations. This approach preserves model performance with mathematical guarantees while significantly improving decoding speed. In addition, it enables training-free acceleration without architectural changes, as well as seamless synergy with existing acceleration techniques. Extensive simulations validate that our PD-VLA maintains competitive success rates while achieving 2.52 times execution frequency on manipulators (with 7 degrees of freedom) compared with the fundamental VLA model. Furthermore, we experimentally identify the most effective settings for acceleration. Finally, real-world experiments validate its high applicability across different tasks.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/EUZWR2TU/Song et al. - 2025 - Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding.pdf;/Users/fangyuan/Zotero/storage/DPTICKAW/2503.html}
}

@misc{spisakDIRIGENtEndToEndRobotic2025,
  title = {{{DIRIGENt}}: {{End-To-End Robotic Imitation}} of {{Human Demonstrations Based}} on a {{Diffusion Model}}},
  shorttitle = {{{DIRIGENt}}},
  author = {Spisak, Josua and Kerzel, Matthias and Wermter, Stefan},
  year = {2025},
  month = jan,
  number = {arXiv:2501.16800},
  eprint = {2501.16800},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.16800},
  urldate = {2025-02-01},
  abstract = {There has been substantial progress in humanoid robots, with new skills continuously being taught, ranging from navigation to manipulation. While these abilities may seem impressive, the teaching methods often remain inefficient. To enhance the process of teaching robots, we propose leveraging a mechanism effectively used by humans: teaching by demonstrating. In this paper, we introduce DIRIGENt (DIrect Robotic Imitation GENeration model), a novel end-to-end diffusion approach that directly generates joint values from observing human demonstrations, enabling a robot to imitate these actions without any existing mapping between it and humans. We create a dataset in which humans imitate a robot and then use this collected data to train a diffusion model that enables a robot to imitate humans. The following three aspects are the core of our contribution. First is our novel dataset with natural pairs between human and robot poses, allowing our approach to imitate humans accurately despite the gap between their anatomies. Second, the diffusion input to our model alleviates the challenge of redundant joint configurations, limiting the search space. And finally, our end-to-end architecture from perception to action leads to an improved learning capability. Through our experimental analysis, we show that combining these three aspects allows DIRIGENt to outperform existing state-of-the-art approaches in the field of generating joint values from RGB images.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/N6K8KGV9/Spisak et al. - 2025 - DIRIGENt End-To-End Robotic Imitation of Human Demonstrations Based on a Diffusion Model.pdf;/Users/fangyuan/Zotero/storage/EF23SWYW/2501.html}
}

@article{stephanGraspTrackObjectGrasp2024,
  title = {{{GraspTrack}}: {{Object}} and {{Grasp Pose Tracking}} for {{Arbitrary Objects}}},
  author = {Stephan, Benedict and Fischedick, S{\"o}hnke Benedikt and Seichter, Daniel and Aganian, Dustin and Gross, Horst-Michael},
  year = {2024},
  abstract = {A necessary skill for robots for human-robotcollaboration in Industry 4.0 scenarios is the manipulation of objects. To manipulate objects, the robot must estimate grasp poses autonomously. Object-agnostic approaches are often realized through frame-based deep-learning methods that take a single frame of information (image or point cloud) as input while discarding previous estimates, even if they were verified by execution. In addition, object-agnostic grasp estimation methods lack the ability to target specific objects in cluttered scenes. To address this, we propose GraspTrack --a class-agnostic pipeline that uses instance segmentation and object pose estimation to track estimated grasps for each object simultaneously. To evaluate our pipeline and its modules, we perform extensive experiments on the GraspNet-1Billion dataset and extend its evaluation to measure grasp quality for each object, capturing the ability to grasp targeted objects more effectively. Our experiments prove our pipeline to be robust against difficult viewing angles and occlusions, outperforming frame-based grasp pose estimation.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/GRWV4NQ6/Stephan et al. - 2024 - GraspTrack Object and Grasp Pose Tracking for Arb.pdf}
}

@article{stepputtisLanguageConditionedImitationLearning,
  title = {Language-{{Conditioned Imitation Learning}} for {{Robot Manipulation Tasks}}},
  author = {Stepputtis, Simon and Campbell, Joseph and Phielipp, Mariano and Lee, Stefan and Baral, Chitta and Amor, Heni Ben},
  pages = {12},
  abstract = {Imitation learning is a popular approach for teaching motor skills to robots. However, most approaches focus on extracting policy parameters from execution traces alone (i.e., motion trajectories and perceptual data). No adequate communication channel exists between the human expert and the robot to describe critical aspects of the task, such as the properties of the target object or the intended shape of the motion. Motivated by insights into the human teaching process, we introduce a method for incorporating unstructured natural language into imitation learning. At training time, the expert can provide demonstrations along with verbal descriptions in order to describe the underlying intent (e.g., ``go to the large green bowl''). The training process then interrelates these two modalities to encode the correlations between language, perception, and motion. The resulting language-conditioned visuomotor policies can be conditioned at runtime on new human commands and instructions, which allows for more fine-grained control over the trained policies while also reducing situational ambiguity. We demonstrate in a set of simulation experiments how our approach can learn language-conditioned manipulation policies for a seven-degree-of-freedom robot arm and compare the results to a variety of alternative methods.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/V2VNLJ5G/Stepputtis et al. - Language-Conditioned Imitation Learning for Robot .pdf}
}

@misc{sukhijaMaxInfoRLBoostingExploration2024,
  title = {{{MaxInfoRL}}: {{Boosting}} Exploration in Reinforcement Learning through Information Gain Maximization},
  shorttitle = {{{MaxInfoRL}}},
  author = {Sukhija, Bhavya and Coros, Stelian and Krause, Andreas and Abbeel, Pieter and Sferrazza, Carmelo},
  year = {2024},
  month = dec,
  number = {arXiv:2412.12098},
  eprint = {2412.12098},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.12098},
  urldate = {2024-12-18},
  abstract = {Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/6CH484WJ/Sukhija et al. - 2024 - MaxInfoRL Boosting exploration in reinforcement learning through information gain maximization.pdf;/Users/fangyuan/Zotero/storage/3H54DDII/2412.html}
}

@misc{sunEmmaXEmbodiedMultimodal2024,
  title = {Emma-{{X}}: {{An Embodied Multimodal Action Model}} with {{Grounded Chain}} of {{Thought}} and {{Look-ahead Spatial Reasoning}}},
  shorttitle = {Emma-{{X}}},
  author = {Sun, Qi and Hong, Pengfei and Pala, Tej Deep and Toh, Vernon and Tan, U.-Xuan and Ghosal, Deepanway and Poria, Soujanya},
  year = {2024},
  month = dec,
  number = {arXiv:2412.11974},
  eprint = {2412.11974},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.11974},
  urldate = {2024-12-18},
  abstract = {Traditional reinforcement learning-based robotic control methods are often task-specific and fail to generalize across diverse environments or unseen objects and instructions. Visual Language Models (VLMs) demonstrate strong scene understanding and planning capabilities but lack the ability to generate actionable policies tailored to specific robotic embodiments. To address this, Visual-Language-Action (VLA) models have emerged, yet they face challenges in long-horizon spatial reasoning and grounded task planning. In this work, we propose the Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed hierarchical embodiment dataset based on BridgeV2, containing 60,000 robot manipulation trajectories auto-annotated with grounded task reasoning and spatial guidance. Additionally, we introduce a trajectory segmentation strategy based on gripper states and motion trajectories, which can help mitigate hallucination in grounding subtask reasoning generation. Experimental results demonstrate that Emma-X achieves superior performance over competitive baselines, particularly in real-world robotic tasks requiring spatial reasoning.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/GRFIX6E6/Sun et al. - 2024 - Emma-X An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Re.pdf;/Users/fangyuan/Zotero/storage/9I55DEAI/2412.html}
}

@misc{sunFullyAutonomousRealWorld2021,
  title = {Fully {{Autonomous Real-World Reinforcement Learning}} with {{Applications}} to {{Mobile Manipulation}}},
  author = {Sun, Charles and Orbik, J{\k e}drzej and Devin, Coline and Yang, Brian and Gupta, Abhishek and Berseth, Glen and Levine, Sergey},
  year = {2021},
  month = dec,
  number = {arXiv:2107.13545},
  eprint = {2107.13545},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-10},
  abstract = {We study how robots can autonomously learn skills that require a combination of navigation and grasping. While reinforcement learning in principle provides for automated robotic skill learning, in practice reinforcement learning in the real world is challenging and often requires extensive instrumentation and supervision. Our aim is to devise a robotic reinforcement learning system for learning navigation and manipulation together, in an autonomous way without human intervention, enabling continual learning under realistic assumptions. Our proposed system, ReLMM, can learn continuously on a real-world platform without any environment instrumentation, without human intervention, and without access to privileged information, such as maps, objects positions, or a global view of the environment. Our method employs a modularized policy with components for manipulation and navigation, where manipulation policy uncertainty drives exploration for the navigation controller, and the manipulation module provides rewards for navigation. We evaluate our method on a room cleanup task, where the robot must navigate to and pick up items scattered on the floor. After a grasp curriculum training phase, ReLMM can learn navigation and grasping together fully automatically in around 40 hours of autonomous real-world training.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/9YPSPUIA/Sun et al. - 2021 - Fully Autonomous Real-World Reinforcement Learning.pdf}
}

@misc{sunInverseRLignmentLargeLanguage2025,
  title = {Inverse-{{RLignment}}: {{Large Language Model Alignment}} from {{Demonstrations}} through {{Inverse Reinforcement Learning}}},
  shorttitle = {Inverse-{{RLignment}}},
  author = {Sun, Hao and van der Schaar, Mihaela},
  year = {2025},
  month = jan,
  number = {arXiv:2405.15624},
  eprint = {2405.15624},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.15624},
  urldate = {2025-02-01},
  abstract = {Aligning Large Language Models (LLMs) is crucial for enhancing their safety and utility. However, existing methods, primarily based on preference datasets, face challenges such as noisy labels, high annotation costs, and privacy concerns. In this work, we introduce Alignment from Demonstrations (AfD), a novel approach leveraging high-quality demonstration data to overcome these challenges. We formalize AfD within a sequential decision-making framework, highlighting its unique challenge of missing reward signals. Drawing insights from forward and inverse reinforcement learning, we introduce divergence minimization objectives for AfD. Analytically, we elucidate the mass-covering and mode-seeking behaviors of various approaches, explaining when and why certain methods are superior. Practically, we propose a computationally efficient algorithm that extrapolates over a tailored reward model for AfD. We validate our key insights through experiments on the Harmless and Helpful tasks, demonstrating their strong empirical performance while maintaining simplicity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/T9CP9ZCH/Sun and Schaar - 2025 - Inverse-RLignment Large Language Model Alignment from Demonstrations through Inverse Reinforcement.pdf;/Users/fangyuan/Zotero/storage/RCFDU4E9/2405.html}
}

@article{sunRealtimeCoordinationMultiple2024,
  title = {Real-Time {{Coordination}} of {{Multiple Robotic Arms}} with {{Reactive Trajectory Modulation}}},
  author = {Sun, Da and Liao, Qianfang},
  year = {2024},
  number = {24},
  abstract = {Efficiently coordinating multiple robotic arms is vital for secure and optimal operation in a shared workspace. This requires not only successful task completion but also minimizing collision risks from overlapping movements. Introducing realtime motion modulation adds an extra layer of challenge to this coordination task. In this article, we introduce a novel framework for real-time multi-arm coordination, offering several contributions: Firstly, based on Fuzzy Model-based Movement Primitives (FMP), we propose a method for real-time trajectory modulation by learning from single demonstrations. This capability allows robots to modulate their motions online to reach arbitrary new desired places smoothly without necessitating extra demonstrations from users. Secondly, our framework incorporates a real-time multi-arm coordination strategy that seamlessly integrates the trajectory modulation method with an extended reactive approach. This strategy empowers multiple robotic arms operating within a shared workspace to dynamically regulate their movements and execute tasks in parallel in a humandesired manner while reactively avoiding mutual collisions. In the experiments, we utilize a group of robotic arms working in a shared workspace to assess the effectiveness of our framework and to make comparisons with state-of-the-art methods.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/CADUC9LY/Sun and Liao - 2024 - Real-time Coordination of Multiple Robotic Arms wi.pdf}
}

@misc{sunSPARKModularBenchmark2025,
  title = {{{SPARK}}: {{A Modular Benchmark}} for {{Humanoid Robot Safety}}},
  shorttitle = {{{SPARK}}},
  author = {Sun, Yifan and Chen, Rui and Yun, Kai S. and Fang, Yikuan and Jung, Sebin and Li, Feihan and Li, Bowei and Zhao, Weiye and Liu, Changliu},
  year = {2025},
  month = feb,
  number = {arXiv:2502.03132},
  eprint = {2502.03132},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.03132},
  urldate = {2025-02-07},
  abstract = {This paper introduces the Safe Protective and Assistive Robot Kit (SPARK), a comprehensive benchmark designed to ensure safety in humanoid autonomy and teleoperation. Humanoid robots pose significant safety risks due to their physical capabilities of interacting with complex environments. The physical structures of humanoid robots further add complexity to the design of general safety solutions. To facilitate the safe deployment of complex robot systems, SPARK can be used as a toolbox that comes with state-of-the-art safe control algorithms in a modular and composable robot control framework. Users can easily configure safety criteria and sensitivity levels to optimize the balance between safety and performance. To accelerate humanoid safety research and development, SPARK provides a simulation benchmark that compares safety approaches in a variety of environments, tasks, and robot models. Furthermore, SPARK allows quick deployment of synthesized safe controllers on real robots. For hardware deployment, SPARK supports Apple Vision Pro (AVP) or a Motion Capture System as external sensors, while also offering interfaces for seamless integration with alternative hardware setups. This paper demonstrates SPARK's capability with both simulation experiments and case studies with a Unitree G1 humanoid robot. Leveraging these advantages of SPARK, users and researchers can significantly improve the safety of their humanoid systems as well as accelerate relevant research. The open-source code is available at https://github.com/intelligent-control-lab/spark.},
  archiveprefix = {arXiv},
  keywords = {Benchmark,Safety},
  file = {/Users/fangyuan/Zotero/storage/96U3VJK2/Sun et al. - 2025 - SPARK A Modular Benchmark for Humanoid Robot Safety.pdf;/Users/fangyuan/Zotero/storage/NT2V8MFP/2502.html}
}

@article{sunSPARKToolboxSafe,
  title = {{{SPARK}}: {{A Toolbox}} for {{Safe Humanoid Autonomy}} and {{Teleoperation}}},
  author = {Sun, Yifan and Chen, Rui and Yun, Kai S and Fang, Yikuan and Jung, Sebin and Zhao, Weiye and Liu, Changliu},
  abstract = {This paper presents the Safe Protective and Assistive Robot Kit (SPARK), a comprehensive toolbox designed to ensure safety in humanoid autonomy and teleoperation. Humanoid robots have gained significant popularity due to their versatility in interacting with complex environments. It is critical to ensure that no harm is caused during those interactions since unsafe behaviors can pose serious risks to the environment and human safety, considering humanoids' physical capabilities. Therefore, robust safety measures are essential in general humanoid robotics research and deployment. The inherently complex physical structures of humanoid robots further complicate the design of task-specific safety solutions. To alleviate this challenge, we introduce SPARK, a modular toolbox that integrates state-ofthe-art safe control algorithms into a generic robot control framework. SPARK enables users to configure safety behaviors across multiple dimensions, such as defining safety criteria and sensitivity levels, allowing for optimized trade-offs between safety and performance. The toolbox is natively compatible with the Unitree G1 humanoid robot, utilizing Apple Vision Pro (AVP) for perception, while also offering interfaces for seamless customization with alternative hardware setups. By integrating SPARK as a fail-safe mechanism, users can significantly improve the safety of their existing humanoid systems.},
  langid = {english},
  keywords = {Manipulation,Teleoperation},
  file = {/Users/fangyuan/Zotero/storage/ZG5IVH6L/Sun et al. - SPARK A Toolbox for Safe Humanoid Autonomy and Teleoperation.pdf}
}

@article{suttonMDPsSemiMDPsFramework1999,
  title = {Between {{MDPs}} and Semi-{{MDPs}}: {{A}} Framework for Temporal Abstraction in Reinforcement Learning},
  shorttitle = {Between {{MDPs}} and Semi-{{MDPs}}},
  author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
  year = {1999},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {112},
  number = {1},
  pages = {181--211},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(99)00052-1},
  urldate = {2022-12-19},
  abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options---closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
  langid = {english},
  keywords = {Hierarchical planning,Intra-option learning,Macroactions,Macros,Markov decision processes,Options,Reinforcement learning,Semi-Markov decision processes,Subgoals,Temporal abstraction},
  file = {/Users/fangyuan/Zotero/storage/KSH6ZXXS/Sutton et al. - 1999 - Between MDPs and semi-MDPs A framework for tempor.pdf;/Users/fangyuan/Zotero/storage/DX6X3AY6/S0004370299000521.html}
}

@book{suttonReinforcementLearningSecond2018,
  title = {Reinforcement {{Learning}}, Second Edition: {{An Introduction}}},
  shorttitle = {Reinforcement {{Learning}}, Second Edition},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  month = nov,
  publisher = {MIT Press},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  googlebooks = {uWV0DwAAQBAJ},
  isbn = {978-0-262-35270-3},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General}
}

@misc{tagliabueDemonstrationEfficientGuidedPolicy2021,
  title = {Demonstration-{{Efficient Guided Policy Search}} via {{Imitation}} of {{Robust Tube MPC}}},
  author = {Tagliabue, Andrea and Kim, Dong-Ki and Everett, Michael and How, Jonathan P.},
  year = {2021},
  month = sep,
  number = {arXiv:2109.09910},
  eprint = {2109.09910},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-17},
  abstract = {We propose a demonstration-efficient strategy to compress a computationally expensive Model Predictive Controller (MPC) into a more computationally efficient representation based on a deep neural network and Imitation Learning (IL). By generating a Robust Tube variant (RTMPC) of the MPC and leveraging properties from the tube, we introduce a data augmentation method that enables high demonstrationefficiency, being capable to compensate the distribution shifts typically encountered in IL. Our approach opens the possibility of zero-shot transfer from a single demonstration collected in a nominal domain, such as a simulation or a robot in a lab/controlled environment, to a domain with bounded model errors/perturbations. Numerical and experimental evaluations performed on a trajectory tracking MPC for a quadrotor show that our method outperforms strategies commonly employed in IL, such as DAgger and Domain Randomization, in terms of demonstration-efficiency and robustness to perturbations unseen during training.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/MBD458UV/Tagliabue et al. - 2021 - Demonstration-Efficient Guided Policy Search via I.pdf}
}

@misc{tangPoolingAttentionWhat2024,
  title = {Pooling {{And Attention}}: {{What Are Effective Designs For LLM-Based Embedding Models}}?},
  shorttitle = {Pooling {{And Attention}}},
  author = {Tang, Yixuan and Yang, Yi},
  year = {2024},
  month = sep,
  number = {arXiv:2409.02727},
  eprint = {2409.02727},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.02727},
  urldate = {2025-04-24},
  abstract = {The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/fangyuan/Zotero/storage/FNS6KF8L/Tang and Yang - 2024 - Pooling And Attention What Are Effective Designs For LLM-Based Embedding Models.pdf;/Users/fangyuan/Zotero/storage/L3646DUV/2409.html}
}

@misc{taoManiSkill3GPUParallelized2024,
  title = {{{ManiSkill3}}: {{GPU Parallelized Robotics Simulation}} and {{Rendering}} for {{Generalizable Embodied AI}}},
  shorttitle = {{{ManiSkill3}}},
  author = {Tao, Stone and Xiang, Fanbo and Shukla, Arth and Qin, Yuzhe and Hinrichsen, Xander and Yuan, Xiaodi and Bao, Chen and Lin, Xinsong and Liu, Yulin and Chan, Tse-kai and Gao, Yuan and Li, Xuanlin and Mu, Tongzhou and Xiao, Nan and Gurha, Arnav and Huang, Zhiao and Calandra, Roberto and Chen, Rui and Luo, Shan and Su, Hao},
  year = {2024},
  month = oct,
  number = {arXiv:2410.00425},
  eprint = {2410.00425},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.00425},
  urldate = {2024-12-06},
  abstract = {Simulation has enabled unprecedented compute-scalable approaches to robot learning. However, many existing simulation frameworks typically support a narrow range of scenes/tasks and lack features critical for scaling generalizable robotics and sim2real. We introduce and open source ManiSkill3, the fastest state-visual GPU parallelized robotics simulator with contact-rich physics targeting generalizable manipulation. ManiSkill3 supports GPU parallelization of many aspects including simulation+rendering, heterogeneous simulation, pointclouds/voxels visual input, and more. Simulation with rendering on ManiSkill3 can run 10-1000x faster with 2-3x less GPU memory usage than other platforms, achieving up to 30,000+ FPS in benchmarked environments due to minimal python/pytorch overhead in the system, simulation on the GPU, and the use of the SAPIEN parallel rendering system. Tasks that used to take hours to train can now take minutes. We further provide the most comprehensive range of GPU parallelized environments/tasks spanning 12 distinct domains including but not limited to mobile manipulation for tasks such as drawing, humanoids, and dextrous manipulation in realistic scenes designed by artists or real-world digital twins. In addition, millions of demonstration frames are provided from motion planning, RL, and teleoperation. ManiSkill3 also provides a comprehensive set of baselines that span popular RL and learning-from-demonstrations algorithms.},
  archiveprefix = {arXiv},
  keywords = {Benchmark},
  file = {/Users/fangyuan/Zotero/storage/XMGJHLSF/Tao et al. - 2024 - ManiSkill3 GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI.pdf;/Users/fangyuan/Zotero/storage/T27EQ238/2410.html}
}

@misc{taouilPhysicallyConsistentHumanoid2025,
  title = {Physically {{Consistent Humanoid Loco-Manipulation}} Using {{Latent Diffusion Models}}},
  author = {Taouil, Ilyass and Zhao, Haizhou and Dai, Angela and Khadiv, Majid},
  year = {2025},
  month = apr,
  number = {arXiv:2504.16843},
  eprint = {2504.16843},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16843},
  urldate = {2025-04-25},
  abstract = {This paper uses the capabilities of latent diffusion models (LDMs) to generate realistic RGB human-object interaction scenes to guide humanoid loco-manipulation planning. To do so, we extract from the generated images both the contact locations and robot configurations that are then used inside a whole-body trajectory optimization (TO) formulation to generate physically consistent trajectories for humanoids. We validate our full pipeline in simulation for different long-horizon loco-manipulation scenarios and perform an extensive analysis of the proposed contact and robot configuration extraction pipeline. Our results show that using the information extracted from LDMs, we can generate physically consistent trajectories that require long-horizon reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Graphics,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/Z2Y8TMNX/Taouil et al. - 2025 - Physically Consistent Humanoid Loco-Manipulation using Latent Diffusion Models.pdf;/Users/fangyuan/Zotero/storage/3LF2I3DA/2504.html}
}

@misc{tirinzoniZeroShotWholeBodyHumanoid2025,
  title = {Zero-{{Shot Whole-Body Humanoid Control}} via {{Behavioral Foundation Models}}},
  author = {Tirinzoni, Andrea and Touati, Ahmed and Farebrother, Jesse and Guzek, Mateusz and Kanervisto, Anssi and Xu, Yingchen and Lazaric, Alessandro and Pirotta, Matteo},
  year = {2025},
  month = apr,
  number = {arXiv:2504.11054},
  eprint = {2504.11054},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.11054},
  urldate = {2025-04-18},
  abstract = {Unsupervised reinforcement learning (RL) aims at pre-training agents that can solve a wide range of downstream tasks in complex environments. Despite recent advancements, existing approaches suffer from several limitations: they may require running an RL process on each downstream task to achieve a satisfactory performance, they may need access to datasets with good coverage or well-curated task-specific samples, or they may pre-train policies with unsupervised losses that are poorly correlated with the downstream tasks of interest. In this paper, we introduce a novel algorithm regularizing unsupervised RL towards imitating trajectories from unlabeled behavior datasets. The key technical novelty of our method, called Forward-Backward Representations with Conditional-Policy Regularization, is to train forward-backward representations to embed the unlabeled trajectories to the same latent space used to represent states, rewards, and policies, and use a latent-conditional discriminator to encourage policies to ``cover'' the states in the unlabeled behavior dataset. As a result, we can learn policies that are well aligned with the behaviors in the dataset, while retaining zero-shot generalization capabilities for reward-based and imitation tasks. We demonstrate the effectiveness of this new approach in a challenging humanoid control problem: leveraging observation-only motion capture datasets, we train Meta Motivo, the first humanoid behavioral foundation model that can be prompted to solve a variety of whole-body tasks, including motion tracking, goal reaching, and reward optimization. The resulting model is capable of expressing human-like behaviors and it achieves competitive performance with task-specific methods while outperforming state-of-the-art unsupervised RL and model-based baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/GGA3LD7C/Tirinzoni et al. - 2025 - Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models.pdf;/Users/fangyuan/Zotero/storage/6ZU2E7E5/2504.html}
}

@misc{torabiBehavioralCloningObservation2018,
  title = {Behavioral {{Cloning}} from {{Observation}}},
  author = {Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  year = {2018},
  month = may,
  number = {arXiv:1805.01954},
  eprint = {1805.01954},
  publisher = {arXiv},
  urldate = {2024-10-31},
  abstract = {Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations. While extending this paradigm to autonomous agents is a well-studied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly. In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects. First, we allow the agent to acquire experience in a self-supervised fashion. This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken. We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/S25PH8A7/Torabi et al. - 2018 - Behavioral Cloning from Observation.pdf;/Users/fangyuan/Zotero/storage/3CRUD2QZ/1805.html}
}

@misc{torneBreadcrumbsGoalGoalConditioned2023,
  title = {Breadcrumbs to the {{Goal}}: {{Goal-Conditioned Exploration}} from {{Human-in-the-Loop Feedback}}},
  shorttitle = {Breadcrumbs to the {{Goal}}},
  author = {Torne, Marcel and Balsells, Max and Wang, Zihan and Desai, Samedh and Chen, Tao and Agrawal, Pulkit and Gupta, Abhishek},
  year = {2023},
  month = jul,
  number = {arXiv:2307.11049},
  eprint = {2307.11049},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-01},
  abstract = {Exploration and reward specification are fundamental and intertwined challenges for reinforcement learning. Solving sequential decision-making tasks requiring expansive exploration requires either careful design of reward functions or the use of novelty-seeking exploration bonuses. Human supervisors can provide effective guidance in the loop to direct the exploration process, but prior methods to leverage this guidance require constant synchronous high-quality human feedback, which is expensive and impractical to obtain. In this work, we present a technique called Human Guided Exploration (HuGE), which uses low-quality feedback from non-expert users that may be sporadic, asynchronous, and noisy. HuGE guides exploration for reinforcement learning not only in simulation but also in the real world, all without meticulous reward specification. The key concept involves bifurcating human feedback and policy learning: human feedback steers exploration, while self-supervised learning from the exploration data yields unbiased policies. This procedure can leverage noisy, asynchronous human feedback to learn policies with no hand-crafted reward design or exploration bonuses. HuGE is able to learn a variety of challenging multi-stage robotic navigation and manipulation tasks in simulation using crowdsourced feedback from non-expert users. Moreover, this paradigm can be scaled to learning directly on real-world robots, using occasional, asynchronous feedback from human supervisors.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/45BPKQ4V/Torne et al. - 2023 - Breadcrumbs to the Goal Goal-Conditioned Explorat.pdf;/Users/fangyuan/Zotero/storage/T4NCJIFR/2307.html}
}

@misc{ueharaRewardGuidedControlledGeneration2025,
  title = {Reward-{{Guided Controlled Generation}} for {{Inference-Time Alignment}} in {{Diffusion Models}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reward-{{Guided Controlled Generation}} for {{Inference-Time Alignment}} in {{Diffusion Models}}},
  author = {Uehara, Masatoshi and Zhao, Yulai and Wang, Chenyu and Li, Xiner and Regev, Aviv and Levine, Sergey and Biancalani, Tommaso},
  year = {2025},
  month = jan,
  number = {arXiv:2501.09685},
  eprint = {2501.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.09685},
  urldate = {2025-01-17},
  abstract = {This tutorial provides an in-depth guide on inference-time guidance and alignment methods for optimizing downstream reward functions in diffusion models. While diffusion models are renowned for their generative modeling capabilities, practical applications in fields such as biology often require sample generation that maximizes specific metrics (e.g., stability, affinity in proteins, closeness to target structures). In these scenarios, diffusion models can be adapted not only to generate realistic samples but also to explicitly maximize desired measures at inference time without fine-tuning. This tutorial explores the foundational aspects of such inference-time algorithms. We review these methods from a unified perspective, demonstrating that current techniques -- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling, and classifier guidance -- aim to approximate soft optimal denoising processes (a.k.a. policies in RL) that combine pre-trained denoising processes with value functions serving as look-ahead functions that predict from intermediate states to terminal rewards. Within this framework, we present several novel algorithms not yet covered in the literature. Furthermore, we discuss (1) fine-tuning methods combined with inference-time techniques, (2) inference-time algorithms based on search algorithms such as Monte Carlo tree search, which have received limited attention in current research, and (3) connections between inference-time algorithms in language models and diffusion models. The code of this tutorial on protein design is available at https://github.com/masa-ue/AlignInversePro},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/GPBNYVBR/Uehara et al. - 2025 - Reward-Guided Controlled Generation for Inference-Time Alignment in Diffusion Models Tutorial and R.pdf;/Users/fangyuan/Zotero/storage/FA7BJZ2M/2501.html}
}

@misc{vuongDualGeneratorOffline2022,
  title = {Dual {{Generator Offline Reinforcement Learning}}},
  author = {Vuong, Quan and Kumar, Aviral and Levine, Sergey and Chebotar, Yevgen},
  year = {2022},
  month = nov,
  number = {arXiv:2211.01471},
  eprint = {2211.01471},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.01471},
  urldate = {2022-11-05},
  abstract = {In offline RL, constraining the learned policy to remain close to the data is essential to prevent the policy from outputting out-of-distribution (OOD) actions with erroneously overestimated values. In principle, generative adversarial networks (GAN) can provide an elegant solution to do so, with the discriminator directly providing a probability that quantifies distributional shift. However, in practice, GAN-based offline RL methods have not performed as well as alternative approaches, perhaps because the generator is trained to both fool the discriminator and maximize return -- two objectives that can be at odds with each other. In this paper, we show that the issue of conflicting objectives can be resolved by training two generators: one that maximizes return, with the other capturing the ``remainder'' of the data distribution in the offline dataset, such that the mixture of the two is close to the behavior policy. We show that not only does having two generators enable an effective GAN-based offline RL method, but also approximates a support constraint, where the policy does not need to match the entire data distribution, but only the slice of the data that leads to high long term performance. We name our method DASCO, for Dual-Generator Adversarial Support Constrained Offline RL. On benchmark tasks that require learning from sub-optimal data, DASCO significantly outperforms prior methods that enforce distribution constraint.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/XMEPRG6M/Vuong et al. - 2022 - Dual Generator Offline Reinforcement Learning.pdf;/Users/fangyuan/Zotero/storage/X4W5YQVP/2211.html}
}

@misc{wangGraphToolInstructionRevolutionizingGraph2024,
  title = {{{GraphTool-Instruction}}: {{Revolutionizing Graph Reasoning}} in {{LLMs}} through {{Decomposed Subtask Instruction}}},
  shorttitle = {{{GraphTool-Instruction}}},
  author = {Wang, Rongzheng and Liang, Shuang and Chen, Qizhi and Zhang, Jiasheng and Qin, Ke},
  year = {2024},
  month = dec,
  number = {arXiv:2412.12152},
  eprint = {2412.12152},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.12152},
  urldate = {2024-12-18},
  abstract = {Large language models (LLMs) have been demonstrated to possess the capabilities to understand fundamental graph properties and address various graph reasoning tasks. Existing methods fine-tune LLMs to understand and execute graph reasoning tasks by specially designed task instructions. However, these Text-Instruction methods generally exhibit poor performance. Inspired by tool learning, researchers propose Tool-Instruction methods to solve various graph problems by special tool calling (e.g., function, API and model), achieving significant improvements in graph reasoning tasks. Nevertheless, current Tool-Instruction approaches focus on the tool information and ignore the graph structure information, which leads to significantly inferior performance on small-scale LLMs (less than 13B). To tackle this issue, we propose GraphTool-Instruction, an innovative Instruction-tuning approach that decomposes the graph reasoning task into three distinct subtasks (i.e., graph extraction, tool name identification and tool parameter extraction), and design specialized instructions for each subtask. Our GraphTool-Instruction can be used as a plug-and-play prompt for different LLMs without fine-tuning. Moreover, building on GraphTool-Instruction, we develop GTools, a dataset that includes twenty graph reasoning tasks, and create a graph reasoning LLM called GraphForge based on Llama3-8B. We conduct extensive experiments on twenty graph reasoning tasks with different graph types (e.g., graph size or graph direction), and we find that GraphTool-Instruction achieves SOTA compared to Text-Instruction and Tool-Instruction methods. Fine-tuned on GTools, GraphForge gets further improvement of over 30\% compared to the Tool-Instruction enhanced GPT-3.5-turbo, and it performs comparably to the high-cost GPT-4o. Our codes and data are available at https://anonymous.4open.science/r/GraphTool-Instruction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/93F4PXI5/Wang et al. - 2024 - GraphTool-Instruction Revolutionizing Graph Reasoning in LLMs through Decomposed Subtask Instructio.pdf;/Users/fangyuan/Zotero/storage/8WTAT2JN/2412.html}
}

@article{wangLearningLongHorizonSparseReward2022,
  title = {Learning of {{Long-Horizon Sparse-Reward Robotic Manipulator Tasks With Base Controllers}}},
  author = {Wang, Guangming and Xin, Minjian and Wu, Wenhua and Liu, Zhe and Wang, Hesheng},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--10},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2022.3201705},
  abstract = {Deep reinforcement learning (DRL) enables robots to perform some intelligent tasks end-to-end. However, there are still many challenges for long-horizon sparse-reward robotic manipulator tasks. On the one hand, a sparse-reward setting causes exploration inefficient. On the other hand, exploration using physical robots is of high cost and unsafe. In this article, we propose a method of learning long-horizon sparse-reward tasks utilizing one or more existing traditional controllers named base controllers in this article. Built upon deep deterministic policy gradients (DDPGs), our algorithm incorporates the existing base controllers into stages of exploration, value learning, and policy update. Furthermore, we present a straightforward way of synthesizing different base controllers to integrate their strengths. Through experiments ranging from stacking blocks to cups, it is demonstrated that the learned state-based or image-based policies steadily outperform base controllers. Compared to previous works of learning from demonstrations, our method improves sample efficiency by orders of magnitude and improves performance. Overall, our method bears the potential of leveraging existing industrial robot manipulation systems to build more flexible and intelligent controllers.},
  keywords = {Base controllers,Costs,deep reinforcement learning (DRL),Distance measurement,long-horizon sparse reward,Manipulators,robotic manipulator tasks,Robots,Task analysis,Training,Uncertainty},
  file = {/Users/fangyuan/Zotero/storage/4G572KWM/Wang et al. - 2022 - Learning of Long-Horizon Sparse-Reward Robotic Man.pdf;/Users/fangyuan/Zotero/storage/JTM7NLTT/stamp.html}
}

@misc{wangPoseDiffusionSolvingPose2023,
  title = {{{PoseDiffusion}}: {{Solving Pose Estimation}} via {{Diffusion-aided Bundle Adjustment}}},
  shorttitle = {{{PoseDiffusion}}},
  author = {Wang, Jianyuan and Rupprecht, Christian and Novotny, David},
  year = {2023},
  month = jun,
  number = {arXiv:2306.15667},
  eprint = {2306.15667},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-10},
  abstract = {Camera pose estimation is a long-standing computer vision problem that to date often relies on classical methods, such as handcrafted keypoint matching, RANSAC and bundle adjustment. In this paper, we propose to formulate the Structure from Motion (SfM) problem inside a probabilistic diffusion framework, modelling the conditional distribution of camera poses given input images. This novel view of an old problem has several advantages. (i) The nature of the diffusion framework mirrors the iterative procedure of bundle adjustment. (ii) The formulation allows a seamless integration of geometric constraints from epipolar geometry. (iii) It excels in typically difficult scenarios such as sparse views with wide baselines. (iv) The method can predict intrinsics and extrinsics for an arbitrary amount of images. We demonstrate that our method PoseDiffusion significantly improves over the classic SfM pipelines and the learned approaches on two real-world datasets. Finally, it is observed that our method can generalize across datasets without further training. Project page: https://posediffusion.github.io/},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/NQQ3QZTY/Wang et al. - 2023 - PoseDiffusion Solving Pose Estimation via Diffusi.pdf}
}

@misc{wangScalingProprioceptiveVisualLearning2024,
  title = {Scaling {{Proprioceptive-Visual Learning}} with {{Heterogeneous Pre-trained Transformers}}},
  author = {Wang, Lirui and Chen, Xinlei and Zhao, Jialiang and He, Kaiming},
  year = {2024},
  month = sep,
  number = {arXiv:2409.20537},
  eprint = {2409.20537},
  publisher = {arXiv},
  urldate = {2024-10-14},
  abstract = {One of the roadblocks for training generalist robotic models today is heterogeneity. Previous robot learning methods often collect data to train with one specific embodiment for one task, which is expensive and prone to overfitting. This work studies the problem of learning policy representations through heterogeneous pre-training on robot data across different embodiments and tasks at scale. We propose Heterogeneous Pre-trained Transformers (HPT), which pre-train a large, shareable trunk of a policy neural network to learn a task and embodiment agnostic shared representation. This general architecture aligns the specific proprioception and vision inputs from distinct embodiments to a short sequence of tokens and then processes such tokens to map to control robots for different tasks. Leveraging the recent large-scale multi-embodiment real-world robotic datasets as well as simulation, deployed robots, and human video datasets, we investigate pre-training policies across heterogeneity. We conduct experiments to investigate the scaling behaviors of training objectives, to the extent of 52 datasets. HPTs outperform several baselines and enhance the fine-tuned policy performance by over 20\% on unseen tasks in multiple simulator benchmarks and real-world settings. See the project website (https://liruiw.github.io/hpt/) for code and videos.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,Robotic Foundation Model},
  file = {/Users/fangyuan/Zotero/storage/3UYQKZVR/Wang et al. - 2024 - Scaling Proprioceptive-Visual Learning with Heterogeneous Pre-trained Transformers.pdf;/Users/fangyuan/Zotero/storage/U2WXBXMQ/2409.html}
}

@misc{wangSelfConsistencyImprovesChain2023,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  year = {2023},
  month = mar,
  number = {arXiv:2203.11171},
  eprint = {2203.11171},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.11171},
  urldate = {2023-11-22},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/SYFJ77M2/Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf;/Users/fangyuan/Zotero/storage/R9FG4Z8S/2203.html}
}

@article{wangTaskDrivenReinforcementLearning2023,
  title = {Task-{{Driven Reinforcement Learning With Action Primitives}} for {{Long-Horizon Manipulation Skills}}},
  author = {Wang, Hao and Zhang, Hao and Li, Lin and Kan, Zhen and Song, Yongduan},
  year = {2023},
  journal = {IEEE Transactions on Cybernetics},
  pages = {1--14},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2023.3298195},
  abstract = {It is an interesting open problem to enable robots to efficiently and effectively learn long-horizon manipulation skills. Motivated to augment robot learning via more effective exploration, this work develops task-driven reinforcement learning with action primitives (TRAPs), a new manipulation skill learning framework that augments standard reinforcement learning algorithms with formal methods and parameterized action space (PAS). In particular, TRAPs uses linear temporal logic (LTL) to specify complex manipulation skills. LTL progression, a semantics-preserving rewriting operation, is then used to decompose the training task at an abstract level, informs the robot about their current task progress, and guides them via reward functions. The PAS, a predefined library of heterogeneous action primitives, further improves the efficiency of robot exploration. We highlight that TRAPs augments the learning of manipulation skills in both learning efficiency and effectiveness (i.e., task constraints). Extensive empirical studies demonstrate that TRAPs outperforms most existing methods.Sign},
  keywords = {Action primitives,Libraries,linear temporal logic (LTL),long-horizon manipulation skills,Planning,Reinforcement learning,Robot learning,Task analysis,task-driven RL,Training,Transformers},
  file = {/Users/fangyuan/Zotero/storage/M2HENNTE/Wang et al. - 2023 - Task-Driven Reinforcement Learning With Action Pri.pdf;/Users/fangyuan/Zotero/storage/2BP3YM8N/10215053.html}
}

@misc{wangTooManyCooks2020,
  title = {Too Many Cooks: {{Bayesian}} Inference for Coordinating Multi-Agent Collaboration},
  shorttitle = {Too Many Cooks},
  author = {Wang, Rose E. and Wu, Sarah A. and Evans, James A. and Tenenbaum, Joshua B. and Parkes, David C. and {Kleiman-Weiner}, Max},
  year = {2020},
  month = jul,
  number = {arXiv:2003.11778},
  eprint = {2003.11778},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-29},
  abstract = {Collaboration requires agents to coordinate their behavior on the fly, sometimes cooperating to solve a single task together and other times dividing it up into sub-tasks to work on in parallel. Underlying the human ability to collaborate is theory-of-mind, the ability to infer the hidden mental states that drive others to act. Here, we develop Bayesian Delegation, a decentralized multi-agent learning mechanism with these abilities. Bayesian Delegation enables agents to rapidly infer the hidden intentions of others by inverse planning. We test Bayesian Delegation in a suite of multi-agent Markov decision processes inspired by cooking problems. On these tasks, agents with Bayesian Delegation coordinate both their high-level plans (e.g. what sub-task they should work on) and their low-level actions (e.g. avoiding getting in each other's way). In a self-play evaluation, Bayesian Delegation outperforms alternative algorithms. Bayesian Delegation is also a capable ad-hoc collaborator and successfully coordinates with other agent types even in the absence of prior experience. Finally, in a behavioral experiment, we show that Bayesian Delegation makes inferences similar to human observers about the intent of others. Together, these results demonstrate the power of Bayesian Delegation for decentralized multi-agent collaboration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Multiagent Systems},
  file = {/Users/fangyuan/Zotero/storage/U4RN2INW/Wang et al. - 2020 - Too many cooks Bayesian inference for coordinatin.pdf}
}

@misc{warde-farleyUnsupervisedControlNonParametric2018,
  title = {Unsupervised {{Control Through Non-Parametric Discriminative Rewards}}},
  author = {{Warde-Farley}, David and {Van de Wiele}, Tom and Kulkarni, Tejas and Ionescu, Catalin and Hansen, Steven and Mnih, Volodymyr},
  year = {2018},
  month = nov,
  number = {arXiv:1811.11359},
  eprint = {1811.11359},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2022-11-18},
  abstract = {Learning to control an environment without hand-crafted rewards or expert data remains challenging and is at the frontier of reinforcement learning research. We present an unsupervised learning algorithm to train agents to achieve perceptuallyspecified goals using only a stream of observations and actions. Our agent simultaneously learns a goal-conditioned policy and a goal achievement reward function that measures how similar a state is to the goal state. This dual optimization leads to a co-operative game, giving rise to a learned reward function that reflects similarity in controllable aspects of the environment instead of distance in the space of observations. We demonstrate the efficacy of our agent to learn, in an unsupervised manner, to reach a diverse set of goals on three domains -- Atari, the DeepMind Control Suite and DeepMind Lab.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/NS6FDMV7/Warde-Farley et al. - 2018 - Unsupervised Control Through Non-Parametric Discri.pdf}
}

@misc{weiChainofThoughtPromptingElicits2023,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2201.11903},
  urldate = {2023-11-22},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/JE2DFUG9/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf;/Users/fangyuan/Zotero/storage/PY3NGRQY/2201.html}
}

@inproceedings{weiEmbeddedFeatureSelection2022,
  title = {An {{Embedded Feature Selection Framework}} for {{Control}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Wei, Jiawen and Wang, Fangyuan and Zeng, Wanxin and Lin, Wenwei and Gui, Ning},
  year = {2022},
  month = aug,
  eprint = {2206.11064},
  primaryclass = {cs},
  pages = {1979--1988},
  doi = {10.1145/3534678.3539290},
  urldate = {2023-12-17},
  abstract = {Reducing sensor requirements while keeping optimal control performance is crucial to many industrial control applications to achieve robust, low-cost, and computation-efficient controllers. However, existing feature selection solutions for the typical machine learning domain can hardly be applied in the domain of control with changing dynamics. In this paper, a novel framework, namely the Dual-world embedded Attentive Feature Selection (D-AFS), can efficiently select the most relevant sensors for the system under dynamic control. Rather than the one world used in most Deep Reinforcement Learning (DRL) algorithms, D-AFS has both the real world and its virtual peer with twisted features. By analyzing the DRL's response in two worlds, D-AFS can quantitatively identify respective features' importance towards control. A well-known active flow control problem, cylinder drag reduction, is used for evaluation. Results show that D-AFS successfully finds an optimized five-probes layout with 18.7{\textbackslash}\% drag reduction than the state-of-the-art solution with 151 probes and 49.2{\textbackslash}\% reduction than five-probes layout by human experts. We also apply this solution to four OpenAI classical control cases. In all cases, D-AFS achieves the same or better sensor configurations than originally provided solutions. Results highlight, we argued, a new way to achieve efficient and optimal sensor designs for experimental or industrial systems. Our source codes are made publicly available at https://github.com/G-AILab/DAFSFluid.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/UBWWSDCV/Wei et al. - 2022 - An Embedded Feature Selection Framework for Contro.pdf;/Users/fangyuan/Zotero/storage/EKI6UZ6D/2206.html}
}

@article{weiUnderstandingExplorationDiscovery2022,
  title = {Understanding via {{Exploration}}: {{Discovery}} of {{Interpretable Features With Deep Reinforcement Learning}}},
  shorttitle = {Understanding via {{Exploration}}},
  author = {Wei, Jiawen and Qiu, Zhifeng and Wang, Fangyuan and Lin, Wenwei and Gui, Ning and Gui, Weihua},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--12},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2022.3184956},
  urldate = {2023-12-17},
  abstract = {Understanding the environments through interactions has been one of the most important human intellectual activities in mastering unknown systems. Deep reinforcement learning (DRL) has already been known to achieve effective control through human-like exploration and exploitation in many applications. However, the opaque nature of deep neural network (DNN) often hides critical information about feature relevance to control, which is essential for understanding the target systems. In this article, a novel online feature selection framework, namely, the dual-world-based attentive feature selection (D-AFS), is first proposed to identify the contribution of the inputs over the whole control process. Rather than the one world used in most DRL, D-AFS has both the real world and its virtual peer with twisted features. The newly introduced attention-based evaluation (AR) module performs the dynamic mapping from the real world to the virtual world. The existing DRL algorithms, with slight modification, can learn in the dual world. By analyzing the DRL's response in the two worlds, D-AFS can quantitatively identify respective features' importance toward control. A set of experiments is performed on four classical control systems in OpenAI Gym. Results show that D-AFS can generate the same or even better feature combinations than the solutions provided by human experts and seven recent feature selection baselines. In all cases, the selected feature representations are closely correlated with the ones used by underlying system dynamic models.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/623K6CWY/Wei et al. - 2022 - Understanding via Exploration Discovery of Interp.pdf}
}

@misc{wenDexVLAVisionLanguageModel2025,
  title = {{{DexVLA}}: {{Vision-Language Model}} with {{Plug-In Diffusion Expert}} for {{General Robot Control}}},
  shorttitle = {{{DexVLA}}},
  author = {Wen, Junjie and Zhu, Yichen and Li, Jinming and Tang, Zhibin and Shen, Chaomin and Feng, Feifei},
  year = {2025},
  month = feb,
  number = {arXiv:2502.05855},
  eprint = {2502.05855},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.05855},
  urldate = {2025-02-14},
  abstract = {Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert that is separable from the VLA on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks. We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like Octo, OpenVLA, and Diffusion Policy.},
  archiveprefix = {arXiv},
  keywords = {Diffusion,Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/5E4FCZVM/Wen et al. - 2025 - DexVLA Vision-Language Model with Plug-In Diffusion Expert for General Robot Control.pdf;/Users/fangyuan/Zotero/storage/74WIAUAZ/2502.html}
}

@misc{wenDiffusionVLAScalingRobot2024,
  title = {Diffusion-{{VLA}}: {{Scaling Robot Foundation Models}} via {{Unified Diffusion}} and {{Autoregression}}},
  shorttitle = {Diffusion-{{VLA}}},
  author = {Wen, Junjie and Zhu, Minjie and Zhu, Yichen and Tang, Zhibin and Li, Jinming and Zhou, Zhongyi and Li, Chengmeng and Liu, Xiaoyu and Peng, Yaxin and Shen, Chaomin and Feng, Feifei},
  year = {2024},
  month = dec,
  number = {arXiv:2412.03293},
  eprint = {2412.03293},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.03293},
  urldate = {2024-12-18},
  abstract = {In this paper, we present DiffusionVLA, a novel framework that seamlessly combines the autoregression model with the diffusion model for learning visuomotor policy. Central to our approach is a next-token prediction objective, enabling the model to reason effectively over the user's query in the context of current observations. Subsequently, a diffusion model is attached to generate robust action outputs. To enhance policy learning through self-reasoning, we introduce a novel reasoning injection module that integrates reasoning phrases directly into the policy learning process. The whole framework is simple and flexible, making it easy to deploy and upgrade. We conduct extensive experiments using multiple real robots to validate the effectiveness of DiffusionVLA. Our tests include a challenging factory sorting task, where DiffusionVLA successfully categorizes objects, including those not seen during training. We observe that the reasoning module makes the model interpretable. It allows observers to understand the model thought process and identify potential causes of policy failures. Additionally, we test DiffusionVLA on a zero-shot bin-picking task, achieving 63.7{\textbackslash}\% accuracy on 102 previously unseen objects. Our method demonstrates robustness to visual changes, such as distractors and new backgrounds, and easily adapts to new embodiments. Furthermore, DiffusionVLA can follow novel instructions and retain conversational ability. Notably, DiffusionVLA is data-efficient and fast at inference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can train from scratch on less than 50 demonstrations for a complex task. Finally, we scale the model from 2B to 72B parameters, showcasing improved generalization capabilities with increased model size.},
  archiveprefix = {arXiv},
  keywords = {Diffusion,Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/SHB7I3IL/Wen et al. - 2024 - Diffusion-VLA Scaling Robot Foundation Models via Unified Diffusion and Autoregression.pdf;/Users/fangyuan/Zotero/storage/T3HBD8WV/2412.html}
}

@misc{wenTinyVLAFastDataEfficient2024,
  title = {{{TinyVLA}}: {{Towards Fast}}, {{Data-Efficient Vision-Language-Action Models}} for {{Robotic Manipulation}}},
  shorttitle = {{{TinyVLA}}},
  author = {Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and Feng, Feifei and Tang, Jian},
  year = {2024},
  month = sep,
  number = {arXiv:2409.12514},
  eprint = {2409.12514},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-20},
  abstract = {Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that TinyVLA offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/93XY4GJR/Wen et al. - 2024 - TinyVLA Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation.pdf}
}

@article{wiegleyLedgerCommandLineAccounting,
  title = {Ledger: {{Command-Line Accounting}}},
  author = {Wiegley, John},
  pages = {142},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/W73E8WUD/Wiegley - Ledger Command-Line Accounting.pdf}
}

@misc{wuDexterousFunctionalPreGrasp2024,
  title = {Dexterous {{Functional Pre-Grasp Manipulation}} with {{Diffusion Policy}}},
  author = {Wu, Tianhao and Gan, Yunchong and Wu, Mingdong and Cheng, Jingbo and Yang, Yaodong and Zhu, Yixin and Dong, Hao},
  year = {2024},
  month = may,
  number = {arXiv:2403.12421},
  eprint = {2403.12421},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.12421},
  urldate = {2024-05-20},
  abstract = {In real-world scenarios, objects often require repositioning and reorientation before they can be grasped, a process known as pre-grasp manipulation. Learning universal dexterous functional pre-grasp manipulation requires precise control over the relative position, orientation, and contact between the hand and object while generalizing to diverse dynamic scenarios with varying objects and goal poses. To address this challenge, we propose a teacher-student learning approach that utilizes a novel mutual reward, incentivizing agents to optimize three key criteria jointly. Additionally, we introduce a pipeline that employs a mixture-of-experts strategy to learn diverse manipulation policies, followed by a diffusion policy to capture complex action distributions from these experts. Our method achieves a success rate of 72.6{\textbackslash}\% across more than 30 object categories by leveraging extrinsic dexterity and adjusting from feedback.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/JT5PP99D/Wu et al. - 2024 - Dexterous Functional Pre-Grasp Manipulation with D.pdf;/Users/fangyuan/Zotero/storage/M95QGJJK/2403.html}
}

@misc{wuGoalExplorationAugmentation2022,
  title = {Goal {{Exploration Augmentation}} via {{Pre-trained Skills}} for {{Sparse-Reward Long-Horizon Goal-Conditioned Reinforcement Learning}}},
  author = {Wu, Lisheng and Chen, Ke},
  year = {2022},
  month = oct,
  number = {arXiv:2210.16058},
  eprint = {2210.16058},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-10-31},
  abstract = {Reinforcement learning (RL) often struggles to accomplish a sparse-reward long-horizon task in a complex environment. Goal-conditioned reinforcement learning (GCRL) has been employed to tackle this difficult problem via a curriculum of easy-to-reach sub-goals. In GCRL, exploring novel sub-goals is essential for the agent to ultimately find the pathway to the desired goal. How to explore novel sub-goals efficiently is one of the most challenging issues in GCRL. Several goal exploration methods have been proposed to address this issue but still struggle to find the desired goals efficiently. In this paper, we propose a novel learning objective by optimizing the entropy of both achieved and new goals to be explored for more efficient goal exploration in sub-goal selection based GCRL. To optimize this objective, we first explore and exploit the frequently occurring goal-transition patterns mined in the environments similar to the current task to compose skills via skill learning. Then, the pretrained skills are applied in goal exploration. Evaluation on a variety of spare-reward long-horizon benchmark tasks suggests that incorporating our method into several state-of-the-art GCRL baselines significantly boosts their exploration efficiency while improving or maintaining their performance.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/7JNRXEYB/Wu and Chen - 2022 - Goal Exploration Augmentation via Pre-trained Skil.pdf}
}

@misc{wuPairwiseProximalPolicy2023,
  title = {Pairwise {{Proximal Policy Optimization}}: {{Harnessing Relative Feedback}} for {{LLM Alignment}}},
  shorttitle = {Pairwise {{Proximal Policy Optimization}}},
  author = {Wu, Tianhao and Zhu, Banghua and Zhang, Ruoyu and Wen, Zhaojin and Ramchandran, Kannan and Jiao, Jiantao},
  year = {2023},
  month = oct,
  number = {arXiv:2310.00212},
  eprint = {2310.00212},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-20},
  abstract = {Large Language Models (LLMs) can acquire extensive world knowledge through pre-training on large corpora. However, due to exposure to low-quality data, LLMs may exhibit harmful behavior without aligning with human values. The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO has limitations when optimizing rewards trained from comparison-based loss. Primarily, PPO is not invariant to equivalent reward functions containing identical preference information due to the need to calibrate the reward scale. Additionally, PPO's necessity for token-wise updates introduces complexity in both function approximation and algorithm design compared to trajectory-wise optimization. This paper proposes a new framework, reinforcement learning with relative feedback, and a novel trajectory-wise policy gradient algorithm, Pairwise Proximal Policy Optimization (P3O) that operates directly on comparative rewards. We show theoretically that P3O is invariant to equivalent rewards and avoids the complexity of PPO. Empirical evaluations demonstrate that P3O outperforms PPO in the KL-Reward trade-off and can align with human preferences as well as or better than prior methods. In summary, this work introduces a simpler yet effective approach for aligning LLMs to human preferences through relative feedback.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/RF4WGR7E/Wu et al. - 2023 - Pairwise Proximal Policy Optimization Harnessing .pdf;/Users/fangyuan/Zotero/storage/SF7L5KEC/2310.html}
}

@misc{wuRoboMINDBenchmarkMultiembodiment2024,
  title = {{{RoboMIND}}: {{Benchmark}} on {{Multi-embodiment Intelligence Normative Data}} for {{Robot Manipulation}}},
  shorttitle = {{{RoboMIND}}},
  author = {Wu, Kun and Hou, Chengkai and Liu, Jiaming and Che, Zhengping and Ju, Xiaozhu and Yang, Zhuqin and Li, Meng and Zhao, Yinuo and Xu, Zhiyuan and Yang, Guang and Zhao, Zhen and Li, Guangyu and Jin, Zhao and Wang, Lecheng and Mao, Jilei and Wang, Xinhua and Fan, Shichao and Liu, Ning and Ren, Pei and Zhang, Qiang and Lyu, Yaoxu and Liu, Mengzhen and He, Jingyang and Luo, Yulin and Gao, Zeyu and Li, Chenxuan and Gu, Chenyang and Fu, Yankai and Wu, Di and Wang, Xingyu and Chen, Sixiang and Wang, Zhenyu and An, Pengju and Qian, Siyuan and Zhang, Shanghang and Tang, Jian},
  year = {2024},
  month = dec,
  number = {arXiv:2412.13877},
  eprint = {2412.13877},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.13877},
  urldate = {2024-12-19},
  abstract = {Developing robust and general-purpose robotic manipulation policies is a key goal in the field of robotics. To achieve effective generalization, it is essential to construct comprehensive datasets that encompass a large number of demonstration trajectories and diverse tasks. Unlike vision or language data that can be collected from the Internet, robotic datasets require detailed observations and manipulation actions, necessitating significant investment in hardware-software infrastructure and human labor. While existing works have focused on assembling various individual robot datasets, there remains a lack of a unified data collection standard and insufficient diversity in tasks, scenarios, and robot types. In this paper, we introduce RoboMIND (Multi-embodiment Intelligence Normative Data for Robot manipulation), featuring 55k real-world demonstration trajectories across 279 diverse tasks involving 61 different object classes. RoboMIND is collected through human teleoperation and encompasses comprehensive robotic-related information, including multi-view RGB-D images, proprioceptive robot state information, end effector details, and linguistic task descriptions. To ensure dataset consistency and reliability during policy learning, RoboMIND is built on a unified data collection platform and standardized protocol, covering four distinct robotic embodiments. We provide a thorough quantitative and qualitative analysis of RoboMIND across multiple dimensions, offering detailed insights into the diversity of our datasets. In our experiments, we conduct extensive real-world testing with four state-of-the-art imitation learning methods, demonstrating that training with RoboMIND data results in a high manipulation success rate and strong generalization. Our project is at https://x-humanoid-robomind.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Benchmark,Tien Kung},
  file = {/Users/fangyuan/Zotero/storage/VSUR6DMQ/Wu et al. - 2024 - RoboMIND Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation.pdf;/Users/fangyuan/Zotero/storage/7JHX9QBM/2412.html}
}

@misc{wuScalableTrustregionMethod2017,
  title = {Scalable Trust-Region Method for Deep Reinforcement Learning Using {{Kronecker-factored}} Approximation},
  author = {Wu, Yuhuai and Mansimov, Elman and Liao, Shun and Grosse, Roger and Ba, Jimmy},
  year = {2017},
  month = aug,
  eprint = {1708.05144},
  primaryclass = {cs},
  urldate = {2022-07-19},
  abstract = {In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/3KNTTHNS/Wu et al. - 2017 - Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation.pdf}
}

@article{wuSemisupervisedSim2realTactile2024,
  title = {Semi-Supervised {{Sim2real}} of {{Tactile Control Policies}} for {{Contact-Rich Manipulation Tasks}}},
  author = {Wu, Qiwei and Peng, Xuanbin and Zhou, Jiayu and Sun, Zhuoran and Xiong, Xiaogang and Lou, Yunjiang},
  year = {2024},
  abstract = {An increasing number of robotic manipulation tasks now use optical tactile sensors to provide tactile feedback, making tactile servo control a crucial aspect of robotic operations. This paper presents a framework that achieves optical-tactile image sim2real transfer and robust tactile servo control using limited paired data. The sim2real aspect employs a semi-supervised approach, beginning with pretraining the latent space representations of tactile images and subsequently mapping different tactile image domains to a shared latent space within a simulated tactile image domain. This latent space, combined with the proprioceptive information of the robotic arm, is then integrated into a privileged learning framework for policy training, which results in a deployable tactile control policy. Our results demonstrate the robustness of the proposed framework in achieving task objectives across different tactile sensors with varying physical parameters. Furthermore, with manipulators equipped with tactile sensors, it allows for rapid training and deployment for diverse contactrich tasks, including object pushing and surface following.},
  langid = {english},
  keywords = {review},
  file = {/Users/fangyuan/Zotero/storage/GLA5D5CD/Wu et al. - 2024 - Semi-supervised Sim2real of Tactile Control Polici.pdf}
}

@misc{wuVisualChatGPTTalking2023,
  title = {Visual {{ChatGPT}}: {{Talking}}, {{Drawing}} and {{Editing}} with {{Visual Foundation Models}}},
  shorttitle = {Visual {{ChatGPT}}},
  author = {Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
  year = {2023},
  month = mar,
  number = {arXiv:2303.04671},
  eprint = {2303.04671},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.04671},
  urldate = {2023-03-10},
  abstract = {ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called {\textbackslash}textbf\{Visual ChatGPT\}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at {\textbackslash}url\{https://github.com/microsoft/visual-chatgpt\}.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/2NC94W2N/Wu et al. - 2023 - Visual ChatGPT Talking, Drawing and Editing with .pdf;/Users/fangyuan/Zotero/storage/AGVZP985/2303.html}
}

@misc{xieDecomposingGeneralizationGap2023,
  title = {Decomposing the {{Generalization Gap}} in {{Imitation Learning}} for {{Visual Robotic Manipulation}}},
  author = {Xie, Annie and Lee, Lisa and Xiao, Ted and Finn, Chelsea},
  year = {2023},
  month = jul,
  number = {arXiv:2307.03659},
  eprint = {2307.03659},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-10},
  abstract = {What makes generalization hard for imitation learning in visual robotic manipulation? This question is difficult to approach at face value, but the environment from the perspective of a robot can often be decomposed into enumerable factors of variation, such as the lighting conditions or the placement of the camera. Empirically, generalization to some of these factors have presented a greater obstacle than others, but existing work sheds little light on precisely how much each factor contributes to the generalization gap. Towards an answer to this question, we study imitation learning policies in simulation and on a real robot language-conditioned manipulation task to quantify the difficulty of generalization to different (sets of) factors. We also design a new simulated benchmark of 19 tasks with 11 factors of variation to facilitate more controlled evaluations of generalization. From our study, we determine an ordering of factors based on generalization difficulty, that is consistent across simulation and our real robot setup.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/A49EIVIX/Xie et al. - 2023 - Decomposing the Generalization Gap in Imitation Le.pdf}
}

@misc{xieLatentDiffusionPlanning2025,
  title = {Latent {{Diffusion Planning}} for {{Imitation Learning}}},
  author = {Xie, Amber and Rybkin, Oleh and Sadigh, Dorsa and Finn, Chelsea},
  year = {2025},
  month = apr,
  number = {arXiv:2504.16925},
  eprint = {2504.16925},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.16925},
  urldate = {2025-04-27},
  abstract = {Recent progress in imitation learning has been enabled by policy architectures that scale to complex visuomotor tasks, multimodal distributions, and large datasets. However, these methods often rely on learning from large amount of expert demonstrations. To address these shortcomings, we propose Latent Diffusion Planning (LDP), a modular approach consisting of a planner which can leverage action-free demonstrations, and an inverse dynamics model which can leverage suboptimal data, that both operate over a learned latent space. First, we learn a compact latent space through a variational autoencoder, enabling effective forecasting of future states in image-based domains. Then, we train a planner and an inverse dynamics model with diffusion objectives. By separating planning from action prediction, LDP can benefit from the denser supervision signals of suboptimal and action-free data. On simulated visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation learning approaches, as they cannot leverage such additional data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/EXJL8LRU/Xie et al. - 2025 - Latent Diffusion Planning for Imitation Learning.pdf;/Users/fangyuan/Zotero/storage/NW5PLWPS/2504.html}
}

@misc{xiongAICMLLMAutonomous2024,
  title = {{{AIC MLLM}}: {{Autonomous Interactive Correction MLLM}} for {{Robust Robotic Manipulation}}},
  shorttitle = {{{AIC MLLM}}},
  author = {Xiong, Chuyan and Shen, Chengyu and Li, Xiaoqi and Zhou, Kaichen and Liu, Jiaming and Wang, Ruiping and Dong, Hao},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11548},
  eprint = {2406.11548},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-19},
  abstract = {The ability to reflect on and correct failures is crucial for robotic systems to interact stably with real-life objects.Observing the generalization and reasoning capabilities of Multimodal Large Language Models (MLLMs), previous approaches have aimed to utilize these models to enhance robotic systems accordingly.However, these methods typically focus on high-level planning corrections using an additional MLLM, with limited utilization of failed samples to correct low-level contact poses. To address this gap, we propose an Autonomous Interactive Correction (AIC) MLLM, which makes use of previous low-level interaction experiences to correct SE(3) pose predictions. Specifically, AIC MLLM is initially fine-tuned to acquire both pose prediction and feedback prompt comprehension abilities.We carefully design two types of prompt instructions through interactions with objects: 1) visual masks to highlight unmovable parts for position correction, and 2)textual descriptions to indicate potential directions for rotation correction.During inference, a Feedback Information Extraction module is introduced to recognize the failure cause, allowing AIC MLLM to adaptively correct the pose prediction using the corresponding prompts.To further enhance manipulation stability, we devise a Test Time Adaptation strategy that enables AIC MLLM to better adapt to the current scene configuration.Finally, extensive experiments are conducted in both simulated and real-world environments to evaluate the proposed method. The results demonstrate that our AIC MLLM can efficiently correct failure samples by leveraging interaction experience prompts.Real-world demonstration can be found at https://sites.google.com/view/aic-mllm},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Manipulation,Robotic Foundation Model},
  file = {/Users/fangyuan/Zotero/storage/INFJZSHQ/2406.pdf}
}

@misc{xuDexterousGraspTransformer2024,
  title = {Dexterous {{Grasp Transformer}}},
  author = {Xu, Guo-Hao and Wei, Yi-Lin and Zheng, Dian and Wu, Xiao-Ming and Zheng, Wei-Shi},
  year = {2024},
  month = apr,
  number = {arXiv:2404.18135},
  eprint = {2404.18135},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-30},
  abstract = {In this work, we propose a novel discriminative framework for dexterous grasp generation, named Dexterous Grasp TRansformer (DGTR), capable of predicting a diverse set of feasible grasp poses by processing the object point cloud with only one forward pass. We formulate dexterous grasp generation as a set prediction task and design a transformer-based grasping model for it. However, we identify that this set prediction paradigm encounters several optimization challenges in the field of dexterous grasping and results in restricted performance. To address these issues, we propose progressive strategies for both the training and testing phases. First, the dynamic-static matching training (DSMT) strategy is presented to enhance the optimization stability during the training phase. Second, we introduce the adversarial-balanced test-time adaptation (AB-TTA) with a pair of adversarial losses to improve grasping quality during the testing phase. Experimental results on the DexGraspNet dataset demonstrate the capability of DGTR to predict dexterous grasp poses with both high quality and diversity. Notably, while keeping high quality, the diversity of grasp poses predicted by DGTR significantly outperforms previous works in multiple metrics without any data pre-processing. Codes are available at https://github.com/iSEE-Laboratory/DGTR .},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/8NQL7WSL/Xu et al. - 2024 - Dexterous Grasp Transformer.pdf;/Users/fangyuan/Zotero/storage/AKYUEXL5/2404.html}
}

@misc{xuDexterousManipulationImages2022,
  title = {Dexterous {{Manipulation}} from {{Images}}: {{Autonomous Real-World RL}} via {{Substep Guidance}}},
  shorttitle = {Dexterous {{Manipulation}} from {{Images}}},
  author = {Xu, Kelvin and Hu, Zheyuan and Doshi, Ria and Rovinsky, Aaron and Kumar, Vikash and Gupta, Abhishek and Levine, Sergey},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09902},
  eprint = {2212.09902},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09902},
  urldate = {2022-12-26},
  abstract = {Complex and contact-rich robotic manipulation tasks, particularly those that involve multi-fingered hands and underactuated object manipulation, present a significant challenge to any control method. Methods based on reinforcement learning offer an appealing choice for such settings, as they can enable robots to learn to delicately balance contact forces and dexterously reposition objects without strong modeling assumptions. However, running reinforcement learning on real-world dexterous manipulation systems often requires significant manual engineering. This negates the benefits of autonomous data collection and ease of use that reinforcement learning should in principle provide. In this paper, we describe a system for vision-based dexterous manipulation that provides a "programming-free" approach for users to define new tasks and enable robots with complex multi-fingered hands to learn to perform them through interaction. The core principle underlying our system is that, in a vision-based setting, users should be able to provide high-level intermediate supervision that circumvents challenges in teleoperation or kinesthetic teaching which allow a robot to not only learn a task efficiently but also to autonomously practice. Our system includes a framework for users to define a final task and intermediate sub-tasks with image examples, a reinforcement learning procedure that learns the task autonomously without interventions, and experimental results with a four-finger robotic hand learning multi-stage object manipulation tasks directly in the real world, without simulation, manual modeling, or reward engineering.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/3336HC7S/Xu et al. - 2022 - Dexterous Manipulation from Images Autonomous Rea.pdf;/Users/fangyuan/Zotero/storage/AISFD3PG/2212.html}
}

@misc{xuDynamicsGuidedDiffusionModel2025,
  title = {Dynamics-{{Guided Diffusion Model}} for {{Sensor-less Robot Manipulator Design}}},
  author = {Xu, Xiaomeng and Ha, Huy and Song, Shuran},
  year = {2025},
  month = mar,
  number = {arXiv:2402.15038},
  eprint = {2402.15038},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.15038},
  urldate = {2025-04-01},
  abstract = {We present Dynamics-Guided Diffusion Model (DGDM), a data-driven framework for generating task-specific manipulator designs without task-specific training. Given object shapes and task specifications, DGDM generates sensor-less manipulator designs that can blindly manipulate objects towards desired motions and poses using an open-loop parallel motion. This framework 1) flexibly represents manipulation tasks as interaction profiles, 2) represents the design space using a geometric diffusion model, and 3) efficiently searches this design space using the gradients provided by a dynamics network trained without any task information. We evaluate DGDM on various manipulation tasks ranging from shifting/rotating objects to converging objects to a specific pose. Our generated designs outperform optimization-based and unguided diffusion baselines relatively by 31.5\% and 45.3\% on average success rate. With the ability to generate a new design within 0.8s, DGDM facilitates rapid design iteration and enhances the adoption of data-driven approaches for robot mechanism design. Qualitative results are best viewed on our project website https://dgdm-robot.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/75CLEQG7/Xu et al. - 2025 - Dynamics-Guided Diffusion Model for Sensor-less Robot Manipulator Design.pdf;/Users/fangyuan/Zotero/storage/6ZFHBU5S/2402.html}
}

@misc{xuPointLLMEmpoweringLarge2023,
  title = {{{PointLLM}}: {{Empowering Large Language Models}} to {{Understand Point Clouds}}},
  shorttitle = {{{PointLLM}}},
  author = {Xu, Runsen and Wang, Xiaolong and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
  year = {2023},
  month = dec,
  number = {arXiv:2308.16911},
  eprint = {2308.16911},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.16911},
  urldate = {2024-02-01},
  abstract = {The unprecedented advancements in Large Language Models (LLMs) have shown a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM understands colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate the perceptual and generalization capabilities of PointLLM, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different methods, including human evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50\% of the samples. Codes, datasets, and benchmarks are available at https://github.com/OpenRobotLab/PointLLM .},
  archiveprefix = {arXiv},
  keywords = {PointCloud,Robotic Foundation Model},
  file = {/Users/fangyuan/Zotero/storage/38N2P84W/Xu et al. - 2024 - PointLLM Empowering Large Language Models to Understand Point Clouds.pdf;/Users/fangyuan/Zotero/storage/6CBL3CJ4/2308.html}
}

@misc{xuRLDGRoboticGeneralist2024,
  title = {{{RLDG}}: {{Robotic Generalist Policy Distillation}} via {{Reinforcement Learning}}},
  shorttitle = {{{RLDG}}},
  author = {Xu, Charles and Li, Qiyang and Luo, Jianlan and Levine, Sergey},
  year = {2024},
  month = dec,
  number = {arXiv:2412.09858},
  eprint = {2412.09858},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.09858},
  urldate = {2024-12-18},
  abstract = {Recent advances in robotic foundation models have enabled the development of generalist policies that can adapt to diverse tasks. While these models show impressive flexibility, their performance heavily depends on the quality of their training data. In this work, we propose Reinforcement Learning Distilled Generalists (RLDG), a method that leverages reinforcement learning to generate high-quality training data for finetuning generalist policies. Through extensive real-world experiments on precise manipulation tasks like connector insertion and assembly, we demonstrate that generalist policies trained with RL-generated data consistently outperform those trained with human demonstrations, achieving up to 40\% higher success rates while generalizing better to new tasks. We also provide a detailed analysis that reveals this performance gain stems from both optimized action distributions and improved state coverage. Our results suggest that combining task-specific RL with generalist policy distillation offers a promising approach for developing more capable and efficient robotic manipulation systems that maintain the flexibility of foundation models while achieving the performance of specialized controllers. Videos and code can be found on our project website https://generalist-distillation.github.io},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/TDN8GQKJ/Xu et al. - 2024 - RLDG Robotic Generalist Policy Distillation via Reinforcement Learning.pdf;/Users/fangyuan/Zotero/storage/PCLWCR8D/2412.html}
}

@misc{xuSurveyRoboticsFoundation2024,
  title = {A {{Survey}} on {{Robotics}} with {{Foundation Models}}: Toward {{Embodied AI}}},
  shorttitle = {A {{Survey}} on {{Robotics}} with {{Foundation Models}}},
  author = {Xu, Zhiyuan and Wu, Kun and Wen, Junjie and Li, Jinming and Liu, Ning and Che, Zhengping and Tang, Jian},
  year = {2024},
  month = feb,
  number = {arXiv:2402.02385},
  eprint = {2402.02385},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/VLEKWXZ8/Xu et al. - 2024 - A Survey on Robotics with Foundation Models toward Embodied AI.pdf;/Users/fangyuan/Zotero/storage/6DVRN3AT/2402.html}
}

@misc{xuVLACacheEfficientVisionLanguageAction2025,
  title = {{{VLA-Cache}}: {{Towards Efficient Vision-Language-Action Model}} via {{Adaptive Token Caching}} in {{Robotic Manipulation}}},
  shorttitle = {{{VLA-Cache}}},
  author = {Xu, Siyu and Wang, Yunke and Xia, Chenghao and Zhu, Dihao and Huang, Tao and Xu, Chang},
  year = {2025},
  month = feb,
  number = {arXiv:2502.02175},
  eprint = {2502.02175},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.02175},
  urldate = {2025-02-07},
  abstract = {Vision-Language-Action (VLA) model can process instructions and visual perception to directly generate actions as output in an end-to-end fashion due to its strong multi-modal reasoning capabilities. While the performance of VLA models is promising, their computational cost can be substantial. This raises challenge for applying them on robotics tasks, which requires real-time decision-making to respond quickly to environmental changes. Since robotic control involves sequential decision-making, the visual input often exhibits minimal variation between successive steps. A natural idea is to reuse the computational results of unchanged visual tokens from the last step. Motivated by this idea, we propose VLA-Cache, an efficient vision-language-action model. VLA-Cache incorporates a token-selection mechanism that compares the visual input at each step with the input from the previous step, adaptively identifying visual tokens with minimal changes. The computational results for these unchanged tokens are then reused in subsequent steps via KV-cache, thereby significantly improving the efficiency of the VLA-Cache model. Experimental results on both simulation (e.g., LIBERO benchmark and SIMPLER) and real-world robot valid VLA-Cache can achieve practical acceleration with minimal sacrifice in success rate.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/UWIGH3T6/Xu et al. - 2025 - VLA-Cache Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Mani.pdf;/Users/fangyuan/Zotero/storage/YPF39SBC/2502.html}
}

@inproceedings{yangACECrossplatformVisualExoskeletons2024,
  title = {{{ACE}}: {{A Cross-platform}} and Visual-{{Exoskeletons System}} for {{Low-Cost Dexterous Teleoperation}}},
  shorttitle = {{{ACE}}},
  booktitle = {8th {{Annual Conference}} on {{Robot Learning}}},
  author = {Yang, Shiqi and Liu, Minghuan and Qin, Yuzhe and Ding, Runyu and Li, Jialong and Cheng, Xuxin and Yang, Ruihan and Yi, Sha and Wang, Xiaolong},
  year = {2024},
  month = sep,
  urldate = {2024-12-07},
  abstract = {Bimanual robotic manipulation with dexterous hands has a large potential workability and a wide workspace as it follows the most natural human workflow. Learning from human demonstrations has proven highly effective for learning a dexterous manipulation policy. To collect such data, teleoperation serves as a straightforward and efficient way to do so. However, a cost-effective and easy-to-use teleoperation system is lacking for anthropomorphic robot hands. To fill the deficiency, we developed {\textbackslash}our, a cross-platform visual-exoskeleton system for low-cost dexterous teleoperation. Our system employs a hand-facing camera to capture 3D hand poses and an exoskeleton mounted on a base that can be easily carried on users' backs. ACE captures both the hand root end-effector and hand pose in real-time and enables cross-platform operations. We evaluate the key system parameters compared with previous teleoperation systems and show clear advantages of {\textbackslash}our. We then showcase the desktop and mobile versions of our system on six different robot platforms (including humanoid-hands, arm-hands, arm-gripper, and quadruped-gripper systems), and demonstrate the effectiveness of learning three difficult real-world tasks through the collected demonstration on two of them.},
  langid = {english},
  keywords = {Manipulation,Teleoperation},
  file = {/Users/fangyuan/Zotero/storage/SQJJ5DTP/Yang et al. - 2024 - ACE A Cross-platform and visual-Exoskeletons System for Low-Cost Dexterous Teleoperation.pdf}
}

@misc{yangPushingLimitsCrossEmbodiment2024,
  title = {Pushing the {{Limits}} of {{Cross-Embodiment Learning}} for {{Manipulation}} and {{Navigation}}},
  author = {Yang, Jonathan and Glossop, Catherine and Bhorkar, Arjun and Shah, Dhruv and Vuong, Quan and Finn, Chelsea and Sadigh, Dorsa and Levine, Sergey},
  year = {2024},
  month = feb,
  number = {arXiv:2402.19432},
  eprint = {2402.19432},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.19432},
  urldate = {2024-03-08},
  abstract = {Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains, such as robotic navigation and manipulation, can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling robotic arms, quadcopters, quadrupeds, and mobile bases. We then investigate the extent to which transfer can occur across navigation and manipulation on these embodiments by framing them as a single goal-reaching task. We find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a novel embodiment in a zero-shot manner. These results provide evidence that large-scale robotic policies can benefit from data collected across various embodiments. Further information and robot videos can be found on our project website http://extreme-cross-embodiment.github.io.},
  archiveprefix = {arXiv},
  keywords = {Diffusion,Manipulation,Navigation},
  file = {/Users/fangyuan/Zotero/storage/3DY3CPP2/Yang et al. - 2024 - Pushing the Limits of Cross-Embodiment Learning fo.pdf;/Users/fangyuan/Zotero/storage/HGWF7ZTF/2402.html}
}

@misc{yaoTreeThoughtsDeliberate2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  year = {2023},
  month = may,
  number = {arXiv:2305.10601},
  eprint = {2305.10601},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.10601},
  urldate = {2023-11-22},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/E4HIQPRJ/Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with .pdf;/Users/fangyuan/Zotero/storage/PIXN7368/2305.html}
}

@misc{yeReinforcementLearningFoundation2024,
  title = {Reinforcement {{Learning}} with {{Foundation Priors}}: {{Let}} the {{Embodied Agent Efficiently Learn}} on {{Its Own}}},
  shorttitle = {Reinforcement {{Learning}} with {{Foundation Priors}}},
  author = {Ye, Weirui and Zhang, Yunsheng and Weng, Haoyang and Gu, Xianfan and Wang, Shengjie and Zhang, Tong and Wang, Mengchen and Abbeel, Pieter and Gao, Yang},
  year = {2024},
  month = oct,
  number = {arXiv:2310.02635},
  eprint = {2310.02635},
  publisher = {arXiv},
  urldate = {2024-10-14},
  abstract = {Reinforcement learning (RL) is a promising approach for solving robotic manipulation tasks. However, it is challenging to apply the RL algorithms directly in the real world. For one thing, RL is data-intensive and typically requires millions of interactions with environments, which are impractical in real scenarios. For another, it is necessary to make heavy engineering efforts to design reward functions manually. To address these issues, we leverage foundation models in this paper. We propose Reinforcement Learning with Foundation Priors (RLFP) to utilize guidance and feedback from policy, value, and success-reward foundation models. Within this framework, we introduce the Foundation-guided Actor-Critic (FAC) algorithm, which enables embodied agents to explore more efficiently with automatic reward functions. The benefits of our framework are threefold: (1) {\textbackslash}textit\{sample efficient\}; (2) {\textbackslash}textit\{minimal and effective reward engineering\}; (3) {\textbackslash}textit\{agnostic to foundation model forms and robust to noisy priors\}. Our method achieves remarkable performances in various manipulation tasks on both real robots and in simulation. Across 5 dexterous tasks with real robots, FAC achieves an average success rate of 86{\textbackslash}\% after one hour of real-time learning. Across 8 tasks in the simulated Meta-world, FAC achieves 100{\textbackslash}\% success rates in 7/8 tasks under less than 100k frames (about 1-hour training), outperforming baseline methods with manual-designed rewards in 1M frames. We believe the RLFP framework can enable future robots to explore and learn autonomously in the physical world for more tasks. Visualizations and code are available at {\textbackslash}url\{https://yewr.github.io/rlfp\}.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/UFUAD65E/Ye et al. - 2024 - Reinforcement Learning with Foundation Priors Let the Embodied Agent Efficiently Learn on Its Own.pdf;/Users/fangyuan/Zotero/storage/VNVJHPJN/2310.html}
}

@misc{yeVideo2PolicyScalingManipulation2025,
  title = {{{Video2Policy}}: {{Scaling}} up {{Manipulation Tasks}} in {{Simulation}} through {{Internet Videos}}},
  shorttitle = {{{Video2Policy}}},
  author = {Ye, Weirui and Liu, Fangchen and Ding, Zheng and Gao, Yang and Rybkin, Oleh and Abbeel, Pieter},
  year = {2025},
  month = feb,
  number = {arXiv:2502.09886},
  eprint = {2502.09886},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.09886},
  urldate = {2025-02-17},
  abstract = {Simulation offers a promising approach for cheaply scaling training data for generalist policies. To scalably generate data from diverse and realistic tasks, existing algorithms either rely on large language models (LLMs) that may hallucinate tasks not interesting for robotics; or digital twins, which require careful real-to-sim alignment and are hard to scale. To address these challenges, we introduce Video2Policy, a novel framework that leverages internet RGB videos to reconstruct tasks based on everyday human behavior. Our approach comprises two phases: (1) task generation in simulation from videos; and (2) reinforcement learning utilizing in-context LLM-generated reward functions iteratively. We demonstrate the efficacy of Video2Policy by reconstructing over 100 videos from the Something-Something-v2 (SSv2) dataset, which depicts diverse and complex human behaviors on 9 different tasks. Our method can successfully train RL policies on such tasks, including complex and challenging tasks such as throwing. Finally, we show that the generated simulation data can be scaled up for training a general policy, and it can be transferred back to the real robot in a Real2Sim2Real way.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/48SV96CQ/Ye et al. - 2025 - Video2Policy Scaling up Manipulation Tasks in Simulation through Internet Videos.pdf;/Users/fangyuan/Zotero/storage/6WYSHIY2/2502.html}
}

@misc{yuanBeing0HumanoidRobotic2025,
  title = {Being-0: {{A Humanoid Robotic Agent}} with {{Vision-Language Models}} and {{Modular Skills}}},
  shorttitle = {Being-0},
  author = {Yuan, Haoqi and Bai, Yu and Fu, Yuhui and Zhou, Bohan and Feng, Yicheng and Xu, Xinrun and Zhan, Yi and Karlsson, B{\"o}rje F. and Lu, Zongqing},
  year = {2025},
  month = mar,
  number = {arXiv:2503.12533},
  eprint = {2503.12533},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.12533},
  urldate = {2025-03-20},
  abstract = {Building autonomous robotic agents capable of achieving human-level performance in real-world embodied tasks is an ultimate goal in humanoid robot research. Recent advances have made significant progress in high-level cognition with Foundation Models (FMs) and low-level skill development for humanoid robots. However, directly combining these components often results in poor robustness and efficiency due to compounding errors in long-horizon tasks and the varied latency of different modules. We introduce Being-0, a hierarchical agent framework that integrates an FM with a modular skill library. The FM handles high-level cognitive tasks such as instruction understanding, task planning, and reasoning, while the skill library provides stable locomotion and dexterous manipulation for low-level control. To bridge the gap between these levels, we propose a novel Connector module, powered by a lightweight vision-language model (VLM). The Connector enhances the FM's embodied capabilities by translating language-based plans into actionable skill commands and dynamically coordinating locomotion and manipulation to improve task success. With all components, except the FM, deployable on low-cost onboard computation devices, Being-0 achieves efficient, real-time performance on a full-sized humanoid robot equipped with dexterous hands and active vision. Extensive experiments in large indoor environments demonstrate Being-0's effectiveness in solving complex, long-horizon tasks that require challenging navigation and manipulation subtasks. For further details and videos, visit https://beingbeyond.github.io/being-0.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/SY659IB6/Yuan et al. - 2025 - Being-0 A Humanoid Robotic Agent with Vision-Language Models and Modular Skills.pdf;/Users/fangyuan/Zotero/storage/MEVVIWCH/2503.html}
}

@misc{yueDeeRVLADynamicInference2024,
  title = {{{DeeR-VLA}}: {{Dynamic Inference}} of {{Multimodal Large Language Models}} for {{Efficient Robot Execution}}},
  shorttitle = {{{DeeR-VLA}}},
  author = {Yue, Yang and Wang, Yulin and Kang, Bingyi and Han, Yizeng and Wang, Shenzhi and Song, Shiji and Feng, Jiashi and Huang, Gao},
  year = {2024},
  month = nov,
  number = {arXiv:2411.02359},
  eprint = {2411.02359},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.02359},
  urldate = {2025-04-23},
  abstract = {MLLMs have demonstrated remarkable comprehension and reasoning capabilities with complex language and visual data. These advances have spurred the vision of establishing a generalist robotic MLLM proficient in understanding complex human instructions and accomplishing various embodied tasks. However, developing MLLMs for real-world robots is challenging due to the typically limited computation and memory capacities available on robotic platforms. In contrast, the inference of MLLMs involves storing billions of parameters and performing tremendous computation, imposing significant hardware demands. In our paper, we propose a Dynamic Early-Exit Framework for Robotic Vision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically adjusts the size of the activated MLLM based on each situation at hand. The approach leverages a multi-exit architecture in MLLMs, which allows the model to terminate processing once a proper size of the model has been activated for a specific situation, thus avoiding further redundant computation. Additionally, we develop novel algorithms that establish early-termination criteria for DeeR, conditioned on predefined demands such as average computational cost (i.e., power consumption), as well as peak computational consumption (i.e., latency) and GPU memory usage. These enhancements ensure that DeeR operates efficiently under varying resource constraints while maintaining competitive performance. On the CALVIN robot manipulation benchmark, DeeR demonstrates significant reductions in computational costs of LLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance. Code and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/7TGC7GEN/Yue et al. - 2024 - DeeR-VLA Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution.pdf;/Users/fangyuan/Zotero/storage/CVL4RVLL/2411.html}
}

@misc{yuHowLeverageUnlabeled2022,
  title = {How to {{Leverage Unlabeled Data}} in {{Offline Reinforcement Learning}}},
  author = {Yu, Tianhe and Kumar, Aviral and Chebotar, Yevgen and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  year = {2022},
  month = jul,
  eprint = {2202.01741},
  primaryclass = {cs},
  urldate = {2022-07-19},
  abstract = {Offline reinforcement learning (RL) can learn control policies from static datasets but, like standard RL methods, it requires reward annotations for every transition. In many cases, labeling large datasets with rewards may be costly, especially if those rewards must be provided by human labelers, while collecting diverse unlabeled data might be comparatively inexpensive. How can we best leverage such unlabeled data in offline RL? One natural solution is to learn a reward function from the labeled data and use it to label the unlabeled data. In this paper, we find that, perhaps surprisingly, a much simpler method that simply applies zero rewards to unlabeled data leads to effective data sharing both in theory and in practice, without learning any reward model at all. While this approach might seem strange (and incorrect) at first, we provide extensive theoretical and empirical analysis that illustrates how it trades off reward bias, sample complexity and distributional shift, often leading to good results. We characterize conditions under which this simple strategy is effective, and further show that extending it with a simple reweighting approach can further alleviate the bias introduced by using incorrect reward labels. Our empirical evaluation confirms these findings in simulated robotic locomotion, navigation, and manipulation settings.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/Y7GWYYYF/Yu et al. - 2022 - How to Leverage Unlabeled Data in Offline Reinforcement Learning.pdf}
}

@article{zawalskiFASTPRECISEADJUSTING,
  title = {{{FAST AND PRECISE}}: {{ADJUSTING PLANNING HORIZON WITH ADAPTIVE SUBGOAL SEARCH}}},
  author = {Zawalski, Micha{\l} and Tyrolski, Micha{\l} and Czechowski, Konrad and Odrzyg{\'o}zdz, Tomasz and Stachura, Damian},
  abstract = {Complex reasoning problems contain states that vary in the computational cost required to determine the right action plan. To take advantage of this property, we propose Adaptive Subgoal Search (AdaSubS), a search method that adaptively adjusts the planning horizon. To this end, AdaSubS generates diverse sets of subgoals at different distances. A verification mechanism is employed to filter out unreachable subgoals swiftly, making it possible to focus on feasible further subgoals. In this way, AdaSubS benefits from the efficiency of planning with longerterm subgoals and the fine control with shorter-term ones, and thus scales well to difficult planning problems. We show that AdaSubS significantly surpasses hierarchical planning algorithms on three complex reasoning tasks: Sokoban, the Rubik's Cube, and the inequality-proving benchmark INT.},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/SYVVY5Q7/Zawalski et al. - FAST AND PRECISE ADJUSTING PLANNING HORIZON WITH .pdf}
}

@misc{zawalskiRoboticControlEmbodied2025,
  title = {Robotic {{Control}} via {{Embodied Chain-of-Thought Reasoning}}},
  author = {Zawalski, Micha{\l} and Chen, William and Pertsch, Karl and Mees, Oier and Finn, Chelsea and Levine, Sergey},
  year = {2025},
  month = mar,
  number = {arXiv:2407.08693},
  eprint = {2407.08693},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.08693},
  urldate = {2025-03-10},
  abstract = {A key limitation of learned robot control policies is their inability to generalize outside their training data. Recent works on vision-language-action models (VLAs) have shown that the use of large, internet pre-trained vision-language models as the backbone of learned robot policies can substantially improve their robustness and generalization ability. Yet, one of the most exciting capabilities of large vision-language models in other domains is their ability to reason iteratively through complex problems. Can that same capability be brought into robotics to allow policies to improve performance by reasoning about a given task before acting? Naive use of "chain-of-thought" (CoT) style prompting is significantly less effective with standard VLAs because of the relatively simple training examples that are available to them. Additionally, purely semantic reasoning about sub-tasks, as is common in regular CoT, is insufficient for robot policies that need to ground their reasoning in sensory observations and the robot state. To this end, we introduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we train VLAs to perform multiple steps of reasoning about plans, sub-tasks, motions, and visually grounded features like object bounding boxes and end effector positions, before predicting the robot action. We design a scalable pipeline for generating synthetic training data for ECoT on large robot datasets. We demonstrate, that ECoT increases the absolute success rate of OpenVLA, the current strongest open-source VLA policy, by 28\% across challenging generalization tasks, without any additional robot training data. Additionally, ECoT makes it easier for humans to interpret a policy's failures and correct its behavior using natural language.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/WDUP93TC/Zawalski et al. - 2025 - Robotic Control via Embodied Chain-of-Thought Reasoning.pdf;/Users/fangyuan/Zotero/storage/2EBYBJJ9/2407.html}
}

@misc{ze3DDiffusionPolicy2024,
  title = {{{3D Diffusion Policy}}: {{Generalizable Visuomotor Policy Learning}} via {{Simple 3D Representations}}},
  shorttitle = {{{3D Diffusion Policy}}},
  author = {Ze, Yanjie and Zhang, Gu and Zhang, Kangning and Hu, Chenyuan and Wang, Muhan and Xu, Huazhe},
  year = {2024},
  month = apr,
  number = {arXiv:2403.03954},
  eprint = {2403.03954},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-11},
  abstract = {Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 24.2\% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85\%, given only 40 demonstrations of each task, and shows excellent generalization abilities in diverse aspects, including space, viewpoint, appearance, and instance. Interestingly, in real robot experiments, DP3 rarely violates safety requirements, in contrast to baseline methods which frequently do, necessitating human intervention. Our extensive evaluation highlights the critical importance of 3D representations in real-world robot learning. Videos, code, and data are available on https://3d-diffusion-policy.github.io .},
  archiveprefix = {arXiv},
  keywords = {Diffusion,Manipulation,Subgoal},
  file = {/Users/fangyuan/Zotero/storage/BEXMFSRZ/Ze et al. - 2024 - 3D Diffusion Policy Generalizable Visuomotor Policy Learning via Simple 3D Representations.pdf;/Users/fangyuan/Zotero/storage/E5BQP3VX/2403.html}
}

@misc{zeGeneralizableHumanoidManipulation2024,
  title = {Generalizable {{Humanoid Manipulation}} with {{Improved 3D Diffusion Policies}}},
  author = {Ze, Yanjie and Chen, Zixuan and Wang, Wenhao and Chen, Tianyi and He, Xialin and Yuan, Ying and Peng, Xue Bin and Wu, Jiajun},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10803},
  eprint = {2410.10803},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10803},
  urldate = {2024-12-07},
  abstract = {Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills. Recent advances in 3D visuomotor policies, such as the 3D Diffusion Policy (DP3), have shown promise in extending these capabilities to wilder environments. However, 3D visuomotor policies often rely on camera calibration and point-cloud segmentation, which present challenges for deployment on mobile robots like humanoids. In this work, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D visuomotor policy that eliminates these constraints by leveraging egocentric 3D visual representations. We demonstrate that iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab. Videos are available at: https://humanoid-manipulation.github.io},
  archiveprefix = {arXiv},
  keywords = {Diffusion,FFTAI,Manipulation},
  file = {/Users/fangyuan/Zotero/storage/IWT6Y6JE/Ze et al. - 2024 - Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies.pdf;/Users/fangyuan/Zotero/storage/87Y7H6KQ/2410.html}
}

@misc{zeGeneralizableHumanoidManipulation2025,
  title = {Generalizable {{Humanoid Manipulation}} with {{3D Diffusion Policies}}},
  author = {Ze, Yanjie and Chen, Zixuan and Wang, Wenhao and Chen, Tianyi and He, Xialin and Yuan, Ying and Peng, Xue Bin and Wu, Jiajun},
  year = {2025},
  month = feb,
  number = {arXiv:2410.10803},
  eprint = {2410.10803},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10803},
  urldate = {2025-02-20},
  abstract = {Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills and the expensiveness of in-the-wild humanoid robot data. In this work, we build a real-world robotic system to address this challenging problem. Our system is mainly an integration of 1) a whole-upper-body robotic teleoperation system to acquire human-like robot data, 2) a 25-DoF humanoid robot platform with a height-adjustable cart and a 3D LiDAR sensor, and 3) an improved 3D Diffusion Policy learning algorithm for humanoid robots to learn from noisy human data. We run more than 2000 episodes of policy rollouts on the real robot for rigorous policy evaluation. Empowered by this system, we show that using only data collected in one single scene and with only onboard computing, a full-sized humanoid robot can autonomously perform skills in diverse real-world scenarios. Videos are available at {\textbackslash}href\{https://humanoid-manipulation.github.io\}\{humanoid-manipulation.github.io\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/7FXC38L8/Ze et al. - 2025 - Generalizable Humanoid Manipulation with 3D Diffusion Policies.pdf;/Users/fangyuan/Zotero/storage/CTEI96RF/2410.html}
}

@misc{zengLearningRewardRobot2024,
  title = {Learning {{Reward}} for {{Robot Skills Using Large Language Models}} via {{Self-Alignment}}},
  author = {Zeng, Yuwei and Mu, Yao and Shao, Lin},
  year = {2024},
  month = may,
  number = {arXiv:2405.07162},
  eprint = {2405.07162},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.07162},
  urldate = {2024-05-21},
  abstract = {Learning reward functions remains the bottleneck to equip a robot with a broad repertoire of skills. Large Language Models (LLM) contain valuable task-related knowledge that can potentially aid in the learning of reward functions. However, the proposed reward function can be imprecise, thus ineffective which requires to be further grounded with environment information. We proposed a method to learn rewards more efficiently in the absence of humans. Our approach consists of two components: We first use the LLM to propose features and parameterization of the reward, then update the parameters through an iterative self-alignment process. In particular, the process minimizes the ranking inconsistency between the LLM and the learnt reward functions based on the execution feedback. The method was validated on 9 tasks across 2 simulation environments. It demonstrates a consistent improvement over training efficacy and efficiency, meanwhile consuming significantly fewer GPT tokens compared to the alternative mutation-based method.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/DP7H6C4G/Zeng et al. - 2024 - Learning Reward for Robot Skills Using Large Langu.pdf;/Users/fangyuan/Zotero/storage/THK5BVJY/2405.html}
}

@misc{zengTransporterNetworksRearranging2022,
  title = {Transporter {{Networks}}: {{Rearranging}} the {{Visual World}} for {{Robotic Manipulation}}},
  shorttitle = {Transporter {{Networks}}},
  author = {Zeng, Andy and Florence, Pete and Tompson, Jonathan and Welker, Stefan and Chien, Jonathan and Attarian, Maria and Armstrong, Travis and Krasin, Ivan and Duong, Dan and Wahid, Ayzaan and Sindhwani, Vikas and Lee, Johnny},
  year = {2022},
  month = jan,
  number = {arXiv:2010.14406},
  eprint = {2010.14406},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-19},
  abstract = {Robotic manipulation can be formulated as inducing a sequence of spatial displacements: where the space being moved can encompass an object, part of an object, or end effector. In this work, we propose the Transporter Network, a simple model architecture that rearranges deep features to infer spatial displacements from visual input - which can parameterize robot actions. It makes no assumptions of objectness (e.g. canonical poses, models, or keypoints), it exploits spatial symmetries, and is orders of magnitude more sample efficient than our benchmarked alternatives in learning vision-based manipulation tasks: from stacking a pyramid of blocks, to assembling kits with unseen objects; from manipulating deformable ropes, to pushing piles of small objects with closed-loop feedback. Our method can represent complex multi-modal policy distributions and generalizes to multi-step sequential tasks, as well as 6DoF pick-and-place. Experiments on 10 simulated tasks show that it learns faster and generalizes better than a variety of end-to-end baselines, including policies that use ground-truth object poses. We validate our methods with hardware in the real world. Experiment videos and code are available at https://transporternets.github.io},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/B86XI69C/Zeng et al. - 2022 - Transporter Networks Rearranging the Visual World.pdf}
}

@misc{zhangAutomaticChainThought2022,
  title = {Automatic {{Chain}} of {{Thought Prompting}} in {{Large Language Models}}},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03493},
  eprint = {2210.03493},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.03493},
  urldate = {2024-02-16},
  abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/QRRQCKTC/Zhang et al. - 2022 - Automatic Chain of Thought Prompting in Large Lang.pdf;/Users/fangyuan/Zotero/storage/K4PTGXXP/2210.html}
}

@inproceedings{zhangGraspStackingDeep2020,
  title = {Grasp for {{Stacking}} via {{Deep Reinforcement Learning}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Zhang, Junhao and Zhang, Wei and Song, Ran and Ma, Lin and Li, Yibin},
  year = {2020},
  month = may,
  pages = {2543--2549},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9197508},
  abstract = {Integrated robotic arm system should contain both grasp and place actions. However, most grasping methods focus more on how to grasp objects, while ignoring the placement of the grasped objects, which limits their applications in various industrial environments. In this research, we propose a model-free deep Q-learning method to learn the grasping-stacking strategy end-to-end from scratch. Our method maps the images to the actions of the robotic arm through two deep networks: the grasping network (GNet) using the observation of the desk and the pile to infer the gripper's position and orientation for grasping, and the stacking network (SNet) using the observation of the platform to infer the optimal location when placing the grasped object. To make a long-range planning, the two observations are integrated in the grasping for stacking network (GSN). We evaluate the proposed GSN on a grasping-stacking task in both simulated and real-world scenarios.},
  keywords = {Feature extraction,Grasping,Manipulators,Stacking,Task analysis,Training},
  file = {/Users/fangyuan/Zotero/storage/YS3ELY84/Zhang et al. - 2020 - Grasp for Stacking via Deep Reinforcement Learning.pdf;/Users/fangyuan/Zotero/storage/K6WSFKK4/stamp.html}
}

@misc{zhangHumanoidPanoHybridSpherical2025,
  title = {{{HumanoidPano}}: {{Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception}} for {{Humanoid Robots}}},
  shorttitle = {{{HumanoidPano}}},
  author = {Zhang, Qiang and Zhang, Zhang and Cui, Wei and Sun, Jingkai and Cao, Jiahang and Guo, Yijie and Han, Gang and Zhao, Wen and Wang, Jiaxu and Sun, Chenghao and Zhang, Lingfeng and Cheng, Hao and Chen, Yujie and Wang, Lin and Tang, Jian and Xu, Renjing},
  year = {2025},
  month = mar,
  number = {arXiv:2503.09010},
  eprint = {2503.09010},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.09010},
  urldate = {2025-03-17},
  abstract = {The perceptual system design for humanoid robots poses unique challenges due to inherent structural constraints that cause severe self-occlusion and limited field-of-view (FOV). We present HumanoidPano, a novel hybrid cross-modal perception framework that synergistically integrates panoramic vision and LiDAR sensing to overcome these limitations. Unlike conventional robot perception systems that rely on monocular cameras or standard multi-sensor configurations, our method establishes geometrically-aware modality alignment through a spherical vision transformer, enabling seamless fusion of 360 visual context with LiDAR's precise depth measurements. First, Spherical Geometry-aware Constraints (SGC) leverage panoramic camera ray properties to guide distortion-regularized sampling offsets for geometric alignment. Second, Spatial Deformable Attention (SDA) aggregates hierarchical 3D features via spherical offsets, enabling efficient 360\{{\textbackslash}deg\}-to-BEV fusion with geometrically complete object representations. Third, Panoramic Augmentation (AUG) combines cross-view transformations and semantic alignment to enhance BEV-panoramic feature consistency during data augmentation. Extensive evaluations demonstrate state-of-the-art performance on the 360BEV-Matterport benchmark. Real-world deployment on humanoid platforms validates the system's capability to generate accurate BEV segmentation maps through panoramic-LiDAR co-perception, directly enabling downstream navigation tasks in complex environments. Our work establishes a new paradigm for embodied perception in humanoid robotics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/U6WFT89S/Zhang et al. - 2025 - HumanoidPano Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception for Humanoid Robots.pdf;/Users/fangyuan/Zotero/storage/ND5KE3IG/2503.html}
}

@misc{zhangMoLeVLADynamicLayerskipping2025,
  title = {{{MoLe-VLA}}: {{Dynamic Layer-skipping Vision Language Action Model}} via {{Mixture-of-Layers}} for {{Efficient Robot Manipulation}}},
  shorttitle = {{{MoLe-VLA}}},
  author = {Zhang, Rongyu and Dong, Menghang and Zhang, Yuan and Heng, Liang and Chi, Xiaowei and Dai, Gaole and Du, Li and Wang, Dan and Du, Yuan and Zhang, Shanghang},
  year = {2025},
  month = mar,
  number = {arXiv:2503.20384},
  eprint = {2503.20384},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.20384},
  urldate = {2025-03-28},
  abstract = {Multimodal Large Language Models (MLLMs) excel in understanding complex language and visual data, enabling generalist robotic systems to interpret instructions and perform embodied tasks. Nevertheless, their real-world deployment is hindered by substantial computational and storage demands. Recent insights into the homogeneous patterns in the LLM layer have inspired sparsification techniques to address these challenges, such as early exit and token pruning. However, these methods often neglect the critical role of the final layers that encode the semantic information most relevant to downstream robotic tasks. Aligning with the recent breakthrough of the Shallow Brain Hypothesis (SBH) in neuroscience and the mixture of experts in model sparsification, we conceptualize each LLM layer as an expert and propose a Mixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe) architecture for dynamic LLM layer activation. We introduce a Spatial-Temporal Aware Router (STAR) for MoLe to selectively activate only parts of the layers based on the robot's current state, mimicking the brain's distinct signal pathways specialized for cognition and causal reasoning. Additionally, to compensate for the cognitive ability of LLMs lost in MoLe, we devise a Cognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the understanding of task demands and improves the generation of task-relevant action sequences by leveraging cognitive features. Extensive experiments conducted in both RLBench simulation and real-world environments demonstrate the superiority of MoLe-VLA in both efficiency and performance. Specifically, MoLe-VLA achieves an 8\% improvement in the mean success rate across ten tasks while reducing computational costs by up to x5.6 compared to standard LLMs.},
  archiveprefix = {arXiv},
  keywords = {Locomotion,Manipulation,Spatial,VLA,Whole Body Control},
  file = {/Users/fangyuan/Zotero/storage/D2GXIQL3/Zhang et al. - 2025 - MoLe-VLA Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Ro.pdf;/Users/fangyuan/Zotero/storage/6SHPZL66/2503.html}
}

@misc{zhangTraversabilityAwareLeggedNavigation2024,
  title = {Traversability-{{Aware Legged Navigation}} by {{Learning}} from {{Real-World Visual Data}}},
  author = {Zhang, Hongbo and Li, Zhongyu and Zeng, Xuanqi and Smith, Laura and Stachowicz, Kyle and Shah, Dhruv and Yue, Linzhu and Song, Zhitao and Xia, Weipeng and Levine, Sergey and Sreenath, Koushil and Liu, Yun-hui},
  year = {2024},
  month = oct,
  number = {arXiv:2410.10621},
  eprint = {2410.10621},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.10621},
  urldate = {2024-10-18},
  abstract = {The enhanced mobility brought by legged locomotion empowers quadrupedal robots to navigate through complex and unstructured environments. However, optimizing agile locomotion while accounting for the varying energy costs of traversing different terrains remains an open challenge. Most previous work focuses on planning trajectories with traversability cost estimation based on human-labeled environmental features. However, this human-centric approach is insufficient because it does not account for the varying capabilities of the robot locomotion controllers over challenging terrains. To address this, we develop a novel traversability estimator in a robot-centric manner, based on the value function of the robot's locomotion controller. This estimator is integrated into a new learning-based RGBD navigation framework. The framework develops a planner that guides the robot in avoiding obstacles and hard-to-traverse terrains while reaching its goals. The training of the navigation planner is directly performed in the real world using a sample efficient reinforcement learning method. Through extensive benchmarking, we demonstrate that the proposed framework achieves the best performance in accurate traversability cost estimation and efficient learning from multi-modal data (the robot's color and depth vision, and proprioceptive feedback) for real-world training. Using the proposed method, a quadrupedal robot learns to perform traversability-aware navigation through trial and error in various real-world environments with challenging terrains that are difficult to classify using depth vision alone.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/ZN2BYRB8/Zhang et al. - 2024 - Traversability-Aware Legged Navigation by Learning from Real-World Visual Data.pdf;/Users/fangyuan/Zotero/storage/ADZLGR85/2410.html}
}

@misc{zhangUPVLAUnifiedUnderstanding2025,
  title = {{{UP-VLA}}: {{A Unified Understanding}} and {{Prediction Model}} for {{Embodied Agent}}},
  shorttitle = {{{UP-VLA}}},
  author = {Zhang, Jianke and Guo, Yanjiang and Hu, Yucheng and Chen, Xiaoyu and Zhu, Xiang and Chen, Jianyu},
  year = {2025},
  month = feb,
  number = {arXiv:2501.18867},
  eprint = {2501.18867},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.18867},
  urldate = {2025-02-07},
  abstract = {Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce {\textbackslash}textbf\{UP-VLA\}, a {\textbackslash}textbf\{U\}nified VLA model training with both multi-modal {\textbackslash}textbf\{U\}nderstanding and future {\textbackslash}textbf\{P\}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33\% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/8UXUU5CK/Zhang et al. - 2025 - UP-VLA A Unified Understanding and Prediction Model for Embodied Agent.pdf;/Users/fangyuan/Zotero/storage/IKAQBS8I/2501.html}
}

@misc{zhangWholebodyHumanoidRobot2024,
  title = {Whole-Body {{Humanoid Robot Locomotion}} with {{Human Reference}}},
  author = {Zhang, Qiang and Cui, Peter and Yan, David and Sun, Jingkai and Duan, Yiqun and Han, Gang and Zhao, Wen and Zhang, Weining and Guo, Yijie and Zhang, Arthur and Xu, Renjing},
  year = {2024},
  month = aug,
  number = {arXiv:2402.18294},
  eprint = {2402.18294},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.18294},
  urldate = {2024-12-06},
  abstract = {Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, "Adam", whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the proposed framework enables Adam to achieve human-comparable performance in complex locomotion tasks, marking the first time that human locomotion data has been used for imitation learning in a full-size humanoid robot.},
  archiveprefix = {arXiv},
  keywords = {Adam,Locomotion},
  file = {/Users/fangyuan/Zotero/storage/QDHG3E3D/Zhang et al. - 2024 - Whole-body Humanoid Robot Locomotion with Human Reference.pdf;/Users/fangyuan/Zotero/storage/P37NFJQ4/2402.html}
}

@misc{zhangWholebodyHumanoidRobot2024a,
  title = {Whole-Body {{Humanoid Robot Locomotion}} with {{Human Reference}}},
  author = {Zhang, Qiang and Cui, Peter and Yan, David and Sun, Jingkai and Duan, Yiqun and Han, Gang and Zhao, Wen and Zhang, Weining and Guo, Yijie and Zhang, Arthur and Xu, Renjing},
  year = {2024},
  month = aug,
  number = {arXiv:2402.18294},
  eprint = {2402.18294},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-30},
  abstract = {Recently, humanoid robots have made significant advances in their ability to perform challenging tasks due to the deployment of Reinforcement Learning (RL), however, the inherent complexity of humanoid robots, including the difficulty of designing complicated reward functions and training entire sophisticated systems, still poses a notable challenge. To conquer these challenges, after many iterations and in-depth investigations, we have meticulously developed a full-size humanoid robot, "Adam", whose innovative structural design greatly improves the efficiency and effectiveness of the imitation learning process. In addition, we have developed a novel imitation learning framework based on an adversarial motion prior, which applies not only to Adam but also to humanoid robots in general. Using the framework, Adam can exhibit unprecedented human-like characteristics in locomotion tasks. Our experimental results demonstrate that the proposed framework enables Adam to achieve human-comparable performance in complex locomotion tasks, marking the first time that human locomotion data has been used for imitation learning in a full-size humanoid robot.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fangyuan/Zotero/storage/7NUYGIGQ/Zhang et al. - 2024 - Whole-body Humanoid Robot Locomotion with Human Reference.pdf}
}

@misc{zhangWoCoCoLearningWholeBody2024,
  title = {{{WoCoCo}}: {{Learning Whole-Body Humanoid Control}} with {{Sequential Contacts}}},
  shorttitle = {{{WoCoCo}}},
  author = {Zhang, Chong and Xiao, Wenli and He, Tairan and Shi, Guanya},
  year = {2024},
  month = nov,
  number = {arXiv:2406.06005},
  eprint = {2406.06005},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.06005},
  urldate = {2024-11-10},
  abstract = {Humanoid activities involving sequential contacts are crucial for complex robotic interactions and operations in the real world and are traditionally solved by model-based motion planning, which is time-consuming and often relies on simplified dynamics models. Although model-free reinforcement learning (RL) has become a powerful tool for versatile and robust whole-body humanoid control, it still requires tedious task-specific tuning and state machine design and suffers from long-horizon exploration issues in tasks involving contact sequences. In this work, we propose WoCoCo (Whole-Body Control with Sequential Contacts), a unified framework to learn whole-body humanoid control with sequential contacts by naturally decomposing the tasks into separate contact stages. Such decomposition facilitates simple and general policy learning pipelines through task-agnostic reward and sim-to-real designs, requiring only one or two task-related terms to be specified for each task. We demonstrated that end-to-end RL-based controllers trained with WoCoCo enable four challenging whole-body humanoid tasks involving diverse contact sequences in the real world without any motion priors: 1) versatile parkour jumping, 2) box loco-manipulation, 3) dynamic clap-and-tap dancing, and 4) cliffside climbing. We further show that WoCoCo is a general framework beyond humanoid by applying it in 22-DoF dinosaur robot loco-manipulation tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Graphics},
  file = {/Users/fangyuan/Zotero/storage/7SLMA8VE/Zhang et al. - 2024 - WoCoCo Learning Whole-Body Humanoid Control with Sequential Contacts.pdf;/Users/fangyuan/Zotero/storage/EMK94ZPJ/2406.html}
}

@misc{zhaoCoTVLAVisualChainofThought2025,
  title = {{{CoT-VLA}}: {{Visual Chain-of-Thought Reasoning}} for {{Vision-Language-Action Models}}},
  shorttitle = {{{CoT-VLA}}},
  author = {Zhao, Qingqing and Lu, Yao and Kim, Moo Jin and Fu, Zipeng and Zhang, Zhuoyang and Wu, Yecheng and Li, Zhaoshuo and Ma, Qianli and Han, Song and Finn, Chelsea and Handa, Ankur and Liu, Ming-Yu and Xiang, Donglai and Wetzstein, Gordon and Lin, Tsung-Yi},
  year = {2025},
  month = mar,
  number = {arXiv:2503.22020},
  eprint = {2503.22020},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.22020},
  urldate = {2025-04-05},
  abstract = {Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input--output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. In this paper, we introduce a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Our experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17\% in real-world manipulation tasks and 6\% in simulation benchmarks. Project website: https://cot-vla.github.io/},
  archiveprefix = {arXiv},
  keywords = {Manipulation,Subgoal,VLA},
  file = {/Users/fangyuan/Zotero/storage/K7F9NA7M/Zhao et al. - 2025 - CoT-VLA Visual Chain-of-Thought Reasoning for Vision-Language-Action Models.pdf;/Users/fangyuan/Zotero/storage/IB3UUEGL/2503.html}
}

@misc{zhaoVLASVisionLanguageActionModel2025,
  title = {{{VLAS}}: {{Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation}}},
  shorttitle = {{{VLAS}}},
  author = {Zhao, Wei and Ding, Pengxiang and Zhang, Min and Gong, Zhefei and Bai, Shuanghao and Zhao, Han and Wang, Donglin},
  year = {2025},
  month = feb,
  number = {arXiv:2502.13508},
  eprint = {2502.13508},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13508},
  urldate = {2025-02-25},
  abstract = {Vision-language-action models (VLAs) have become increasingly popular in robot manipulation for their end-to-end design and remarkable performance. However, existing VLAs rely heavily on vision-language models (VLMs) that only support text-based instructions, neglecting the more natural speech modality for human-robot interaction. Traditional speech integration methods usually involves a separate speech recognition system, which complicates the model and introduces error propagation. Moreover, the transcription procedure would lose non-semantic information in the raw speech, such as voiceprint, which may be crucial for robots to successfully complete customized tasks. To overcome above challenges, we propose VLAS, a novel end-to-end VLA that integrates speech recognition directly into the robot policy model. VLAS allows the robot to understand spoken commands through inner speech-text alignment and produces corresponding actions to fulfill the task. We also present two new datasets, SQA and CSI, to support a three-stage tuning process for speech instructions, which empowers VLAS with the ability of multimodal interaction across text, image, speech, and robot actions. Taking a step further, a voice retrieval-augmented generation (RAG) paradigm is designed to enable our model to effectively handle tasks that require individual-specific knowledge. Our extensive experiments show that VLAS can effectively accomplish robot manipulation tasks with diverse speech commands, offering a seamless and customized interaction experience.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA,Voice},
  file = {/Users/fangyuan/Zotero/storage/HJSPW6FS/Zhao et al. - 2025 - VLAS Vision-Language-Action Model With Speech Instructions For Customized Robot Manipulation.pdf;/Users/fangyuan/Zotero/storage/U2VA3ZFM/2502.html}
}

@misc{zhengTraceVLAVisualTrace2024,
  title = {{{TraceVLA}}: {{Visual Trace Prompting Enhances Spatial-Temporal Awareness}} for {{Generalist Robotic Policies}}},
  shorttitle = {{{TraceVLA}}},
  author = {Zheng, Ruijie and Liang, Yongyuan and Huang, Shuaiyi and Gao, Jianfeng and III, Hal Daum{\'e} and Kolobov, Andrey and Huang, Furong and Yang, Jianwei},
  year = {2024},
  month = dec,
  number = {arXiv:2412.10345},
  eprint = {2412.10345},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.10345},
  urldate = {2025-01-04},
  abstract = {Although large vision-language-action (VLA) models pretrained on extensive robot datasets offer promising generalist policies for robotic learning, they still struggle with spatial-temporal dynamics in interactive robotics, making them less effective in handling complex tasks, such as manipulation. In this work, we introduce visual trace prompting, a simple yet effective approach to facilitate VLA models' spatial-temporal awareness for action prediction by encoding state-action trajectories visually. We develop a new TraceVLA model by finetuning OpenVLA on our own collected dataset of 150K robot manipulation trajectories using visual trace prompting. Evaluations of TraceVLA across 137 configurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate state-of-the-art performance, outperforming OpenVLA by 10\% on SimplerEnv and 3.5x on real-robot tasks and exhibiting robust generalization across diverse embodiments and scenarios. To further validate the effectiveness and generality of our method, we present a compact VLA model based on 4B Phi-3-Vision, pretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B OpenVLA baseline while significantly improving inference efficiency.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,VLA},
  file = {/Users/fangyuan/Zotero/storage/XHMSBM93/Zheng et al. - 2024 - TraceVLA Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies.pdf;/Users/fangyuan/Zotero/storage/2TWTI6V5/2412.html}
}

@misc{zhongDexGraspVLAVisionLanguageActionFramework2025,
  title = {{{DexGraspVLA}}: {{A Vision-Language-Action Framework Towards General Dexterous Grasping}}},
  shorttitle = {{{DexGraspVLA}}},
  author = {Zhong, Yifan and Huang, Xuchuan and Li, Ruochong and Zhang, Ceyao and Liang, Yitao and Yang, Yaodong and Chen, Yuanpei},
  year = {2025},
  month = mar,
  number = {arXiv:2502.20900},
  eprint = {2502.20900},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.20900},
  urldate = {2025-03-07},
  abstract = {Dexterous grasping remains a fundamental yet challenging problem in robotics. A general-purpose robot must be capable of grasping diverse objects in arbitrary scenarios. However, existing research typically relies on specific assumptions, such as single-object settings or limited environments, leading to constrained generalization. Our solution is DexGraspVLA, a hierarchical framework that utilizes a pre-trained Vision-Language model as the high-level task planner and learns a diffusion-based policy as the low-level Action controller. The key insight lies in iteratively transforming diverse language and visual inputs into domain-invariant representations, where imitation learning can be effectively applied due to the alleviation of domain shift. Thus, it enables robust generalization across a wide range of real-world scenarios. Notably, our method achieves a 90+\% success rate under thousands of unseen object, lighting, and background combinations in a ``zero-shot'' environment. Empirical analysis further confirms the consistency of internal model behavior across environmental variations, thereby validating our design and explaining its generalization performance. We hope our work can be a step forward in achieving general dexterous grasping. Our demo and code can be found at https://dexgraspvla.github.io/.},
  archiveprefix = {arXiv},
  keywords = {Dexterous Hand,Manipulation,Robotic Foundation Model},
  file = {/Users/fangyuan/Zotero/storage/UA87LMP9/Zhong et al. - 2025 - DexGraspVLA A Vision-Language-Action Framework Towards General Dexterous Grasping.pdf;/Users/fangyuan/Zotero/storage/DPPM4S7E/2502.html}
}

@misc{zhouAutoEvalAutonomousEvaluation2025,
  title = {{{AutoEval}}: {{Autonomous Evaluation}} of {{Generalist Robot Manipulation Policies}} in the {{Real World}}},
  shorttitle = {{{AutoEval}}},
  author = {Zhou, Zhiyuan and Atreya, Pranav and Tan, You Liang and Pertsch, Karl and Levine, Sergey},
  year = {2025},
  month = mar,
  number = {arXiv:2503.24278},
  eprint = {2503.24278},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.24278},
  urldate = {2025-04-01},
  abstract = {Scalable and reproducible policy evaluation has been a long-standing challenge in robot learning. Evaluations are critical to assess progress and build better policies, but evaluation in the real world, especially at a scale that would provide statistically reliable results, is costly in terms of human time and hard to obtain. Evaluation of increasingly generalist robot policies requires an increasingly diverse repertoire of evaluation environments, making the evaluation bottleneck even more pronounced. To make real-world evaluation of robotic policies more practical, we propose AutoEval, a system to autonomously evaluate generalist robot policies around the clock with minimal human intervention. Users interact with AutoEval by submitting evaluation jobs to the AutoEval queue, much like how software jobs are submitted with a cluster scheduling system, and AutoEval will schedule the policies for evaluation within a framework supplying automatic success detection and automatic scene resets. We show that AutoEval can nearly fully eliminate human involvement in the evaluation process, permitting around the clock evaluations, and the evaluation results correspond closely to ground truth evaluations conducted by hand. To facilitate the evaluation of generalist policies in the robotics community, we provide public access to multiple AutoEval scenes in the popular BridgeData robot setup with WidowX robot arms. In the future, we hope that AutoEval scenes can be set up across institutions to form a diverse and distributed evaluation network.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/4VDR3BK4/Zhou et al. - 2025 - AutoEval Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World.pdf;/Users/fangyuan/Zotero/storage/PTTZSKNS/2503.html}
}

@misc{zhouChatVLAUnifiedMultimodal2025,
  title = {{{ChatVLA}}: {{Unified Multimodal Understanding}} and {{Robot Control}} with {{Vision-Language-Action Model}}},
  shorttitle = {{{ChatVLA}}},
  author = {Zhou, Zhongyi and Zhu, Yichen and Zhu, Minjie and Wen, Junjie and Liu, Ning and Xu, Zhiyuan and Meng, Weibin and Cheng, Ran and Peng, Yaxin and Shen, Chaomin and Feng, Feifei},
  year = {2025},
  month = feb,
  number = {arXiv:2502.14420},
  eprint = {2502.14420},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.14420},
  urldate = {2025-02-25},
  abstract = {Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2\% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control.},
  archiveprefix = {arXiv},
  keywords = {Manipulation,MoE,VLA},
  file = {/Users/fangyuan/Zotero/storage/UQGJXRN9/Zhou et al. - 2025 - ChatVLA Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model.pdf;/Users/fangyuan/Zotero/storage/K29TN2NL/2502.html}
}

@misc{zhouLargeLanguageModels2023,
  title = {Large {{Language Models Are Human-Level Prompt Engineers}}},
  author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  year = {2023},
  month = mar,
  number = {arXiv:2211.01910},
  eprint = {2211.01910},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.01910},
  urldate = {2024-02-16},
  abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
  archiveprefix = {arXiv},
  file = {/Users/fangyuan/Zotero/storage/KBEKGVMX/Zhou et al. - 2023 - Large Language Models Are Human-Level Prompt Engin.pdf;/Users/fangyuan/Zotero/storage/SYSAR73H/2211.html}
}

@misc{zhouProposerAgentEvaluatorPAEAutonomousSkill2024,
  title = {Proposer-{{Agent-Evaluator}}({{PAE}}): {{Autonomous Skill Discovery For Foundation Model Internet Agents}}},
  shorttitle = {Proposer-{{Agent-Evaluator}}({{PAE}})},
  author = {Zhou, Yifei and Yang, Qianlan and Lin, Kaixiang and Bai, Min and Zhou, Xiong and Wang, Yu-Xiong and Levine, Sergey and Li, Erran},
  year = {2024},
  month = dec,
  number = {arXiv:2412.13194},
  eprint = {2412.13194},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.13194},
  urldate = {2024-12-18},
  abstract = {The vision of a broadly capable and goal-directed agent, such as an Internet-browsing agent in the digital world and a household humanoid in the physical world, has rapidly advanced, thanks to the generalization capability of foundation models. Such a generalist agent needs to have a large and diverse skill repertoire, such as finding directions between two travel locations and buying specific items from the Internet. If each skill needs to be specified manually through a fixed set of human-annotated instructions, the agent's skill repertoire will necessarily be limited due to the quantity and diversity of human-annotated instructions. In this work, we address this challenge by proposing Proposer-Agent-Evaluator, an effective learning system that enables foundation model agents to autonomously discover and practice skills in the wild. At the heart of PAE is a context-aware task proposer that autonomously proposes tasks for the agent to practice with context information of the environment such as user demos or even just the name of the website itself for Internet-browsing agents. Then, the agent policy attempts those tasks with thoughts and actual grounded operations in the real world with resulting trajectories evaluated by an autonomous VLM-based success evaluator. The success evaluation serves as the reward signal for the agent to refine its policies through RL. We validate PAE on challenging vision-based web navigation, using both real-world and self-hosted websites from WebVoyager and WebArena.To the best of our knowledge, this work represents the first effective learning system to apply autonomous task proposal with RL for agents that generalizes real-world human-annotated benchmarks with SOTA performances. Our open-source checkpoints and code can be found in https://yanqval.github.io/PAE/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/fangyuan/Zotero/storage/FEYBF8M8/Zhou et al. - 2024 - Proposer-Agent-Evaluator(PAE) Autonomous Skill Discovery For Foundation Model Internet Agents.pdf;/Users/fangyuan/Zotero/storage/ZWGIFWRI/2412.html}
}

@misc{zhuangHumanoidParkourLearning2024,
  title = {Humanoid {{Parkour Learning}}},
  author = {Zhuang, Ziwen and Yao, Shenzhe and Zhao, Hang},
  year = {2024},
  month = sep,
  number = {arXiv:2406.10759},
  eprint = {2406.10759},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.10759},
  urldate = {2024-12-06},
  abstract = {Parkour is a grand challenge for legged locomotion, even for quadruped robots, requiring active perception and various maneuvers to overcome multiple challenging obstacles. Existing methods for humanoid locomotion either optimize a trajectory for a single parkour track or train a reinforcement learning policy only to walk with a significant amount of motion references. In this work, we propose a framework for learning an end-to-end vision-based whole-body-control parkour policy for humanoid robots that overcomes multiple parkour skills without any motion prior. Using the parkour policy, the humanoid robot can jump on a 0.42m platform, leap over hurdles, 0.8m gaps, and much more. It can also run at 1.8m/s in the wild and walk robustly on different terrains. We test our policy in indoor and outdoor environments to demonstrate that it can autonomously select parkour skills while following the rotation command of the joystick. We override the arm actions and show that this framework can easily transfer to humanoid mobile manipulation tasks. Videos can be found at https://humanoid4parkour.github.io},
  archiveprefix = {arXiv},
  keywords = {Locomotion,Unitree},
  file = {/Users/fangyuan/Zotero/storage/YZEHT7PA/Zhuang et al. - 2024 - Humanoid Parkour Learning.pdf;/Users/fangyuan/Zotero/storage/FQ53RGIQ/2406.html}
}

@misc{zhuangTDMPBCSelfImitativeReinforcement2025,
  title = {{{TDMPBC}}: {{Self-Imitative Reinforcement Learning}} for {{Humanoid Robot Control}}},
  shorttitle = {{{TDMPBC}}},
  author = {Zhuang, Zifeng and Shi, Diyuan and Suo, Runze and He, Xiao and Zhang, Hongyin and Wang, Ting and Lyu, Shangke and Wang, Donglin},
  year = {2025},
  month = feb,
  number = {arXiv:2502.17322},
  eprint = {2502.17322},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.17322},
  urldate = {2025-02-25},
  abstract = {Complex high-dimensional spaces with high Degree-of-Freedom and complicated action spaces, such as humanoid robots equipped with dexterous hands, pose significant challenges for reinforcement learning (RL) algorithms, which need to wisely balance exploration and exploitation under limited sample budgets. In general, feasible regions for accomplishing tasks within complex high-dimensional spaces are exceedingly narrow. For instance, in the context of humanoid robot motion control, the vast majority of space corresponds to falling, while only a minuscule fraction corresponds to standing upright, which is conducive to the completion of downstream tasks. Once the robot explores into a potentially task-relevant region, it should place greater emphasis on the data within that region. Building on this insight, we propose the \${\textbackslash}textbf\{S\}\$elf-\${\textbackslash}textbf\{I\}\$mitative \${\textbackslash}textbf\{R\}\$einforcement \${\textbackslash}textbf\{L\}\$earning (\${\textbackslash}textbf\{SIRL\}\$) framework, where the RL algorithm also imitates potentially task-relevant trajectories. Specifically, trajectory return is utilized to determine its relevance to the task and an additional behavior cloning is adopted whose weight is dynamically adjusted based on the trajectory return. As a result, our proposed algorithm achieves 120\% performance improvement on the challenging HumanoidBench with 5\% extra computation overhead. With further visualization, we find the significant performance gain does lead to meaningful behavior improvement that several tasks are solved successfully.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/fangyuan/Zotero/storage/XAXHMBX9/Zhuang et al. - 2025 - TDMPBC Self-Imitative Reinforcement Learning for Humanoid Robot Control.pdf;/Users/fangyuan/Zotero/storage/WTHDFPTE/2502.html}
}

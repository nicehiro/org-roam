:PROPERTIES:
:id: A8FBDBE5-1F67-4869-BAFB-C479A242A161
:END:
#+title: Deterministic Policy Gradient
#+STARTUP: latexpreview
#+filetags: :rl:dpg:

* Related Works
The basic idea of policy gradient algorithm in continuous action spaces
environment is to represent the policy by a *parametric probability distribution*
$\pi_{\theta}(a|s)=\mathbb{P}[a|s;\theta]$ that stochastically selects action a
in state s according to parameter vector $\theta$.

In this paper we instead consider deterministic policies $a=\mu_{\theta}(s)$.

In the stochastic case, the policy gradient integrates over both *state* and
*action* spaces, whereas in the deterministic case it *only* integrates over the
*state* spaces. As a result, computing the stochastic policy gradient may require
more samples, especially if action space has many dimensions.

Then we introduce the off-policy learning algorithm to ensure that our
deterministic policy continue to explore satisfactorily.
** Preliminary
We denote the density at state $s^{\prime}$ after transitioning for $t$ time
steps from state $s$ by
$$p(s \rightarrow s^{\prime}, t, \pi)$$
and the discounted state distribution by
$$\rho^{\pi}(s^{\prime}) := \int_{S}\sum_{t=1}^{\infty}\gamma^{t-1}p_{1}(s)p(s \rightarrow s^{\prime}, t, \pi)ds$$
*** Stochastic Policy Gradient Theorem
The basic idea behind these algorithm is to adjust the parameters $\theta$ of
the policy in the direction of the performance gradient $\nabla_{\theta}J(\pi_{\theta})$.

We can write the performance objective as an expectation:

\begin{equation*}
\begin{aligned}
\nabla_{\theta} J\left(\pi_{\theta}\right) &=\int_{\mathcal{S}} \rho^{\pi}(s) \int_{\mathcal{A}} \nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a) \mathrm{d} a \mathrm{~d} s \\
&=\mathbb{E}_{s \sim \rho^{\pi}, a \sim \pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(a \mid s) Q^{\pi}(s, a)\right]
\end{aligned}
\end{equation*}

*Notes*:
1. $\pi_{\theta}(a|s)$ is the probability distribution of action spaces.
   For instance, in the discrete action spaces, policy net's outputs are several
   action probability number. Our goal here is to increase the probability of
   good actions.

2. $Q^{\pi}(s,a)$ is the unbiased expectation of state and action pair.
   We use this to estimate the goodness of the action.

And the derivation procedure of the above is in [[id:A84D656F-B4D8-4BBA-B20E-9416230D05CF][Policy Gradient]].
*** Stochastic Actor-Critic Algorithm
The actor-critic is a widely used architecture based on the policy gradient
theorem. The actor adjusts the parameters $\theta$ of the stochastic policy
$\pi_{\theta}$ by above equation. The critic uses an action-value function
$Q^{w}(s,a)$ with parameter vector $w$ instead of unknown true action-value
function $Q^{\pi}(s,a)$.
*** Off-Policy Actor-Critic
See [[id:388AB2A0-7456-4885-A12B-FB3CA09AD9BC][Off Policy Actor Critic]]
* Views
** Deterministic Policy Gradient Theorem
In continuous action spaces, greedy policy improvement becomes problematic,
requiring a global maximization at every step.
Instead, a simple and computationally attractive alternative is to move the
policy in the direction of the gradient of $Q$, rather than globally
maximizing $Q$. Specifically, for each visited state $s$, the policy parameters
$\theta$ are updated in proportion to the gradient $\nabla_{\theta}Q^{\mu}(s,\mu_{\theta}(s))$.

By applying the chain rule we see that the policy improvement may be decomposed
into the gradient of the action value with respect to actions, and the gradient
of the policy with respect to the policy parameters.

\begin{equation*}
\begin{aligned}
\theta^{k+1} &= \theta^{k} + \alpha\mathbb{E}_{s\sim\rho^{\mu^{k}}}
\left[
\nabla_{\theta} Q^{\mu^{k}} (s,\mu_{\theta}(s))
\right] \\
&=
\theta^{k} + \alpha\mathbb{E}_{s\sim\rho^{\mu^{k}}}
\left[
\nabla_{\theta}\mu_{\theta}(s)
\nabla_{a} Q^{\mu^{k}} (s,a)|_{a=\mu_{\theta}(s)}
\right] \\
\end{aligned}
\end{equation*}

And then the deterministic policy gradient theorem is

\begin{equation*}
\nabla_{\theta} J(\mu_{\theta}) =
\mathbb{E}_{s\sim\rho^{\mu}}
\left[
\nabla_{\theta}\mu_{\theta}(s)\nabla_{a}\mu^{\mu}(s,a)|_{a=\mu_{\theta}(s)}
\right]
\end{equation*}

Actually, the deterministic policy gradient is indeed a *special case* of the
stochastic policy gradient.
We parameterize stochastic policies $\pi_{\mu_{\theta},\sigma}$ by a deterministic
policy $\mu_{\theta}:\mathcal{S}\rightarrow\mathcal{A}$ and a variance parameter
$\sigma$, such that for $\sigma = 0$ the stochastic policy is equivalent to the
deterministic policy.
** On-Policy Deterministic Actor-Critic
Like stochastic actor-critic algorithm, we substitute a differentiable action-
value function $Q^{w}(s,a)$ in place of the true action-value function $Q^{\mu}(s,a)$.

For example, in the following deterministic actor-critic algorithm, the critic
uses Sarsa updates to estimate the action-value function.

\begin{equation}
\begin{aligned}
\delta_{t} &=r_{t}+\gamma Q^{w}\left(s_{t+1}, a_{t+1}\right)-Q^{w}\left(s_{t}, a_{t}\right) \\
w_{t+1} &=w_{t}+\alpha_{w} \delta_{t} \nabla_{w} Q^{w}\left(s_{t}, a_{t}\right) \\
\theta_{t+1} &=\theta_{t}+\left.\alpha_{\theta} \nabla_{\theta} \mu_{\theta}\left(s_{t}\right) \nabla_{a} Q^{w}\left(s_{t}, a_{t}\right)\right|_{a=\mu_{\theta}(s)}
\end{aligned}
\end{equation}
** Off-Policy Deterministic Actor-Critic
We now consider off-policy methods that learn a deterministic target policy
$\mu_{\theta}(s)$ from trajectories generated by an arbitrary stochastic
behaviour policy $\pi(s,a)$.

As before, we modify the performance objective to be the value function of
the target policy, averaged over the *state* distribution of the behaviour
policy.

\begin{equation}
\begin{aligned}
\nabla_{\theta} J_{\beta}\left(\mu_{\theta}\right) & \approx \int_{\mathcal{S}} \rho^{\beta}(s) \nabla_{\theta} \mu_{\theta}(a \mid s) Q^{\mu}(s, a) \mathrm{d} s \\
&=\mathbb{E}_{s \sim \rho^{\beta}}\left[\left.\nabla_{\theta} \mu_{\theta}(s) \nabla_{a} Q^{\mu}(s, a)\right|_{a=\mu_{\theta}(s)}\right]
\end{aligned}
\end{equation}

*Notes*
1. We only calculate the expectation over the state distribution cause the
    action is deterministic.
2. *STILL* don't understand the approximation equation.


We again substitute a differentiable action-value function $Q^{w}(s,a)$ in place
of the true action-value function $Q^{\mu}(s,a)$. A critic estimates the action-
value function $Q^{w}(s,a)$, off-policy from trajectories generated by $\beta(a|s)$,
using an appropriate policy evaluation algorithm.


In the following off-policy deterministic actor-critic algorithm, the critic uses
Q-learning updates to estimate the action-value function.

\begin{equation}
\begin{aligned}
\delta_{t} &=r_{t}+\gamma Q^{w}\left(s_{t+1}, \mu_{\theta}\left(s_{t+1}\right)\right)-Q^{w}\left(s_{t}, a_{t}\right) \\
w_{t+1} &=w_{t}+\alpha_{w} \delta_{t} \nabla_{w} Q^{w}\left(s_{t}, a_{t}\right) \\
\theta_{t+1} &=\theta_{t}+\left.\alpha_{\theta} \nabla_{\theta} \mu_{\theta}\left(s_{t}\right) \nabla_{a} Q^{w}\left(s_{t}, a_{t}\right)\right|_{a=\mu_{\theta}(s)}
\end{aligned}
\end{equation}

Because the deterministic policy gradient _removes the integral over action_, we
can avoid importance sampling in the actor; and by _using Q-learning_, we can
avoid importance sampling in the critic.
* Reference
- [[ebib:silverDeterministicPolicyGradient][Deterministic Policy Gradient]]